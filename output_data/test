public ApplicationId getApplicationId() {return this.applicationId;}public String getNewName() {return "checkpoint_" + name;}protected AbstractFSContract createContract(Configuration conf) {return new LocalFSContract(conf);}short getMode() {return mode;}public DatanodeStorage getStorage() {return storage;}
public boolean isFailed() {return failed;}
public long getCapacity() {return capacity;}
public long getDfsUsed() {return dfsUsed;}
public long getRemaining() {return remaining;}
public long getBlockPoolUsed() {return blockPoolUsed;}public String getUser() {return iug.getUserName(mCredentialsSys.getUID(), Nfs3Constant.UNKNOWN_USER);}
public boolean shouldSilentlyDrop(RpcCall request) {return false;}
public VerifierNone getVerifer(RpcCall request) {return new VerifierNone();}
public int getUid() {return mCredentialsSys.getUID();}
public int getGid() {return mCredentialsSys.getGID();}
public int[] getAuxGids() {return mCredentialsSys.getAuxGIDs();}public void run(String argv[]) {int exitCode = -1;try {exitCode = pgd.run(argv);} catch (Throwable e) {e.printStackTrace();}System.exit(exitCode);}
public static void main(String argv[]) {new CoreTestDriver().run(argv);}public void setPostOpAttr(Nfs3FileAttributes postOpAttr) {this.postOpAttr = postOpAttr;}
public XDR writeHeaderAndResponse(XDR out, int xid, Verifier verifier) {super.writeHeaderAndResponse(out, xid, verifier);if (getStatus() == Nfs3Status.NFS3_OK) {postOpAttr.serialize(out);}return out;}public RefreshNodesResponseProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}public RefreshUserToGroupsMappingsRequestProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}public void testConstructor() {FileHandle handle = new FileHandle(1024);XDR xdr = new XDR();handle.serialize(xdr);Assert.assertEquals(handle.getFileId(), 1024);// Deserialize it back FileHandle handle2 = new FileHandle();handle2.deserialize(xdr.asReadOnlyWrap());Assert.assertEquals("Failed: Assert 1024 is id ", 1024, handle.getFileId());}public long getOffset() {return this.offset;}
public void setOffset(long offset) {this.offset = offset;}
public int getCount() {return this.count;}
public void setCount(int count) {this.count = count;}
public WriteStableHow getStableHow() {return this.stableHow;}
public ByteBuffer getData() {return this.data;}
public void serialize(XDR xdr) {handle.serialize(xdr);xdr.writeLongAsHyper(offset);xdr.writeInt(count);xdr.writeInt(stableHow.getValue());xdr.writeInt(count);xdr.writeFixedOpaque(data.array(), count);}
public String toString() {return String.format("fileId: %d offset: %d count: %d stableHow: %s", handle.getFileId(), offset, count, stableHow.name());}public static PipelineTest initTest() {return thepipelinetest = new HFlushTest();}
public void run(DatanodeID id) {final Pipeline p = getPipelineTest().getPipelineForDatanode(id);if (p == null) {return;}if (p.contains(index, id)) {final String s = super.toString(id);FiTestUtil.LOG.info(s);throw new DiskErrorException(s);}}public Path getPath() {return file;}
public long getStart() {return start;}
public long getLength() {return length;}
public String toString() {return file + ":" + start + "+" + length;}
public void write(DataOutput out) {Text.writeString(out, file.toString());out.writeLong(start);out.writeLong(length);}
public void readFields(DataInput in) {file = new Path(Text.readString(in));start = in.readLong();length = in.readLong();hosts = null;}
public String[] getLocations() {if (this.hosts == null) {return new String[] {};} else {return this.hosts;}}
public SplitLocationInfo[] getLocationInfo() {return hostInfos;}protected MessageDigest initialValue() {MessageDigest md5 = null;try {md5 = MessageDigest.getInstance("MD5");} catch (NoSuchAlgorithmException nsae) {throw new RuntimeException("Can't create MD5 digests", nsae);}return md5;}
public static long getSeed(String streamId, long masterSeed) {MessageDigest md5 = md5Holder.get();md5.reset();String str = streamId + '/' + masterSeed;byte[] digest = md5.digest(str.getBytes());// Paranoids could have XOR folded the other 8 bytes in too. long seed = 0;for (int i = 0; i < 8; i++) {seed = (seed << 8) + ((int) digest[i] + 128);}return seed;}public static void stop(Service service) {if (service != null) {service.stop();}}
public static Exception stopQuietly(Service service) {return stopQuietly(LOG, service);}
public static Exception stopQuietly(Log log, Service service) {try {stop(service);} catch (Exception e) {log.warn("When stopping the service " + service.getName() + " : " + e, e);return e;}return null;}
public synchronized void add(ServiceStateChangeListener l) {if (!listeners.contains(l)) {listeners.add(l);}}
public synchronized boolean remove(ServiceStateChangeListener l) {return listeners.remove(l);}
public synchronized void reset() {listeners.clear();}
public void notifyListeners(Service service) {//very much like CopyOnWriteArrayList, only more minimalServiceStateChangeListener[] callbacks;synchronized (this) {callbacks = listeners.toArray(new ServiceStateChangeListener[listeners.size()]);}//ensuring that listener registration/unregistration doesn't break anythingfor (ServiceStateChangeListener l : callbacks) {l.stateChanged(service);}}public CommandExecutor getExecutor(String tag) {throw new IllegalArgumentException("Method isn't supported");}public String getClientName() {return clientName;}
void setClientName(String clientName) {this.clientName = clientName;}
public String getClientMachine() {return clientMachine;}
void updateLengthOfLastBlock(INodeFile f, long lastBlockLength) {BlockInfo lastBlock = f.getLastBlock();assert (lastBlock != null) : "The last block for path " + f.getFullPathName() + " is null when updating its length";assert (lastBlock instanceof BlockInfoUnderConstruction) : "The last block for path " + f.getFullPathName() + " is not a BlockInfoUnderConstruction when updating its length";lastBlock.setNumBytes(lastBlockLength);}
void cleanZeroSizeBlock(final INodeFile f, final BlocksMapUpdateInfo collectedBlocks) {final BlockInfo[] blocks = f.getBlocks();if (blocks != null && blocks.length > 0 && blocks[blocks.length - 1] instanceof BlockInfoUnderConstruction) {BlockInfoUnderConstruction lastUC = (BlockInfoUnderConstruction) blocks[blocks.length - 1];if (lastUC.getNumBytes() == 0) {// this is a 0-sized block. do not need check its UC state herecollectedBlocks.addDeleteBlock(lastUC);f.removeLastBlock(lastUC);}}}public final void readFields(DataInput in) {compressed = new byte[in.readInt()];in.readFully(compressed, 0, compressed.length);}
protected void ensureInflated() {if (compressed != null) {try {ByteArrayInputStream deflated = new ByteArrayInputStream(compressed);DataInput inflater = new DataInputStream(new InflaterInputStream(deflated));readFieldsCompressed(inflater);compressed = null;} catch (IOException e) {throw new RuntimeException(e);}}}
public final void write(DataOutput out) {if (compressed == null) {ByteArrayOutputStream deflated = new ByteArrayOutputStream();Deflater deflater = new Deflater(Deflater.BEST_SPEED);DataOutputStream dout = new DataOutputStream(new DeflaterOutputStream(deflated, deflater));writeCompressed(dout);dout.close();deflater.end();compressed = deflated.toByteArray();}out.writeInt(compressed.length);out.write(compressed);}public float getCapacity() {return this.capacity;}
public float getUsedCapacity() {return this.usedCapacity;}
public float getMaxCapacity() {return this.maxCapacity;}
public String getQueueName() {return this.queueName;}
public CapacitySchedulerQueueInfoList getQueues() {return this.queues;}
protected CapacitySchedulerQueueInfoList getQueues(CSQueue parent) {CSQueue parentQueue = parent;CapacitySchedulerQueueInfoList queuesInfo = new CapacitySchedulerQueueInfoList();for (CSQueue queue : parentQueue.getChildQueues()) {CapacitySchedulerQueueInfo info;if (queue instanceof LeafQueue) {info = new CapacitySchedulerLeafQueueInfo((LeafQueue) queue);} else {info = new CapacitySchedulerQueueInfo(queue);info.queues = getQueues(queue);}queuesInfo.addToQueueInfoList(info);}return queuesInfo;}public RefreshCallQueueResponseProto refreshCallQueue(RpcController controller, RefreshCallQueueRequestProto request) {try {impl.refreshCallQueue();} catch (IOException e) {throw new ServiceException(e);}return VOID_REFRESH_CALL_QUEUE_RESPONSE;}protected Short parse(String str) {return Short.parseShort(str, radix);}
protected String getDomain() {return "a short";}public void init(Properties config, long tokenValidity) {initSecrets(generateNewSecret(), null);startScheduler(tokenValidity, tokenValidity);}
protected void initSecrets(byte[] currentSecret, byte[] previousSecret) {secrets = new byte[][] { currentSecret, previousSecret };}
protected synchronized void startScheduler(long initialDelay, long period) {if (!schedulerRunning) {schedulerRunning = true;scheduler = Executors.newSingleThreadScheduledExecutor();scheduler.scheduleAtFixedRate(new Runnable() {
@Overridepublic void run() {rollSecret();}}, initialDelay, period, TimeUnit.MILLISECONDS);}}
public void run() {rollSecret();}
public synchronized void destroy() {if (!isDestroyed) {isDestroyed = true;if (scheduler != null) {scheduler.shutdown();}schedulerRunning = false;super.destroy();}}
protected synchronized void rollSecret() {if (!isDestroyed) {LOG.debug("rolling secret");byte[] newSecret = generateNewSecret();secrets = new byte[][] { newSecret, secrets[0] };}}
public byte[] getCurrentSecret() {return secrets[0];}
public byte[][] getAllSecrets() {return secrets;}public void resetStream() {mark = 0;reset();}
public boolean hasNext() {return infbuf != null && inbuf.available() > 0;}
public boolean next(X val) {if (hasNext()) {inbuf.mark(0);val.readFields(infbuf);return true;}return false;}
public boolean replay(X val) {inbuf.reset();if (0 == inbuf.available())return false;val.readFields(infbuf);return true;}
public void reset() {if (null != outfbuf) {inbuf = new ReplayableByteInputStream(outbuf.toByteArray());infbuf = new DataInputStream(inbuf);outfbuf = null;}inbuf.resetStream();}
public void add(X item) {item.write(outfbuf);}
public void close() {if (null != infbuf)infbuf.close();if (null != outfbuf)outfbuf.close();}
public void clear() {if (null != inbuf)inbuf.resetStream();outbuf.reset();outfbuf = new DataOutputStream(outbuf);}public static byte[] getFileData(int numOfBlocks, long blockSize) {byte[] data = new byte[(int) (numOfBlocks * blockSize)];for (int i = 0; i < data.length; i++) {data[i] = (byte) (i % 10);}return data;}
public Path getTestRootPath() {return makeQualified(new Path(testRootDir));}
public Path getTestRootPath(String pathString) {return makeQualified(new Path(testRootDir, pathString));}
public String getAbsoluteTestRootDir() {if (absTestRootDir == null) {Path testRootPath = new Path(testRootDir);if (testRootPath.isAbsolute()) {absTestRootDir = testRootDir;} else {absTestRootDir = getWorkingDirectory().toString() + "/" + testRootDir;}}return absTestRootDir;}
public Path getAbsoluteTestRootPath() {return makeQualified(new Path(getAbsoluteTestRootDir()));}public void write(int b) {buf.put((byte) b);}
public void write(byte[] b, int off, int len) {buf.put(b, off, len);}public synchronized void activateApplication(String user, ApplicationId applicationId) {Set<ApplicationId> userApps = usersApplications.get(user);if (userApps == null) {userApps = new HashSet<ApplicationId>();usersApplications.put(user, userApps);++activeUsers;metrics.incrActiveUsers();LOG.debug("User " + user + " added to activeUsers, currently: " + activeUsers);}if (userApps.add(applicationId)) {metrics.activateApp(user);}}
public synchronized void deactivateApplication(String user, ApplicationId applicationId) {Set<ApplicationId> userApps = usersApplications.get(user);if (userApps != null) {if (userApps.remove(applicationId)) {metrics.deactivateApp(user);}if (userApps.isEmpty()) {usersApplications.remove(user);--activeUsers;metrics.decrActiveUsers();LOG.debug("User " + user + " removed from activeUsers, currently: " + activeUsers);}}}
public synchronized int getNumActiveUsers() {return activeUsers;}public ChannelBuffer data() {return data;}
public SocketAddress remoteAddress() {return remoteAddress;}public long getEpoch() {return epoch;}
public void setEpoch(long epoch) {this.epoch = epoch;}
public String getJournalId() {return jid;}
public long getIpcSerialNumber() {return ipcSerialNumber;}
public void setIpcSerialNumber(long ipcSerialNumber) {this.ipcSerialNumber = ipcSerialNumber;}
public long getCommittedTxId() {return committedTxId;}
public boolean hasCommittedTxId() {return (committedTxId != HdfsConstants.INVALID_TXID);}public void setFairShare(Resource resource) {fairShareMB.set(resource.getMemory());fairShareVCores.set(resource.getVirtualCores());}
public int getFairShareMB() {return fairShareMB.value();}
public int getFairShareVirtualCores() {return fairShareVCores.value();}
public void setSteadyFairShare(Resource resource) {steadyFairShareMB.set(resource.getMemory());steadyFairShareVCores.set(resource.getVirtualCores());}
public int getSteadyFairShareMB() {return steadyFairShareMB.value();}
public int getSteadyFairShareVCores() {return steadyFairShareVCores.value();}
public void setMinShare(Resource resource) {minShareMB.set(resource.getMemory());minShareVCores.set(resource.getVirtualCores());}
public int getMinShareMB() {return minShareMB.value();}
public int getMinShareVirtualCores() {return minShareVCores.value();}
public void setMaxShare(Resource resource) {maxShareMB.set(resource.getMemory());maxShareVCores.set(resource.getVirtualCores());}
public int getMaxShareMB() {return maxShareMB.value();}
public int getMaxShareVirtualCores() {return maxShareVCores.value();}
public static synchronized FSQueueMetrics forQueue(String queueName, Queue parent, boolean enableUserMetrics, Configuration conf) {MetricsSystem ms = DefaultMetricsSystem.instance();QueueMetrics metrics = queueMetrics.get(queueName);if (metrics == null) {metrics = new FSQueueMetrics(ms, queueName, parent, enableUserMetrics, conf).tag(QUEUE_INFO, queueName);// Register with the MetricsSystemsif (ms != null) {metrics = ms.register(sourceName(queueName).toString(), "Metrics for queue: " + queueName, metrics);}queueMetrics.put(queueName, metrics);}return (FSQueueMetrics) metrics;}public void render(Block html) {html.p()._("Lorem ipsum dolor sit amet, consectetur adipiscing elit.", "Vivamus eu dui in ipsum tincidunt egestas ac sed nibh.", "Praesent quis nisl lorem, nec interdum urna.", "Duis sagittis dignissim purus sed sollicitudin.", "Morbi quis diam eu enim semper suscipit.", "Nullam pretium faucibus sapien placerat tincidunt.", "Donec eget lorem at quam fermentum vulputate a ac purus.", "Cras ac dui felis, in pulvinar est.", "Praesent tempor est sed neque pulvinar dictum.", "Nullam magna augue, egestas luctus sollicitudin sed,", "venenatis nec turpis.", "Ut ante enim, congue sed laoreet et, accumsan id metus.", "Mauris tincidunt imperdiet est, sed porta arcu vehicula et.", "Etiam in nisi nunc.", "Phasellus vehicula scelerisque quam, ac dignissim felis euismod a.", "Proin eu ante nisl, vel porttitor eros.", "Aliquam gravida luctus augue, at scelerisque enim consectetur vel.", "Donec interdum tempor nisl, quis laoreet enim venenatis eu.", "Quisque elit elit, vulputate eget porta vel, laoreet ac lacus.")._();}public static void checkClassLoader(Class cls, boolean shouldBeLoadedByAppClassLoader) {boolean loadedByAppClassLoader = cls.getClassLoader() instanceof ApplicationClassLoader;if ((shouldBeLoadedByAppClassLoader && !loadedByAppClassLoader) || (!shouldBeLoadedByAppClassLoader && loadedByAppClassLoader)) {throw new RuntimeException("incorrect classloader used");}}public void setUp() {testAccount = AzureBlobStorageTestAccount.createMock();fs = testAccount.getFileSystem();backingStore = testAccount.getMockStorage().getBackingStore();}
public void tearDown() {testAccount.cleanup();fs = null;backingStore = null;}
private int getNumTempBlobs() {int count = 0;for (String key : backingStore.getKeys()) {if (key.contains(NativeAzureFileSystem.AZURE_TEMP_FOLDER)) {count++;}}return count;}
private void runFsck(String command) {Configuration conf = fs.getConf();// dangling.conf.setInt(NativeAzureFileSystem.AZURE_TEMP_EXPIRY_PROPERTY_NAME, 0);WasbFsck fsck = new WasbFsck(conf);fsck.setMockFileSystemForTesting(fs);fsck.run(new String[] { AzureBlobStorageTestAccount.MOCK_WASB_URI, command });}
public void testDelete() {Path danglingFile = new Path("/crashedInTheMiddle");// Create a file and leave it dangling and try to delete it.FSDataOutputStream stream = fs.create(danglingFile);stream.write(new byte[] { 1, 2, 3 });stream.flush();// Now we should still only see a zero-byte file in this placeFileStatus fileStatus = fs.getFileStatus(danglingFile);assertNotNull(fileStatus);assertEquals(0, fileStatus.getLen());assertEquals(1, getNumTempBlobs());// Run WasbFsck -delete to delete the file.runFsck("-delete");// Now we should see no trace of the file.assertEquals(0, getNumTempBlobs());assertFalse(fs.exists(danglingFile));}public ContainerResourceDecreaseProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public ContainerId getContainerId() {ContainerResourceDecreaseProtoOrBuilder p = viaProto ? proto : builder;if (this.existingContainerId != null) {return this.existingContainerId;}if (p.hasContainerId()) {this.existingContainerId = convertFromProtoFormat(p.getContainerId());}return this.existingContainerId;}
public void setContainerId(ContainerId existingContainerId) {maybeInitBuilder();if (existingContainerId == null) {builder.clearContainerId();}this.existingContainerId = existingContainerId;}
public Resource getCapability() {ContainerResourceDecreaseProtoOrBuilder p = viaProto ? proto : builder;if (this.targetCapability != null) {return this.targetCapability;}if (p.hasCapability()) {this.targetCapability = convertFromProtoFormat(p.getCapability());}return this.targetCapability;}
public void setCapability(Resource targetCapability) {maybeInitBuilder();if (targetCapability == null) {builder.clearCapability();}this.targetCapability = targetCapability;}
private ContainerIdPBImpl convertFromProtoFormat(ContainerIdProto p) {return new ContainerIdPBImpl(p);}
private ContainerIdProto convertToProtoFormat(ContainerId t) {return ((ContainerIdPBImpl) t).getProto();}
private Resource convertFromProtoFormat(ResourceProto p) {return new ResourcePBImpl(p);}
private ResourceProto convertToProtoFormat(Resource t) {return ((ResourcePBImpl) t).getProto();}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ContainerResourceDecreaseProto.newBuilder(proto);}viaProto = false;}
private void mergeLocalToBuilder() {if (this.existingContainerId != null) {builder.setContainerId(convertToProtoFormat(this.existingContainerId));}if (this.targetCapability != null) {builder.setCapability(convertToProtoFormat(this.targetCapability));}}public String getAlias() {return alias;}
public char[] getCredential() {return credential;}
public String toString() {StringBuilder buf = new StringBuilder();buf.append("alias(");buf.append(alias);buf.append(")=");if (credential == null) {buf.append("null");} else {for (char c : credential) {buf.append(c);}}return buf.toString();}
public boolean isTransient() {return false;}protected void createInput() {DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));for (int i = 0; i < 10000; ++i) {out.write(input.getBytes("UTF-8"));}out.close();}
protected String[] genArgs() {return new String[] { "-input", INPUT_FILE.getAbsolutePath(), "-output", OUTPUT_DIR.getAbsolutePath(), "-mapper", map, "-reducer", "org.apache.hadoop.mapred.lib.IdentityReducer", "-numReduceTasks", "0", "-jobconf", "mapreduce.task.files.preserve.failedtasks=true", "-jobconf", "stream.tmpdir=" + System.getProperty("test.build.data", "/tmp") };}
public void testUnconsumedInput() {String outFileName = "part-00000";File outFile = null;try {try {FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());} catch (Exception e) {}createInput();// setup config to ignore unconsumed inputConfiguration conf = new Configuration();conf.set("stream.minRecWrittenToEnableSkip_", "0");job = new StreamJob();job.setConf(conf);int exitCode = job.run(genArgs());assertEquals("Job failed", 0, exitCode);outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();String output = StreamUtil.slurp(outFile);assertEquals("Output was truncated", EXPECTED_OUTPUT_SIZE, StringUtils.countMatches(output, "\t"));} finally {INPUT_FILE.delete();FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());}}public ApplicationAttemptId getApplicationAttemptId() {return applicationAttemptId;}
public boolean getTransferStateFromPreviousAttempt() {return transferStateFromPreviousAttempt;}
public boolean getIsAttemptRecovering() {return isAttemptRecovering;}public synchronized PreemptionContainerProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void mergeLocalToBuilder() {if (id != null) {builder.setId(convertToProtoFormat(id));}}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = PreemptionContainerProto.newBuilder(proto);}viaProto = false;}
public synchronized ContainerId getId() {PreemptionContainerProtoOrBuilder p = viaProto ? proto : builder;if (id != null) {return id;}if (!p.hasId()) {return null;}id = convertFromProtoFormat(p.getId());return id;}
public synchronized void setId(final ContainerId id) {maybeInitBuilder();if (null == id) {builder.clearId();}this.id = id;}
private ContainerIdPBImpl convertFromProtoFormat(ContainerIdProto p) {return new ContainerIdPBImpl(p);}
private ContainerIdProto convertToProtoFormat(ContainerId t) {return ((ContainerIdPBImpl) t).getProto();}public void testRunjar() {File outFile = new File(TEST_ROOT_DIR, "out");if (outFile.exists()) {outFile.delete();}File makeTestJar = makeTestJar();String[] args = new String[3];args[0] = makeTestJar.getAbsolutePath();args[1] = "org.apache.hadoop.util.Hello";args[2] = outFile.toString();RunJar.main(args);Assert.assertTrue("RunJar failed", outFile.exists());}
private File makeTestJar() {File jarFile = new File(TEST_ROOT_DIR, TEST_JAR_NAME);JarOutputStream jstream = new JarOutputStream(new FileOutputStream(jarFile));InputStream entryInputStream = this.getClass().getResourceAsStream(CLASS_NAME);ZipEntry entry = new ZipEntry("org/apache/hadoop/util/" + CLASS_NAME);jstream.putNextEntry(entry);BufferedInputStream bufInputStream = new BufferedInputStream(entryInputStream, 2048);int count;byte[] data = new byte[2048];while ((count = bufInputStream.read(data, 0, 2048)) != -1) {jstream.write(data, 0, count);}jstream.closeEntry();jstream.close();return jarFile;}public GetClusterMetricsRequestProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}public void init(SubsetConfiguration conf) {String filename = conf.getString(FILENAME_KEY);try {writer = filename == null ? new PrintWriter(System.out) : new PrintWriter(new FileWriter(new File(filename), true));} catch (Exception e) {throw new MetricsException("Error creating " + filename, e);}}
public void putMetrics(MetricsRecord record) {writer.print(record.timestamp());writer.print(" ");writer.print(record.context());writer.print(".");writer.print(record.name());String separator = ": ";for (MetricsTag tag : record.tags()) {writer.print(separator);separator = ", ";writer.print(tag.name());writer.print("=");writer.print(tag.value());}for (AbstractMetric metric : record.metrics()) {writer.print(separator);separator = ", ";writer.print(metric.name());writer.print("=");writer.print(metric.value());}writer.println();}
public void flush() {writer.flush();}
public void close() {writer.close();}public void testSnapshotStatsMXBeanInfo() {Configuration conf = new Configuration();MiniDFSCluster cluster = null;String pathName = "/snapshot";Path path = new Path(pathName);try {cluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();SnapshotManager sm = cluster.getNamesystem().getSnapshotManager();DistributedFileSystem dfs = (DistributedFileSystem) cluster.getFileSystem();dfs.mkdirs(path);dfs.allowSnapshot(path);dfs.createSnapshot(path);MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();ObjectName mxbeanName = new ObjectName("Hadoop:service=NameNode,name=SnapshotInfo");CompositeData[] directories = (CompositeData[]) mbs.getAttribute(mxbeanName, "SnapshottableDirectories");int numDirectories = Array.getLength(directories);assertEquals(sm.getNumSnapshottableDirs(), numDirectories);CompositeData[] snapshots = (CompositeData[]) mbs.getAttribute(mxbeanName, "Snapshots");int numSnapshots = Array.getLength(snapshots);assertEquals(sm.getNumSnapshots(), numSnapshots);CompositeData d = (CompositeData) Array.get(directories, 0);CompositeData s = (CompositeData) Array.get(snapshots, 0);assertTrue(((String) d.get("path")).contains(pathName));assertTrue(((String) s.get("snapshotDirectory")).contains(pathName));} finally {if (cluster != null) {cluster.shutdown();}}}public synchronized void incr() {++value;setChanged();}
public synchronized void incr(int delta) {value += delta;setChanged();}
public int value() {return value;}
public void snapshot(MetricsRecordBuilder builder, boolean all) {if (all || changed()) {builder.addCounter(info(), value);clearChanged();}}public long getVmemLimit() {return this.vmemLimit;}
public long getPmemLimit() {return this.pmemLimit;}public static JournalMetrics create(Journal j) {JournalMetrics m = new JournalMetrics(j);return DefaultMetricsSystem.instance().register(m.getName(), null, m);}
String getName() {return "Journal-" + journal.getJournalId();}
public long getLastWriterEpoch() {try {return journal.getLastWriterEpoch();} catch (IOException e) {return -1L;}}
public long getLastPromisedEpoch() {try {return journal.getLastPromisedEpoch();} catch (IOException e) {return -1L;}}
public long getLastWrittenTxId() {return journal.getHighestWrittenTxId();}
public long getCurrentLagTxns() {try {return journal.getCurrentLagTxns();} catch (IOException e) {return -1L;}}
void addSync(long us) {for (MutableQuantiles q : syncsQuantiles) {q.add(us);}}private static String getSnapshotName(String name) {if (Path.CUR_DIR.equals(name)) {// current directoryreturn "";}final int i;if (name.startsWith(HdfsConstants.DOT_SNAPSHOT_DIR + Path.SEPARATOR)) {i = 0;} else if (name.startsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR + Path.SEPARATOR)) {i = 1;} else {return name;}// get the snapshot namereturn name.substring(i + HdfsConstants.DOT_SNAPSHOT_DIR.length() + 1);}
public int run(String[] argv) {String description = "SnapshotDiff <snapshotDir> <from> <to>:\n" + "\tGet the difference between two snapshots, \n" + "\tor between a snapshot and the current tree of a directory.\n" + "\tFor <from>/<to>, users can use \".\" to present the current status,\n" + "\tand use \".snapshot/snapshot_name\" to present a snapshot,\n" + "\twhere \".snapshot/\" can be omitted\n";if (argv.length != 3) {System.err.println("Usage: \n" + description);return 1;}FileSystem fs = FileSystem.get(getConf());if (!(fs instanceof DistributedFileSystem)) {System.err.println("SnapshotDiff can only be used in DistributedFileSystem");return 1;}DistributedFileSystem dfs = (DistributedFileSystem) fs;Path snapshotRoot = new Path(argv[0]);String fromSnapshot = getSnapshotName(argv[1]);String toSnapshot = getSnapshotName(argv[2]);try {SnapshotDiffReport diffReport = dfs.getSnapshotDiffReport(snapshotRoot, fromSnapshot, toSnapshot);System.out.println(diffReport.toString());} catch (IOException e) {String[] content = e.getLocalizedMessage().split("\n");System.err.println("snapshotDiff: " + content[0]);return 1;}return 0;}
public static void main(String[] argv) {int rc = ToolRunner.run(new SnapshotDiff(), argv);System.exit(rc);}public String key() {return variable;}
public String toString() {return variable;}
public String $() {if (Shell.WINDOWS) {return "%" + variable + "%";} else {return "$" + variable;}}
public String $$() {return PARAMETER_EXPANSION_LEFT + variable + PARAMETER_EXPANSION_RIGHT;}protected int[] getValues() {return values;}
protected void initializeInterval() {state.currentAccumulation = 0.0D;}
protected void extend(double newProgress, int newValue) {if (state == null || newProgress < state.oldProgress) {return;}// This correctness of this code depends on 100% * count = count.int oldIndex = (int) (state.oldProgress * count);int newIndex = (int) (newProgress * count);int originalOldValue = state.oldValue;double fullValueDistance = (double) newValue - state.oldValue;double fullProgressDistance = newProgress - state.oldProgress;double originalOldProgress = state.oldProgress;//  crosses a boundary.for (int closee = oldIndex; closee < newIndex; ++closee) {double interpolationProgress = (double) (closee + 1) / count;// In floats, x * y / y might not equal y.interpolationProgress = Math.min(interpolationProgress, newProgress);double progressLength = (interpolationProgress - originalOldProgress);double interpolationProportion = progressLength / fullProgressDistance;double interpolationValueDistance = fullValueDistance * interpolationProportion;// estimates the value at the next [interpolated] subsegment boundaryint interpolationValue = (int) interpolationValueDistance + originalOldValue;extendInternal(interpolationProgress, interpolationValue);advanceState(interpolationProgress, interpolationValue);values[closee] = (int) state.currentAccumulation;initializeInterval();}extendInternal(newProgress, newValue);advanceState(newProgress, newValue);if (newIndex == count) {state = null;}}
protected void advanceState(double newProgress, int newValue) {state.oldValue = newValue;state.oldProgress = newProgress;}
int getCount() {return count;}
int get(int index) {return values[index];}protected void extraStatusAssertions(SwiftFileStatus stat) {//HDFS2assertTrue("isDirectory(): Not a directory: " + stat, stat.isDirectory());assertFalse("isFile(): declares itself a file: " + stat, stat.isFile());assertFalse("isFile(): declares itself a file: " + stat, stat.isSymlink());}private void serializeInt(int times) {XDR w = new XDR();for (int i = 0; i < times; ++i) w.writeInt(WRITE_VALUE);XDR r = w.asReadOnlyWrap();for (int i = 0; i < times; ++i) Assert.assertEquals(WRITE_VALUE, r.readInt());}
private void serializeLong(int times) {XDR w = new XDR();for (int i = 0; i < times; ++i) w.writeLongAsHyper(WRITE_VALUE);XDR r = w.asReadOnlyWrap();for (int i = 0; i < times; ++i) Assert.assertEquals(WRITE_VALUE, r.readHyper());}
public void testPerformance() {final int TEST_TIMES = 8 << 20;serializeInt(TEST_TIMES);serializeLong(TEST_TIMES);}protected FileContextTestHelper getFileContextHelper() {return fileContextTestHelper;}
protected FileContext getFileContext() {return fc;}
public static void clusterSetupAtBegining() {Configuration conf = new HdfsConfiguration();cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();fc = FileContext.getFileContext(cluster.getURI(0), conf);defaultWorkingDirectory = fc.makeQualified(new Path("/user/" + UserGroupInformation.getCurrentUser().getShortUserName()));fc.mkdir(defaultWorkingDirectory, FileContext.DEFAULT_PERM, true);}
public static void ClusterShutdownAtEnd() {cluster.shutdown();}
public void setUp() {super.setUp();}
public void tearDown() {super.tearDown();}public void testRejectStateFromValue() {Assert.assertEquals(RejectState.RPC_MISMATCH, RejectState.fromValue(0));Assert.assertEquals(RejectState.AUTH_ERROR, RejectState.fromValue(1));}
public void testRejectStateFromInvalidValue1() {RejectState.fromValue(2);}
public void testConstructor() {RpcDeniedReply reply = new RpcDeniedReply(0, ReplyState.MSG_ACCEPTED, RejectState.AUTH_ERROR, new VerifierNone());Assert.assertEquals(0, reply.getXid());Assert.assertEquals(RpcMessage.Type.RPC_REPLY, reply.getMessageType());Assert.assertEquals(ReplyState.MSG_ACCEPTED, reply.getState());Assert.assertEquals(RejectState.AUTH_ERROR, reply.getRejectState());}public int getDataLen() {return proto.getDataLen();}
public boolean isLastPacketInBlock() {return proto.getLastPacketInBlock();}
public long getSeqno() {return proto.getSeqno();}
public long getOffsetInBlock() {return proto.getOffsetInBlock();}
public int getPacketLen() {return packetLen;}
public boolean getSyncBlock() {return proto.getSyncBlock();}
public String toString() {return "PacketHeader with packetLen=" + packetLen + " header data: " + proto.toString();}
public void setFieldsFromData(int packetLen, byte[] headerData) {this.packetLen = packetLen;proto = PacketHeaderProto.parseFrom(headerData);}
public void readFields(ByteBuffer buf) {packetLen = buf.getInt();short protoLen = buf.getShort();byte[] data = new byte[protoLen];buf.get(data);proto = PacketHeaderProto.parseFrom(data);}
public void readFields(DataInputStream in) {this.packetLen = in.readInt();short protoLen = in.readShort();byte[] data = new byte[protoLen];in.readFully(data);proto = PacketHeaderProto.parseFrom(data);}
public int getSerializedSize() {return PKT_LENGTHS_LEN + proto.getSerializedSize();}
public void putInBuffer(final ByteBuffer buf) {assert proto.getSerializedSize() <= MAX_PROTO_SIZE : "Expected " + (MAX_PROTO_SIZE) + " got: " + proto.getSerializedSize();try {buf.putInt(packetLen);buf.putShort((short) proto.getSerializedSize());proto.writeTo(new ByteBufferOutputStream(buf));} catch (IOException e) {throw new RuntimeException(e);}}
public void write(DataOutputStream out) {assert proto.getSerializedSize() <= MAX_PROTO_SIZE : "Expected " + (MAX_PROTO_SIZE) + " got: " + proto.getSerializedSize();out.writeInt(packetLen);out.writeShort(proto.getSerializedSize());proto.writeTo(out);}
public byte[] getBytes() {ByteBuffer buf = ByteBuffer.allocate(getSerializedSize());putInBuffer(buf);return buf.array();}
public boolean sanityCheck(long lastSeqNo) {// We should only have a non-positive data length for the last packetif (proto.getDataLen() <= 0 && !proto.getLastPacketInBlock())return false;// The last packet should not contain dataif (proto.getLastPacketInBlock() && proto.getDataLen() != 0)return false;// Seqnos should always increase by 1 with each packet receivedif (proto.getSeqno() != lastSeqNo + 1)return false;return true;}
public boolean equals(Object o) {if (!(o instanceof PacketHeader))return false;PacketHeader other = (PacketHeader) o;return this.proto.equals(other.proto);}
public int hashCode() {return (int) proto.getSeqno();}public void testOverRead() {final String message = "message";final Path filePath = new Path("/test/file.txt");writeTextFile(fs, filePath, message, false);try {readBytesToString(fs, filePath, 20);fail("expected an exception");} catch (EOFException e) {}}
public void testRWJson() {final String message = "{" + " 'json': { 'i':43, 'b':true}," + " 's':'string'" + "}";final Path filePath = new Path("/test/file.json");writeTextFile(fs, filePath, message, false);String readJson = readBytesToString(fs, filePath, message.length());assertEquals(message, readJson);FileStatus status = fs.getFileStatus(filePath);BlockLocation[] locations = fs.getFileBlockLocations(status, 0, 10);}
public void testRWXML() {final String message = "<x>" + " <json i='43' 'b'=true/>" + " string" + "</x>";final Path filePath = new Path("/test/file.xml");writeTextFile(fs, filePath, message, false);String read = readBytesToString(fs, filePath, message.length());assertEquals(message, read);}public static String confAsString(Conf conf) {StringBuilder builder = new StringBuilder();builder.append("shortCircuitStreamsCacheSize = ").append(conf.shortCircuitStreamsCacheSize).append(", shortCircuitStreamsCacheExpiryMs = ").append(conf.shortCircuitStreamsCacheExpiryMs).append(", shortCircuitMmapCacheSize = ").append(conf.shortCircuitMmapCacheSize).append(", shortCircuitMmapCacheExpiryMs = ").append(conf.shortCircuitMmapCacheExpiryMs).append(", shortCircuitMmapCacheRetryTimeout = ").append(conf.shortCircuitMmapCacheRetryTimeout).append(", shortCircuitCacheStaleThresholdMs = ").append(conf.shortCircuitCacheStaleThresholdMs).append(", socketCacheCapacity = ").append(conf.socketCacheCapacity).append(", socketCacheExpiry = ").append(conf.socketCacheExpiry).append(", shortCircuitLocalReads = ").append(conf.shortCircuitLocalReads).append(", useLegacyBlockReaderLocal = ").append(conf.useLegacyBlockReaderLocal).append(", domainSocketDataTraffic = ").append(conf.domainSocketDataTraffic).append(", shortCircuitSharedMemoryWatcherInterruptCheckMs = ").append(conf.shortCircuitSharedMemoryWatcherInterruptCheckMs);return builder.toString();}
public static ClientContext get(String name, Conf conf) {ClientContext context;synchronized (ClientContext.class) {context = CACHES.get(name);if (context == null) {context = new ClientContext(name, conf);CACHES.put(name, context);} else {context.printConfWarningIfNeeded(conf);}}return context;}
public static ClientContext getFromConf(Configuration conf) {return get(conf.get(DFSConfigKeys.DFS_CLIENT_CONTEXT, DFSConfigKeys.DFS_CLIENT_CONTEXT_DEFAULT), new DFSClient.Conf(conf));}
private void printConfWarningIfNeeded(Conf conf) {String existing = this.getConfString();String requested = confAsString(conf);if (!existing.equals(requested)) {if (!printedConfWarning) {printedConfWarning = true;LOG.warn("Existing client context '" + name + "' does not match " + "requested configuration.  Existing: " + existing + ", Requested: " + requested);}}}
public String getConfString() {return confString;}
public ShortCircuitCache getShortCircuitCache() {return shortCircuitCache;}
public PeerCache getPeerCache() {return peerCache;}
public boolean getUseLegacyBlockReaderLocal() {return useLegacyBlockReaderLocal;}
public boolean getDisableLegacyBlockReaderLocal() {return disableLegacyBlockReaderLocal;}
public void setDisableLegacyBlockReaderLocal() {disableLegacyBlockReaderLocal = true;}
public DomainSocketFactory getDomainSocketFactory() {return domainSocketFactory;}public ApplicationId getApplicationId() {if (this.applicationId != null) {return this.applicationId;}ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationId()) {return null;}this.applicationId = convertFromProtoFormat(p.getApplicationId());return this.applicationId;}
public void setApplicationId(ApplicationId applicationId) {maybeInitBuilder();if (applicationId == null) {builder.clearApplicationId();}this.applicationId = applicationId;}
public String getApplicationName() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationName()) {return null;}return p.getApplicationName();}
public void setApplicationName(String applicationName) {maybeInitBuilder();if (applicationName == null) {builder.clearApplicationName();return;}builder.setApplicationName(applicationName);}
public String getApplicationType() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationType()) {return null;}return p.getApplicationType();}
public void setApplicationType(String applicationType) {maybeInitBuilder();if (applicationType == null) {builder.clearApplicationType();return;}builder.setApplicationType(applicationType);}
public String getUser() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasUser()) {return null;}return p.getUser();}
public void setUser(String user) {maybeInitBuilder();if (user == null) {builder.clearUser();return;}builder.setUser(user);}
public String getQueue() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasQueue()) {return null;}return p.getQueue();}
public void setQueue(String queue) {maybeInitBuilder();if (queue == null) {builder.clearQueue();return;}builder.setQueue(queue);}
public long getSubmitTime() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;return p.getSubmitTime();}
public void setSubmitTime(long submitTime) {maybeInitBuilder();builder.setSubmitTime(submitTime);}
public long getStartTime() {ApplicationStartDataProtoOrBuilder p = viaProto ? proto : builder;return p.getStartTime();}
public void setStartTime(long startTime) {maybeInitBuilder();builder.setStartTime(startTime);}
public ApplicationStartDataProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (this.applicationId != null && !((ApplicationIdPBImpl) this.applicationId).getProto().equals(builder.getApplicationId())) {builder.setApplicationId(convertToProtoFormat(this.applicationId));}}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ApplicationStartDataProto.newBuilder(proto);}viaProto = false;}
private ApplicationIdProto convertToProtoFormat(ApplicationId applicationId) {return ((ApplicationIdPBImpl) applicationId).getProto();}
private ApplicationIdPBImpl convertFromProtoFormat(ApplicationIdProto applicationId) {return new ApplicationIdPBImpl(applicationId);}public void run() {RUN = true;}
public Object call() {RUN = true;return null;}
public Object call() {throw new Exception();}
public void runnable() {R r = new R();RunnableCallable rc = new RunnableCallable(r);rc.run();assertTrue(r.RUN);r = new R();rc = new RunnableCallable(r);rc.call();assertTrue(r.RUN);assertEquals(rc.toString(), "R");}
public void callable() {C c = new C();RunnableCallable rc = new RunnableCallable(c);rc.run();assertTrue(c.RUN);c = new C();rc = new RunnableCallable(c);rc.call();assertTrue(c.RUN);assertEquals(rc.toString(), "C");}
public void callableExRun() {CEx c = new CEx();RunnableCallable rc = new RunnableCallable(c);rc.run();}public void setup() {store = new MemoryTimelineStore();store.init(new YarnConfiguration());store.start();loadTestData();loadVerificationData();}
public void tearDown() {store.stop();}
public TimelineStore getTimelineStore() {return store;}
public void testGetSingleEntity() {super.testGetSingleEntity();}
public void testGetEntities() {super.testGetEntities();}
public void testGetEntitiesWithFromId() {super.testGetEntitiesWithFromId();}
public void testGetEntitiesWithFromTs() {super.testGetEntitiesWithFromTs();}
public void testGetEntitiesWithPrimaryFilters() {super.testGetEntitiesWithPrimaryFilters();}
public void testGetEntitiesWithSecondaryFilters() {super.testGetEntitiesWithSecondaryFilters();}
public void testGetEvents() {super.testGetEvents();}public String toString() {return getClass().getSimpleName() + "(action=" + action + ", delayMillis=" + delayMillis + ", reason=" + reason + ")";}public static ApplicationFinishData newInstance(ApplicationId applicationId, long finishTime, String diagnosticsInfo, FinalApplicationStatus finalApplicationStatus, YarnApplicationState yarnApplicationState) {ApplicationFinishData appFD = Records.newRecord(ApplicationFinishData.class);appFD.setApplicationId(applicationId);appFD.setFinishTime(finishTime);appFD.setDiagnosticsInfo(diagnosticsInfo);appFD.setFinalApplicationStatus(finalApplicationStatus);appFD.setYarnApplicationState(yarnApplicationState);return appFD;}public void setUp() {UtilTest.recursiveDelete(TEST_DIR);assertTrue(TEST_DIR.mkdirs());FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());out.write("hello\n".getBytes());out.close();}
public void runStreamJob() {boolean mayExit = false;int returnStatus = 0;StreamJob job = new StreamJob(args, mayExit);returnStatus = job.go();assertEquals("Streaming Job expected to succeed", 0, returnStatus);job.running_.killJob();job.running_.waitForCompletion();}
public void testBackgroundSubmitOk() {runStreamJob();}public void initialize(InputSplit split, TaskAttemptContext context) {// was createdassert fileSplitIsValid(context);delegate.initialize(fileSplit, context);}
private boolean fileSplitIsValid(TaskAttemptContext context) {Configuration conf = context.getConfiguration();long offset = conf.getLong(MRJobConfig.MAP_INPUT_START, 0L);if (fileSplit.getStart() != offset) {return false;}long length = conf.getLong(MRJobConfig.MAP_INPUT_PATH, 0L);if (fileSplit.getLength() != length) {return false;}String path = conf.get(MRJobConfig.MAP_INPUT_FILE);if (!fileSplit.getPath().toString().equals(path)) {return false;}return true;}
public boolean nextKeyValue() {return delegate.nextKeyValue();}
public K getCurrentKey() {return delegate.getCurrentKey();}
public V getCurrentValue() {return delegate.getCurrentValue();}
public float getProgress() {return delegate.getProgress();}
public void close() {delegate.close();}void start() {}
void finish() {output();super.finish();}
void finishAbnormally() {System.out.println("*** Image processing finished abnormally.  Ending ***");output();super.finishAbnormally();}
private void output() {// write the distribution into the output filewrite("Size\tNumFiles\n");for (int i = 0; i < distribution.length; i++) write(((long) i * step) + "\t" + distribution[i] + "\n");System.out.println("totalFiles = " + totalFiles);System.out.println("totalDirectories = " + totalDirectories);System.out.println("totalBlocks = " + totalBlocks);System.out.println("totalSpace = " + totalSpace);System.out.println("maxFileSize = " + maxFileSize);}
void leaveEnclosingElement() {ImageElement elem = elemS.pop();if (elem != ImageElement.INODE && elem != ImageElement.INODE_UNDER_CONSTRUCTION)return;inInode = false;if (current.numBlocks < 0) {totalDirectories++;return;}totalFiles++;totalBlocks += current.numBlocks;totalSpace += current.fileSize * current.replication;if (maxFileSize < current.fileSize)maxFileSize = current.fileSize;int high;if (current.fileSize > maxSize)high = distribution.length - 1;elsehigh = (int) Math.ceil((double) current.fileSize / step);distribution[high]++;if (totalFiles % 1000000 == 1)System.out.println("Files processed: " + totalFiles + "  Current: " + current.path);}
void visit(ImageElement element, String value) {if (inInode) {switch(element) {case INODE_PATH:current.path = (value.equals("") ? "/" : value);break;case REPLICATION:current.replication = Integer.valueOf(value);break;case NUM_BYTES:current.fileSize += Long.valueOf(value);break;default:break;}}}
void visitEnclosingElement(ImageElement element) {elemS.push(element);if (element == ImageElement.INODE || element == ImageElement.INODE_UNDER_CONSTRUCTION) {current = new FileContext();inInode = true;}}
void visitEnclosingElement(ImageElement element, ImageElement key, String value) {elemS.push(element);if (element == ImageElement.INODE || element == ImageElement.INODE_UNDER_CONSTRUCTION)inInode = true;else if (element == ImageElement.BLOCKS)current.numBlocks = Integer.parseInt(value);}public boolean next(GridmixKey key, GridmixRecord val) {if (!factory.next(key, val)) {return false;}for (int len = (null == key ? 0 : key.getSize()) + val.getSize(); len > 0; len -= buf.length) {IOUtils.readFully(src, buf, 0, Math.min(buf.length, len));}return true;}
public float getProgress() {return factory.getProgress();}
public void close() {IOUtils.cleanup(null, src);factory.close();}public void testClusterWithYarnClientProtocolProvider() {Configuration conf = new Configuration(false);Cluster cluster = null;try {cluster = new Cluster(conf);} catch (Exception e) {throw new Exception("Failed to initialize a local runner w/o a cluster framework key", e);}try {assertTrue("client is not a LocalJobRunner", cluster.getClient() instanceof LocalJobRunner);} finally {if (cluster != null) {cluster.close();}}try {conf = new Configuration();conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);cluster = new Cluster(conf);ClientProtocol client = cluster.getClient();assertTrue("client is a YARNRunner", client instanceof YARNRunner);} catch (IOException e) {} finally {if (cluster != null) {cluster.close();}}}
public void testClusterGetDelegationToken() {Configuration conf = new Configuration(false);Cluster cluster = null;try {conf = new Configuration();conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);cluster = new Cluster(conf);YARNRunner yrunner = (YARNRunner) cluster.getClient();GetDelegationTokenResponse getDTResponse = recordFactory.newRecordInstance(GetDelegationTokenResponse.class);org.apache.hadoop.yarn.api.records.Token rmDTToken = recordFactory.newRecordInstance(org.apache.hadoop.yarn.api.records.Token.class);rmDTToken.setIdentifier(ByteBuffer.wrap(new byte[2]));rmDTToken.setKind("Testclusterkind");rmDTToken.setPassword(ByteBuffer.wrap("testcluster".getBytes()));rmDTToken.setService("0.0.0.0:8032");getDTResponse.setRMDelegationToken(rmDTToken);final ApplicationClientProtocol cRMProtocol = mock(ApplicationClientProtocol.class);when(cRMProtocol.getDelegationToken(any(GetDelegationTokenRequest.class))).thenReturn(getDTResponse);ResourceMgrDelegate rmgrDelegate = new ResourceMgrDelegate(new YarnConfiguration(conf)) {
@Overrideprotected void serviceStart() throws Exception {assertTrue(this.client instanceof YarnClientImpl);((YarnClientImpl) this.client).setRMClient(cRMProtocol);}};yrunner.setResourceMgrDelegate(rmgrDelegate);Token t = cluster.getDelegationToken(new Text(" "));assertTrue("Token kind is instead " + t.getKind().toString(), "Testclusterkind".equals(t.getKind().toString()));} finally {if (cluster != null) {cluster.close();}}}
protected void serviceStart() {assertTrue(this.client instanceof YarnClientImpl);((YarnClientImpl) this.client).setRMClient(cRMProtocol);}public boolean getTransferStateFromPreviousAttempt() {return transferStateFromPreviousAttempt;}public MiniDFSCluster startCluster() {cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(3).build();return cluster;}
public FileSystem getFailoverFs() {return HATestUtil.configureFailoverFs(cluster, conf);}
public void addReplicationTriggerThread(final int interval) {testCtx.addThread(new RepeatingTestThread(testCtx) {
@Overridepublic void doAnAction() throws Exception {for (DataNode dn : cluster.getDataNodes()) {DataNodeTestUtils.triggerDeletionReport(dn);DataNodeTestUtils.triggerHeartbeat(dn);}for (int i = 0; i < 2; i++) {NameNode nn = cluster.getNameNode(i);BlockManagerTestUtil.computeAllPendingWork(nn.getNamesystem().getBlockManager());}Thread.sleep(interval);}});}
public void doAnAction() {for (DataNode dn : cluster.getDataNodes()) {DataNodeTestUtils.triggerDeletionReport(dn);DataNodeTestUtils.triggerHeartbeat(dn);}for (int i = 0; i < 2; i++) {NameNode nn = cluster.getNameNode(i);BlockManagerTestUtil.computeAllPendingWork(nn.getNamesystem().getBlockManager());}Thread.sleep(interval);}
public void addFailoverThread(final int msBetweenFailovers) {testCtx.addThread(new RepeatingTestThread(testCtx) {
@Overridepublic void doAnAction() throws Exception {System.err.println("==============================\n" + "Failing over from 0->1\n" + "==================================");cluster.transitionToStandby(0);cluster.transitionToActive(1);Thread.sleep(msBetweenFailovers);System.err.println("==============================\n" + "Failing over from 1->0\n" + "==================================");cluster.transitionToStandby(1);cluster.transitionToActive(0);Thread.sleep(msBetweenFailovers);}});}
public void doAnAction() {System.err.println("==============================\n" + "Failing over from 0->1\n" + "==================================");cluster.transitionToStandby(0);cluster.transitionToActive(1);Thread.sleep(msBetweenFailovers);System.err.println("==============================\n" + "Failing over from 1->0\n" + "==================================");cluster.transitionToStandby(1);cluster.transitionToActive(0);Thread.sleep(msBetweenFailovers);}
public void startThreads() {this.testCtx.startThreads();}
public void stopThreads() {this.testCtx.stop();}
public void shutdown() {this.testCtx.stop();if (cluster != null) {this.cluster.shutdown();cluster = null;}}EstimateVector incorporate(float newProgress, long newAtTime) {if (newAtTime <= atTime || newProgress < basedOnProgress) {return this;}double oldWeighting = value < 0.0 ? 0.0 : Math.exp(((double) (newAtTime - atTime)) / lambda);double newRead = (newProgress - basedOnProgress) / (newAtTime - atTime);if (smoothedValue == SmoothedValue.TIME_PER_UNIT_PROGRESS) {newRead = 1.0 / newRead;}return new EstimateVector(value * oldWeighting + newRead * (1.0 - oldWeighting), newProgress, newAtTime);}
private void incorporateReading(TaskAttemptId attemptID, float newProgress, long newTime) {//TODO: Refactor this method, it seems more complicated than necessary.AtomicReference<EstimateVector> vectorRef = estimates.get(attemptID);if (vectorRef == null) {estimates.putIfAbsent(attemptID, new AtomicReference<EstimateVector>(null));incorporateReading(attemptID, newProgress, newTime);return;}EstimateVector oldVector = vectorRef.get();if (oldVector == null) {if (vectorRef.compareAndSet(null, new EstimateVector(-1.0, 0.0F, Long.MIN_VALUE))) {return;}incorporateReading(attemptID, newProgress, newTime);return;}while (!vectorRef.compareAndSet(oldVector, oldVector.incorporate(newProgress, newTime))) {oldVector = vectorRef.get();}}
private EstimateVector getEstimateVector(TaskAttemptId attemptID) {AtomicReference<EstimateVector> vectorRef = estimates.get(attemptID);if (vectorRef == null) {return null;}return vectorRef.get();}
public void contextualize(Configuration conf, AppContext context) {super.contextualize(conf, context);lambda = conf.getLong(MRJobConfig.MR_AM_TASK_ESTIMATOR_SMOOTH_LAMBDA_MS, MRJobConfig.DEFAULT_MR_AM_TASK_ESTIMATOR_SMOOTH_LAMBDA_MS);smoothedValue = conf.getBoolean(MRJobConfig.MR_AM_TASK_ESTIMATOR_EXPONENTIAL_RATE_ENABLE, true) ? SmoothedValue.RATE : SmoothedValue.TIME_PER_UNIT_PROGRESS;}
public long estimatedRuntime(TaskAttemptId id) {Long startTime = startTimes.get(id);if (startTime == null) {return -1L;}EstimateVector vector = getEstimateVector(id);if (vector == null) {return -1L;}long sunkTime = vector.atTime - startTime;double value = vector.value;float progress = vector.basedOnProgress;if (value == 0) {return -1L;}double rate = smoothedValue == SmoothedValue.RATE ? value : 1.0 / value;if (rate == 0.0) {return -1L;}double remainingTime = (1.0 - progress) / rate;return sunkTime + (long) remainingTime;}
public long runtimeEstimateVariance(TaskAttemptId id) {return -1L;}
public void updateAttempt(TaskAttemptStatus status, long timestamp) {super.updateAttempt(status, timestamp);TaskAttemptId attemptID = status.id;float progress = status.progress;incorporateReading(attemptID, progress, timestamp);}public void setup() {map.add(bpid, new FinalizedReplica(block, null, null));}
public void testGet() {// Test 1: null argument throws invalid argument exceptiontry {map.get(bpid, null);fail("Expected exception not thrown");} catch (IllegalArgumentException expected) {}// Test 2: successful lookup based on blockassertNotNull(map.get(bpid, block));// Test 3: Lookup failure - generation stamp mismatch Block b = new Block(block);b.setGenerationStamp(0);assertNull(map.get(bpid, b));// Test 4: Lookup failure - blockID mismatchb.setGenerationStamp(block.getGenerationStamp());b.setBlockId(0);assertNull(map.get(bpid, b));// Test 5: successful lookup based on block IDassertNotNull(map.get(bpid, block.getBlockId()));// Test 6: failed lookup for invalid block IDassertNull(map.get(bpid, 0));}
public void testAdd() {// Test 1: null argument throws invalid argument exceptiontry {map.add(bpid, null);fail("Expected exception not thrown");} catch (IllegalArgumentException expected) {}}
public void testRemove() {// Test 1: null argument throws invalid argument exceptiontry {map.remove(bpid, null);fail("Expected exception not thrown");} catch (IllegalArgumentException expected) {}// Test 2: remove failure - generation stamp mismatch Block b = new Block(block);b.setGenerationStamp(0);assertNull(map.remove(bpid, b));// Test 3: remove failure - blockID mismatchb.setGenerationStamp(block.getGenerationStamp());b.setBlockId(0);assertNull(map.remove(bpid, b));// Test 4: remove successassertNotNull(map.remove(bpid, block));// Test 5: remove failure - invalid blockIDassertNull(map.remove(bpid, 0));// Test 6: remove successmap.add(bpid, new FinalizedReplica(block, null, null));assertNotNull(map.remove(bpid, block.getBlockId()));}public void setConf(Configuration conf) {if (conf != null) {factor = conf.getFloat("mapred.newjobweightbooster.factor", DEFAULT_FACTOR);duration = conf.getLong("mapred.newjobweightbooster.duration", DEFAULT_DURATION);}super.setConf(conf);}
public double adjustWeight(FSAppAttempt app, double curWeight) {long start = app.getStartTime();long now = System.currentTimeMillis();if (now - start < duration) {return curWeight * factor;} else {return curWeight;}}public synchronized void set(final int newValue) {value = newValue;changed = true;}
public synchronized int get() {return value;}
public synchronized void pushMetric(final MetricsRecord mr) {if (changed) {try {mr.setMetric(getName(), value);} catch (Exception e) {LOG.info("pushMetric failed for " + getName() + "\n", e);}}changed = false;}protected void setUp() {Configuration conf = new Configuration();store = getFileSystemStore();fs = new S3FileSystem(store);fs.initialize(URI.create(conf.get("test.fs.s3.name")), conf);}
protected void tearDown() {store.purge();super.tearDown();}
public void testBlockSize() {Path file = path("/test/hadoop/file");long newBlockSize = fs.getDefaultBlockSize(file) * 2;fs.getConf().setLong("fs.s3.block.size", newBlockSize);createFile(file);assertEquals("Double default block size", newBlockSize, fs.getFileStatus(file).getBlockSize());}
public void testCanonicalName() {assertNull("s3 doesn't support security token and shouldn't have canonical name", fs.getCanonicalServiceName());}public String getDiagnosticMessage() {return diagnosticMesage;}public static Connection getConnection() {Connection connection = mock(FakeConnection.class);try {Statement statement = mock(Statement.class);ResultSet results = mock(ResultSet.class);when(results.getLong(1)).thenReturn(15L);when(statement.executeQuery(any(String.class))).thenReturn(results);when(connection.createStatement()).thenReturn(statement);DatabaseMetaData metadata = mock(DatabaseMetaData.class);when(metadata.getDatabaseProductName()).thenReturn("Test");when(connection.getMetaData()).thenReturn(metadata);PreparedStatement reparedStatement0 = mock(PreparedStatement.class);when(connection.prepareStatement(anyString())).thenReturn(reparedStatement0);PreparedStatement preparedStatement = mock(PreparedStatement.class);ResultSet resultSet = mock(ResultSet.class);when(resultSet.next()).thenReturn(false);when(preparedStatement.executeQuery()).thenReturn(resultSet);when(connection.prepareStatement(anyString(), anyInt(), anyInt())).thenReturn(preparedStatement);} catch (SQLException e) {;}return connection;}
public boolean acceptsURL(String arg0) {return "testUrl".equals(arg0);}
public Connection connect(String arg0, Properties arg1) {return getConnection();}
public int getMajorVersion() {return 1;}
public int getMinorVersion() {return 1;}
public DriverPropertyInfo[] getPropertyInfo(String arg0, Properties arg1) {return null;}
public boolean jdbcCompliant() {return true;}
public Logger getParentLogger() {throw new SQLFeatureNotSupportedException();}public Writable getData() {return data;}
public void write(DataOutput out) {this.tag.write(out);this.data.write(out);}
public void readFields(DataInput in) {this.tag.readFields(in);this.data.readFields(in);}public ZombieJob getNextJob() {LoggedJob job = reader.getNext();if (job == null) {return null;} else if (hasRandomSeed) {long subRandomSeed = RandomSeedGenerator.getSeed("forZombieJob" + job.getJobID(), randomSeed);return new ZombieJob(job, cluster, subRandomSeed);} else {return new ZombieJob(job, cluster);}}
public void close() {reader.close();}public void setup() {errContent.reset();initialStdErr = System.err;System.setErr(new PrintStream(errContent));conf = new Configuration();}
public void cleanUp() {errContent.reset();System.setErr(initialStdErr);}
public void testGetfattrValidations() {errContent.reset();assertFalse("getfattr should fail without path", 0 == runCommand(new String[] { "-getfattr", "-d" }));assertTrue(errContent.toString().contains("<path> is missing"));errContent.reset();assertFalse("getfattr should fail with extra argument", 0 == runCommand(new String[] { "-getfattr", "extra", "-d", "/test" }));assertTrue(errContent.toString().contains("Too many arguments"));errContent.reset();assertFalse("getfattr should fail without \"-n name\" or \"-d\"", 0 == runCommand(new String[] { "-getfattr", "/test" }));assertTrue(errContent.toString().contains("Must specify '-n name' or '-d' option"));errContent.reset();assertFalse("getfattr should fail with invalid encoding", 0 == runCommand(new String[] { "-getfattr", "-d", "-e", "aaa", "/test" }));assertTrue(errContent.toString().contains("Invalid/unsupported encoding option specified: aaa"));}
public void testSetfattrValidations() {errContent.reset();assertFalse("setfattr should fail without path", 0 == runCommand(new String[] { "-setfattr", "-n", "user.a1" }));assertTrue(errContent.toString().contains("<path> is missing"));errContent.reset();assertFalse("setfattr should fail with extra arguments", 0 == runCommand(new String[] { "-setfattr", "extra", "-n", "user.a1", "/test" }));assertTrue(errContent.toString().contains("Too many arguments"));errContent.reset();assertFalse("setfattr should fail without \"-n name\" or \"-x name\"", 0 == runCommand(new String[] { "-setfattr", "/test" }));assertTrue(errContent.toString().contains("Must specify '-n name' or '-x name' option"));}
private int runCommand(String[] commands) {return ToolRunner.run(conf, new FsShell(), commands);}public static ContainerFinishData newInstance(ContainerId containerId, long finishTime, String diagnosticsInfo, int containerExitCode, ContainerState containerState) {ContainerFinishData containerFD = Records.newRecord(ContainerFinishData.class);containerFD.setContainerId(containerId);containerFD.setFinishTime(finishTime);containerFD.setDiagnosticsInfo(diagnosticsInfo);containerFD.setContainerExitStatus(containerExitCode);containerFD.setContainerState(containerState);return containerFD;}public void write(int b) {}
public String toString() {return "";}
public void setCmd(String cmd) {this.cmd = "-" + cmd.trim();}
public void setArgs(String args) {for (String s : args.trim().split("\\s*,\\s*")) argv.add(s);}
public void setOut(String outprop) {this.outprop = outprop;out = new ByteArrayOutputStream();if (outprop.equals(errprop))err = out;}
public void setErr(String errprop) {this.errprop = errprop;err = (errprop.equals(outprop)) ? err = out : new ByteArrayOutputStream();}
public void setConf(String confpath) {confloader = new AntClassLoader(getClass().getClassLoader(), false);confloader.setProject(getProject());if (null != confpath)confloader.addPathElement(confpath);}
public void setFailonerror(boolean failonerror) {this.failonerror = failonerror;}
protected void pushContext() {antOut = System.out;antErr = System.err;System.setOut(new PrintStream(out));System.setErr(out == err ? System.out : new PrintStream(err));}
protected void popContext() {// write output to property, if applicableif (outprop != null && !System.out.checkError())getProject().setNewProperty(outprop, out.toString());if (out != err && errprop != null && !System.err.checkError())getProject().setNewProperty(errprop, err.toString());System.setErr(antErr);System.setOut(antOut);confloader.cleanup();confloader.setParent(null);}
protected int postCmd(int exit_code) {if ("-test".equals(cmd) && exit_code != 0)outprop = null;return exit_code;}
public void execute() {if (null == cmd)throw new BuildException("Missing command (cmd) argument");argv.add(0, cmd);if (null == confloader) {setConf(getProject().getProperty("hadoop.conf.dir"));}int exit_code = 0;try {pushContext();Configuration conf = new HdfsConfiguration();conf.setClassLoader(confloader);exit_code = ToolRunner.run(conf, shell, argv.toArray(new String[argv.size()]));exit_code = postCmd(exit_code);if (0 > exit_code) {StringBuilder msg = new StringBuilder();for (String s : argv) msg.append(s + " ");msg.append("failed: " + exit_code);throw new Exception(msg.toString());}} catch (Exception e) {if (failonerror)throw new BuildException(e);} finally {popContext();}}public float getCapacity() {return this.capacity;}
public float getUsedCapacity() {return this.usedCapacity;}
public float getMaxCapacity() {return this.maxCapacity;}
public float getAbsoluteCapacity() {return absoluteCapacity;}
public float getAbsoluteMaxCapacity() {return absoluteMaxCapacity;}
public float getAbsoluteUsedCapacity() {return absoluteUsedCapacity;}
public int getNumApplications() {return numApplications;}
public String getQueueName() {return this.queueName;}
public String getQueueState() {return this.state.toString();}
public String getQueuePath() {return this.queuePath;}
public CapacitySchedulerQueueInfoList getQueues() {return this.queues;}
public ResourceInfo getResourcesUsed() {return resourcesUsed;}
static float cap(float val, float low, float hi) {return Math.min(Math.max(val, low), hi);}public void testDfsUrls() {Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();FileSystem fs = cluster.getFileSystem();// in TestStreamHandlerFsUrlStreamHandlerFactory factory = new org.apache.hadoop.fs.FsUrlStreamHandlerFactory();java.net.URL.setURLStreamHandlerFactory(factory);Path filePath = new Path("/thefile");try {byte[] fileContent = new byte[1024];for (int i = 0; i < fileContent.length; ++i) fileContent[i] = (byte) i;OutputStream os = fs.create(filePath);os.write(fileContent);os.close();URI uri = fs.getUri();URL fileURL = new URL(uri.getScheme(), uri.getHost(), uri.getPort(), filePath.toString());InputStream is = fileURL.openStream();assertNotNull(is);byte[] bytes = new byte[4096];assertEquals(1024, is.read(bytes));is.close();for (int i = 0; i < fileContent.length; ++i) assertEquals(fileContent[i], bytes[i]);// Cleanup: delete the filefs.delete(filePath, false);} finally {fs.close();cluster.shutdown();}}
public void testFileUrls() {// URLStreamHandler is already set in JVM by testDfsUrls() Configuration conf = new HdfsConfiguration();// Locate the test temporary directory.if (!TEST_ROOT_DIR.exists()) {if (!TEST_ROOT_DIR.mkdirs())throw new IOException("Cannot create temporary directory: " + TEST_ROOT_DIR);}File tmpFile = new File(TEST_ROOT_DIR, "thefile");URI uri = tmpFile.toURI();FileSystem fs = FileSystem.get(uri, conf);try {byte[] fileContent = new byte[1024];for (int i = 0; i < fileContent.length; ++i) fileContent[i] = (byte) i;OutputStream os = fs.create(new Path(uri.getPath()));os.write(fileContent);os.close();URL fileURL = uri.toURL();InputStream is = fileURL.openStream();assertNotNull(is);byte[] bytes = new byte[4096];assertEquals(1024, is.read(bytes));is.close();for (int i = 0; i < fileContent.length; ++i) assertEquals(fileContent[i], bytes[i]);// Cleanup: delete the filefs.delete(new Path(uri.getPath()), false);} finally {fs.close();}}public static void setUp() {config = new Configuration();if (DFS_BASE_DIR.exists() && !FileUtil.fullyDelete(DFS_BASE_DIR)) {throw new IOException("Could not delete hdfs directory '" + DFS_BASE_DIR + "'");}// could be wrong in the format prompting code. (HDFS-1636)LOG.info("hdfsdir is " + DFS_BASE_DIR.getAbsolutePath());File nameDir1 = new File(DFS_BASE_DIR, "name1");File nameDir2 = new File(DFS_BASE_DIR, "name2");// To test multiple directory handling, we pre-create one of the name directories.nameDir1.mkdirs();// Set multiple name directories.config.set(DFS_NAMENODE_NAME_DIR_KEY, nameDir1.getPath() + "," + nameDir2.getPath());config.set(DFS_DATANODE_DATA_DIR_KEY, new File(DFS_BASE_DIR, "data").getPath());config.set(DFS_NAMENODE_CHECKPOINT_DIR_KEY, new File(DFS_BASE_DIR, "secondary").getPath());FileSystem.setDefaultUri(config, "hdfs://" + NAME_NODE_HOST + "0");}
public static void tearDown() {if (cluster != null) {cluster.shutdown();LOG.info("Stopping mini cluster");}if (DFS_BASE_DIR.exists() && !FileUtil.fullyDelete(DFS_BASE_DIR)) {throw new IOException("Could not delete hdfs directory in tearDown '" + DFS_BASE_DIR + "'");}}
public void testAllowFormat() {LOG.info("--starting mini cluster");NameNode nn;// 1. Create a new cluster and format DFSconfig.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);cluster = new MiniDFSCluster.Builder(config).manageDataDfsDirs(false).manageNameDfsDirs(false).build();cluster.waitActive();assertNotNull(cluster);nn = cluster.getNameNode();assertNotNull(nn);LOG.info("Mini cluster created OK");// NOTE: the cluster must be shut down for format to work.LOG.info("Verifying format will fail with allowformat false");config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, false);try {cluster.shutdown();NameNode.format(config);fail("Format succeeded, when it should have failed");} catch (// expected to failIOException // expected to faile) {assertTrue("Exception was not about formatting Namenode", e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));LOG.info("Expected failure: " + StringUtils.stringifyException(e));LOG.info("Done verifying format will fail with allowformat false");}// 3. Try formatting DFS with allowformat trueLOG.info("Verifying format will succeed with allowformat true");config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);NameNode.format(config);LOG.info("Done verifying format will succeed with allowformat true");}
public void testFormatShouldBeIgnoredForNonFileBasedDirs() {Configuration conf = new HdfsConfiguration();String logicalName = "mycluster";// is considered.String localhost = "127.0.0.1";InetSocketAddress nnAddr1 = new InetSocketAddress(localhost, 8020);InetSocketAddress nnAddr2 = new InetSocketAddress(localhost, 9020);HATestUtil.setFailoverConfigurations(conf, logicalName, nnAddr1, nnAddr2);conf.set(DFS_NAMENODE_NAME_DIR_KEY, new File(DFS_BASE_DIR, "name").getAbsolutePath());conf.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY, true);conf.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_EDITS_PLUGIN_PREFIX, "dummy"), DummyJournalManager.class.getName());conf.set(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY, "dummy://" + localhost + ":2181/ledgers");conf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY, "nn1");// An internal assert is added to verify the working of testNameNode.format(conf);}public long getContentLength() {return contentLength;}
public HttpInputStreamWithRelease getInputStream() {return inputStream;}public static Verifier readFlavorAndVerifier(XDR xdr) {AuthFlavor flavor = AuthFlavor.fromValue(xdr.readInt());final Verifier verifer;if (flavor == AuthFlavor.AUTH_NONE) {verifer = new VerifierNone();} else if (flavor == AuthFlavor.RPCSEC_GSS) {verifer = new VerifierGSS();} else {throw new UnsupportedOperationException("Unsupported verifier flavor" + flavor);}verifer.read(xdr);return verifer;}
public static void writeFlavorAndVerifier(Verifier verifier, XDR xdr) {if (verifier instanceof VerifierNone) {xdr.writeInt(AuthFlavor.AUTH_NONE.getValue());} else if (verifier instanceof VerifierGSS) {xdr.writeInt(AuthFlavor.RPCSEC_GSS.getValue());} else {throw new UnsupportedOperationException("Cannot recognize the verifier");}verifier.write(xdr);}public static RefreshUserToGroupsMappingsRequest newInstance() {RefreshUserToGroupsMappingsRequest request = Records.newRecord(RefreshUserToGroupsMappingsRequest.class);return request;}public boolean contains(DatanodeID d) {return datanodes.contains(d.getName());}
public boolean contains(int n, DatanodeID d) {return d.getName().equals(datanodes.get(n));}
public String toString() {return getClass().getSimpleName() + datanodes;}public synchronized void snapshot(MetricsRecordBuilder builder, boolean all) {if (all || changed()) {builder.addGauge(numInfo, previousCount);for (int i = 0; i < quantiles.length; i++) {long newValue = 0;// If snapshot is null, we failed to update since the window was emptyif (previousSnapshot != null) {newValue = previousSnapshot.get(quantiles[i]);}builder.addGauge(quantileInfos[i], newValue);}if (changed()) {clearChanged();}}}
public synchronized void add(long value) {estimator.insert(value);}
public int getInterval() {return interval;}
public void run() {synchronized (parent) {parent.previousCount = parent.estimator.getCount();parent.previousSnapshot = parent.estimator.snapshot();parent.estimator.clear();}parent.setChanged();}protected void serviceStart() {String bindAddress = WebAppUtils.getWebAppBindURL(getConfig(), YarnConfiguration.NM_BIND_HOST, WebAppUtils.getNMWebAppURLWithoutScheme(getConfig()));LOG.info("Instantiating NMWebApp at " + bindAddress);try {this.webApp = WebApps.$for("node", Context.class, this.nmContext, "ws").at(bindAddress).with(getConfig()).withHttpSpnegoPrincipalKey(YarnConfiguration.NM_WEBAPP_SPNEGO_USER_NAME_KEY).withHttpSpnegoKeytabKey(YarnConfiguration.NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).start(this.nmWebApp);this.port = this.webApp.httpServer().getConnectorAddress(0).getPort();} catch (Exception e) {String msg = "NMWebapps failed to start.";LOG.error(msg, e);throw new YarnRuntimeException(msg, e);}super.serviceStart();}
public int getPort() {return this.port;}
protected void serviceStop() {if (this.webApp != null) {LOG.debug("Stopping webapp");this.webApp.stop();}super.serviceStop();}
public void setup() {bind(NMWebServices.class);bind(GenericExceptionHandler.class);bind(JAXBContextResolver.class);bind(ResourceView.class).toInstance(this.resourceView);bind(ApplicationACLsManager.class).toInstance(this.aclsManager);bind(LocalDirsHandlerService.class).toInstance(dirsHandler);route("/", NMController.class, "info");route("/node", NMController.class, "node");route("/allApplications", NMController.class, "allApplications");route("/allContainers", NMController.class, "allContainers");route(pajoin("/application", APPLICATION_ID), NMController.class, "application");route(pajoin("/container", CONTAINER_ID), NMController.class, "container");route(pajoin("/containerlogs", CONTAINER_ID, APP_OWNER, CONTAINER_LOG_TYPE), NMController.class, "logs");}public boolean equals(Object obj) {if (obj instanceof MetricsRecord) {final MetricsRecord other = (MetricsRecord) obj;return Objects.equal(timestamp(), other.timestamp()) && Objects.equal(name(), other.name()) && Objects.equal(description(), other.description()) && Objects.equal(tags(), other.tags()) && Iterables.elementsEqual(metrics(), other.metrics());}return false;}
public int hashCode() {return Objects.hashCode(name(), description(), tags());}
public String toString() {return Objects.toStringHelper(this).add("timestamp", timestamp()).add("name", name()).add("description", description()).add("tags", tags()).add("metrics", Iterables.toString(metrics())).toString();}public void startUpCluster(long splitThreshold) {conf = new HdfsConfiguration();conf.setLong(DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY, splitThreshold);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPL_FACTOR).build();fs = cluster.getFileSystem();bpid = cluster.getNamesystem().getBlockPoolId();}
public void shutDownCluster() {if (cluster != null) {fs.close();cluster.shutdown();cluster = null;}}
private void createFile(String filenamePrefix, int blockCount) {Path path = new Path("/" + filenamePrefix + ".dat");DFSTestUtil.createFile(fs, path, BLOCK_SIZE, blockCount * BLOCK_SIZE, BLOCK_SIZE, REPL_FACTOR, seed);}
private void verifyCapturedArguments(ArgumentCaptor<StorageBlockReport[]> captor, int expectedReportsPerCall, int expectedTotalBlockCount) {List<StorageBlockReport[]> listOfReports = captor.getAllValues();int numBlocksReported = 0;for (StorageBlockReport[] reports : listOfReports) {assertThat(reports.length, is(expectedReportsPerCall));for (StorageBlockReport report : reports) {BlockListAsLongs blockList = new BlockListAsLongs(report.getBlocks());numBlocksReported += blockList.getNumberOfBlocks();}}assert (numBlocksReported >= expectedTotalBlockCount);}
public void testAlwaysSplit() {startUpCluster(0);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(cluster.getStoragesPerDatanode())).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, 1, BLOCKS_IN_FILE);}
public void testCornerCaseUnderThreshold() {startUpCluster(BLOCKS_IN_FILE + 1);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(1)).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, cluster.getStoragesPerDatanode(), BLOCKS_IN_FILE);}
public void testCornerCaseAtThreshold() {startUpCluster(BLOCKS_IN_FILE);NameNode nn = cluster.getNameNode();DataNode dn = cluster.getDataNodes().get(0);// Create a file with a few blocks.createFile(GenericTestUtils.getMethodName(), BLOCKS_IN_FILE);// Insert a spy object for the NN RPC.DatanodeProtocolClientSideTranslatorPB nnSpy = DataNodeTestUtils.spyOnBposToNN(dn, nn);// object.DataNodeTestUtils.triggerBlockReport(dn);ArgumentCaptor<StorageBlockReport[]> captor = ArgumentCaptor.forClass(StorageBlockReport[].class);Mockito.verify(nnSpy, times(cluster.getStoragesPerDatanode())).blockReport(any(DatanodeRegistration.class), anyString(), captor.capture());verifyCapturedArguments(captor, 1, BLOCKS_IN_FILE);}public StepTracking clone() {StepTracking clone = new StepTracking();super.copy(clone);clone.count = new AtomicLong(count.get());clone.total = total;return clone;}public void setupCluster() {Configuration conf = new Configuration();MiniDFSNNTopology topology = new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf("ns1").addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(20001)).addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(20002)));cluster = new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(0).build();cluster.waitActive();nn0 = cluster.getNameNode(0);cluster.transitionToActive(0);cluster.shutdownNameNode(1);}
public void shutdownCluster() {if (cluster != null) {cluster.shutdown();}}
public void testSuccessfulBaseCase() {removeStandbyNameDirs();try {cluster.restartNameNode(1);fail("Did not throw");} catch (IOException ioe) {GenericTestUtils.assertExceptionContains("storage directory does not exist or is not accessible", ioe);}int rc = BootstrapStandby.run(new String[] { "-nonInteractive" }, cluster.getConfiguration(1));assertEquals(0, rc);// Should have copied over the namespace from the activeFSImageTestUtil.assertNNHasCheckpoints(cluster, 1, ImmutableList.of(0));FSImageTestUtil.assertNNFilesMatch(cluster);// We should now be able to start the standby successfully.cluster.restartNameNode(1);}
public void testDownloadingLaterCheckpoint() {// Roll edit logs a few times to inflate txidnn0.getRpcServer().rollEditLog();nn0.getRpcServer().rollEditLog();// Make checkpointNameNodeAdapter.enterSafeMode(nn0, false);NameNodeAdapter.saveNamespace(nn0);NameNodeAdapter.leaveSafeMode(nn0);long expectedCheckpointTxId = NameNodeAdapter.getNamesystem(nn0).getFSImage().getMostRecentCheckpointTxId();assertEquals(6, expectedCheckpointTxId);int rc = BootstrapStandby.run(new String[] { "-force" }, cluster.getConfiguration(1));assertEquals(0, rc);// Should have copied over the namespace from the activeFSImageTestUtil.assertNNHasCheckpoints(cluster, 1, ImmutableList.of((int) expectedCheckpointTxId));FSImageTestUtil.assertNNFilesMatch(cluster);// We should now be able to start the standby successfully.cluster.restartNameNode(1);}
public void testSharedEditsMissingLogs() {removeStandbyNameDirs();CheckpointSignature sig = nn0.getRpcServer().rollEditLog();assertEquals(3, sig.getCurSegmentTxId());// Should have created edits_1-2 in shared edits dirURI editsUri = cluster.getSharedEditsDir(0, 1);File editsDir = new File(editsUri);File editsSegment = new File(new File(editsDir, "current"), NNStorage.getFinalizedEditsFileName(1, 2));GenericTestUtils.assertExists(editsSegment);// Delete the segment.assertTrue(editsSegment.delete());// logs aren't available in the shared dir.LogCapturer logs = GenericTestUtils.LogCapturer.captureLogs(LogFactory.getLog(BootstrapStandby.class));try {int rc = BootstrapStandby.run(new String[] { "-force" }, cluster.getConfiguration(1));assertEquals(BootstrapStandby.ERR_CODE_LOGS_UNAVAILABLE, rc);} finally {logs.stopCapturing();}GenericTestUtils.assertMatches(logs.getOutput(), "FATAL.*Unable to read transaction ids 1-3 from the configured shared");}
public void testStandbyDirsAlreadyExist() {// Should not pass since standby dirs exist, force not givenint rc = BootstrapStandby.run(new String[] { "-nonInteractive" }, cluster.getConfiguration(1));assertEquals(BootstrapStandby.ERR_CODE_ALREADY_FORMATTED, rc);// Should pass with -forcerc = BootstrapStandby.run(new String[] { "-force" }, cluster.getConfiguration(1));assertEquals(0, rc);}
public void testOtherNodeNotActive() {cluster.transitionToStandby(0);int rc = BootstrapStandby.run(new String[] { "-force" }, cluster.getConfiguration(1));assertEquals(0, rc);}
private void removeStandbyNameDirs() {for (URI u : cluster.getNameDirs(1)) {assertTrue(u.getScheme().equals("file"));File dir = new File(u.getPath());LOG.info("Removing standby dir " + dir);assertTrue(FileUtil.fullyDelete(dir));}}public GetClusterMetricsResponseProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (this.yarnClusterMetrics != null) {builder.setClusterMetrics(convertToProtoFormat(this.yarnClusterMetrics));}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = GetClusterMetricsResponseProto.newBuilder(proto);}viaProto = false;}
public YarnClusterMetrics getClusterMetrics() {GetClusterMetricsResponseProtoOrBuilder p = viaProto ? proto : builder;if (this.yarnClusterMetrics != null) {return this.yarnClusterMetrics;}if (!p.hasClusterMetrics()) {return null;}this.yarnClusterMetrics = convertFromProtoFormat(p.getClusterMetrics());return this.yarnClusterMetrics;}
public void setClusterMetrics(YarnClusterMetrics clusterMetrics) {maybeInitBuilder();if (clusterMetrics == null)builder.clearClusterMetrics();this.yarnClusterMetrics = clusterMetrics;}
private YarnClusterMetricsPBImpl convertFromProtoFormat(YarnClusterMetricsProto p) {return new YarnClusterMetricsPBImpl(p);}
private YarnClusterMetricsProto convertToProtoFormat(YarnClusterMetrics t) {return ((YarnClusterMetricsPBImpl) t).getProto();}public ApplicationId getApplicationId() {return applicationId;}
public String getQueue() {return queue;}
public String getUser() {return user;}
public boolean getIsAppRecovering() {return isAppRecovering;}public void pipeline_02_03() {final Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);// create clusterfinal MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();try {//change the lease limits.cluster.setLeasePeriod(SOFT_LEASE_LIMIT, HARD_LEASE_LIMIT);//wait for the clustercluster.waitActive();final FileSystem fs = cluster.getFileSystem();final Path p = new Path(DIR, "file1");final int half = BLOCK_SIZE / 2;//   Do not close file yet.{final FSDataOutputStream out = fs.create(p, true, fs.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), (short) 3, BLOCK_SIZE);write(out, 0, half);//hflush((DFSOutputStream) out.getWrappedStream()).hflush();}//   of data can be read successfully.checkFile(p, half, conf);AppendTestUtil.LOG.info("leasechecker.interruptAndJoin()");((DistributedFileSystem) fs).dfs.getLeaseRenewer().interruptAndJoin();//c. On M1, append another half block of data.  Close file on M1.{//sleep to let the lease is expired.Thread.sleep(2 * SOFT_LEASE_LIMIT);final UserGroupInformation current = UserGroupInformation.getCurrentUser();final UserGroupInformation ugi = UserGroupInformation.createUserForTesting(current.getShortUserName() + "x", new String[] { "supergroup" });final DistributedFileSystem dfs = ugi.doAs(new PrivilegedExceptionAction<DistributedFileSystem>() {
@Overridepublic DistributedFileSystem run() throws Exception {return (DistributedFileSystem) FileSystem.newInstance(conf);}});final FSDataOutputStream out = append(dfs, p);write(out, 0, half);out.close();}//d. On M2, open file and read 1 block of data from it. Close file.checkFile(p, 2 * half, conf);} finally {cluster.shutdown();}}
public DistributedFileSystem run() {return (DistributedFileSystem) FileSystem.newInstance(conf);}
private static FSDataOutputStream append(FileSystem fs, Path p) {for (int i = 0; i < 10; i++) {try {return fs.append(p);} catch (RemoteException re) {if (re.getClassName().equals(RecoveryInProgressException.class.getName())) {AppendTestUtil.LOG.info("Will sleep and retry, i=" + i + ", p=" + p, re);Thread.sleep(1000);} elsethrow re;}}throw new IOException("Cannot append to " + p);}
static void checkFile(Path p, int expectedsize, final Configuration conf) {//open the file with another user accountfinal String username = UserGroupInformation.getCurrentUser().getShortUserName() + "_" + ++userCount;UserGroupInformation ugi = UserGroupInformation.createUserForTesting(username, new String[] { "supergroup" });final FileSystem fs = DFSTestUtil.getFileSystemAs(ugi, conf);final HdfsDataInputStream in = (HdfsDataInputStream) fs.open(p);//Check visible lengthAssert.assertTrue(in.getVisibleLength() >= expectedsize);//Able to read?for (int i = 0; i < expectedsize; i++) {Assert.assertEquals((byte) i, (byte) in.read());}in.close();}
private static void write(OutputStream out, int offset, int length) {final byte[] bytes = new byte[length];for (int i = 0; i < length; i++) {bytes[i] = (byte) (offset + i);}out.write(bytes);}public static GetNewApplicationResponse newInstance(ApplicationId applicationId, Resource minCapability, Resource maxCapability) {GetNewApplicationResponse response = Records.newRecord(GetNewApplicationResponse.class);response.setApplicationId(applicationId);response.setMaximumResourceCapability(maxCapability);return response;}public FailTaskAttemptResponseProto getProto() {proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = FailTaskAttemptResponseProto.newBuilder(proto);}viaProto = false;}public void cleanupJob(JobContext jobContext) {}
public void commitJob(JobContext jobContext) {cleanupJob(jobContext);}
public void abortJob(JobContext jobContext, int status) {cleanupJob(jobContext);}
public boolean isRecoverySupported() {return false;}
public boolean isRecoverySupported(JobContext jobContext) {return isRecoverySupported();}
public void recoverTask(TaskAttemptContext taskContext) {}
public final void setupJob(org.apache.hadoop.mapreduce.JobContext jobContext) {setupJob((JobContext) jobContext);}
public final void cleanupJob(org.apache.hadoop.mapreduce.JobContext context) {cleanupJob((JobContext) context);}
public final void commitJob(org.apache.hadoop.mapreduce.JobContext context) {commitJob((JobContext) context);}
public final void abortJob(org.apache.hadoop.mapreduce.JobContext context, org.apache.hadoop.mapreduce.JobStatus.State runState) {int state = JobStatus.getOldNewJobRunState(runState);if (state != JobStatus.FAILED && state != JobStatus.KILLED) {throw new IOException("Invalid job run state : " + runState.name());}abortJob((JobContext) context, state);}
public final void setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) {setupTask((TaskAttemptContext) taskContext);}
public final boolean needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) {return needsTaskCommit((TaskAttemptContext) taskContext);}
public final void commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) {commitTask((TaskAttemptContext) taskContext);}
public final void abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) {abortTask((TaskAttemptContext) taskContext);}
public final void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) {recoverTask((TaskAttemptContext) taskContext);}
public final boolean isRecoverySupported(org.apache.hadoop.mapreduce.JobContext context) {return isRecoverySupported((JobContext) context);}protected String getUserName() {try {return UserGroupInformation.getCurrentUser().getShortUserName();} catch (IOException ex) {throw new RuntimeException(ex);}}public ApplicationAttemptId getApplicationAttemptID() {return this.applicationAttemptId;}
public String getClientName() {return this.clientName.toString();}
public void write(DataOutput out) {out.writeLong(this.applicationAttemptId.getApplicationId().getClusterTimestamp());out.writeInt(this.applicationAttemptId.getApplicationId().getId());out.writeInt(this.applicationAttemptId.getAttemptId());this.clientName.write(out);}
public void readFields(DataInput in) {this.applicationAttemptId = ApplicationAttemptId.newInstance(ApplicationId.newInstance(in.readLong(), in.readInt()), in.readInt());this.clientName.readFields(in);}
public Text getKind() {return KIND_NAME;}
public UserGroupInformation getUser() {if (this.clientName == null) {return null;}return UserGroupInformation.createRemoteUser(this.clientName.toString());}
protected Text getKind() {return KIND_NAME;}public static NodeManagerMetrics create() {return create(DefaultMetricsSystem.instance());}
static NodeManagerMetrics create(MetricsSystem ms) {JvmMetrics.create("NodeManager", null, ms);return ms.register(new NodeManagerMetrics());}
public void launchedContainer() {containersLaunched.incr();}
public void completedContainer() {containersCompleted.incr();}
public void failedContainer() {containersFailed.incr();}
public void killedContainer() {containersKilled.incr();}
public void initingContainer() {containersIniting.incr();}
public void endInitingContainer() {containersIniting.decr();}
public void runningContainer() {containersRunning.incr();}
public void endRunningContainer() {containersRunning.decr();}
public void allocateContainer(Resource res) {allocatedContainers.incr();allocatedGB.incr(res.getMemory() / 1024);availableGB.decr(res.getMemory() / 1024);allocatedVCores.incr(res.getVirtualCores());availableVCores.decr(res.getVirtualCores());}
public void releaseContainer(Resource res) {allocatedContainers.decr();allocatedGB.decr(res.getMemory() / 1024);availableGB.incr(res.getMemory() / 1024);allocatedVCores.decr(res.getVirtualCores());availableVCores.incr(res.getVirtualCores());}
public void addResource(Resource res) {availableGB.incr(res.getMemory() / 1024);availableVCores.incr(res.getVirtualCores());}
public int getRunningContainers() {return containersRunning.value();}public static MD5Hash getTestHash() {MessageDigest digest = MessageDigest.getInstance("MD5");byte[] buffer = new byte[1024];RANDOM.nextBytes(buffer);digest.update(buffer);return new MD5Hash(digest.digest());}
public void testMD5Hash() {MD5Hash md5Hash = getTestHash();final MD5Hash md5Hash00 = new MD5Hash(D00);final MD5Hash md5HashFF = new MD5Hash(DFF);MD5Hash orderedHash = new MD5Hash(new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 });MD5Hash backwardHash = new MD5Hash(new byte[] { -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16 });MD5Hash closeHash1 = new MD5Hash(new byte[] { -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 });MD5Hash closeHash2 = new MD5Hash(new byte[] { -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 });// test i/oTestWritable.testWritable(md5Hash);TestWritable.testWritable(md5Hash00);TestWritable.testWritable(md5HashFF);// test equals()assertEquals(md5Hash, md5Hash);assertEquals(md5Hash00, md5Hash00);assertEquals(md5HashFF, md5HashFF);// test compareTo()assertTrue(md5Hash.compareTo(md5Hash) == 0);assertTrue(md5Hash00.compareTo(md5Hash) < 0);assertTrue(md5HashFF.compareTo(md5Hash) > 0);// test toString and string ctorassertEquals(md5Hash, new MD5Hash(md5Hash.toString()));assertEquals(md5Hash00, new MD5Hash(md5Hash00.toString()));assertEquals(md5HashFF, new MD5Hash(md5HashFF.toString()));assertEquals(0x01020304, orderedHash.quarterDigest());assertEquals(0xfffefdfc, backwardHash.quarterDigest());assertEquals(0x0102030405060708L, orderedHash.halfDigest());assertEquals(0xfffefdfcfbfaf9f8L, backwardHash.halfDigest());assertTrue("hash collision", closeHash1.hashCode() != closeHash2.hashCode());Thread t1 = new Thread() {
@Overridepublic void run() {for (int i = 0; i < 100; i++) {MD5Hash hash = new MD5Hash(DFF);assertEquals(hash, md5HashFF);}}};Thread t2 = new Thread() {
@Overridepublic void run() {for (int i = 0; i < 100; i++) {MD5Hash hash = new MD5Hash(D00);assertEquals(hash, md5Hash00);}}};t1.start();t2.start();t1.join();t2.join();}
public void run() {for (int i = 0; i < 100; i++) {MD5Hash hash = new MD5Hash(DFF);assertEquals(hash, md5HashFF);}}
public void run() {for (int i = 0; i < 100; i++) {MD5Hash hash = new MD5Hash(D00);assertEquals(hash, md5Hash00);}}
public void testFactoryReturnsClearedHashes() {// A stream that will throw an IOE after reading some bytesByteArrayInputStream failingStream = new ByteArrayInputStream("xxxx".getBytes()) {
@Overridepublic synchronized int read(byte[] b) throws IOException {int ret = super.read(b);if (ret <= 0) {throw new IOException("Injected fault");}return ret;}};final String TEST_STRING = "hello";// Calculate the correct digest for the test stringMD5Hash expectedHash = MD5Hash.digest(TEST_STRING);// Hashing again should give the same resultassertEquals(expectedHash, MD5Hash.digest(TEST_STRING));// Try to hash a stream which will fail halfway throughtry {MD5Hash.digest(failingStream);fail("didnt throw!");} catch (Exception e) {}// Make sure we get the same resultassertEquals(expectedHash, MD5Hash.digest(TEST_STRING));}
public synchronized int read(byte[] b) {int ret = super.read(b);if (ret <= 0) {throw new IOException("Injected fault");}return ret;}public static ResourceBundle getBundle(String bundleName) {return ResourceBundle.getBundle(bundleName.replace('$', '_'), Locale.getDefault(), Thread.currentThread().getContextClassLoader());}
public static synchronized T getValue(String bundleName, String key, String suffix, T defaultValue) {T value;try {ResourceBundle bundle = getBundle(bundleName);value = (T) bundle.getObject(getLookupKey(key, suffix));} catch (Exception e) {return defaultValue;}return value == null ? defaultValue : value;}
private static String getLookupKey(String key, String suffix) {if (suffix == null || suffix.isEmpty())return key;return key + suffix;}
public static String getCounterGroupName(String group, String defaultValue) {return getValue(group, "CounterGroupName", "", defaultValue);}
public static String getCounterName(String group, String counter, String defaultValue) {return getValue(group, counter, ".name", defaultValue);}public int hashCode() {int hash = loc.hashCode() ^ (int) ((timestamp >>> 32) ^ timestamp) * type.hashCode();if (pattern != null) {hash = hash ^ pattern.hashCode();}return hash;}
public boolean equals(Object o) {if (this == o) {return true;}if (!(o instanceof LocalResourceRequest)) {return false;}final LocalResourceRequest other = (LocalResourceRequest) o;String pattern = getPattern();String otherPattern = other.getPattern();boolean patternEquals = (pattern == null && otherPattern == null) || (pattern != null && otherPattern != null && pattern.equals(otherPattern));return getPath().equals(other.getPath()) && getTimestamp() == other.getTimestamp() && getType() == other.getType() && patternEquals;}
public int compareTo(LocalResourceRequest other) {if (this == other) {return 0;}int ret = getPath().compareTo(other.getPath());if (0 == ret) {ret = (int) (getTimestamp() - other.getTimestamp());if (0 == ret) {ret = getType().ordinal() - other.getType().ordinal();if (0 == ret) {String pattern = getPattern();String otherPattern = other.getPattern();if (pattern == null && otherPattern == null) {ret = 0;} else if (pattern == null) {ret = -1;} else if (otherPattern == null) {ret = 1;} else {ret = pattern.compareTo(otherPattern);}}}}return ret;}
public Path getPath() {return loc;}
public long getTimestamp() {return timestamp;}
public LocalResourceType getType() {return type;}
public URL getResource() {return ConverterUtils.getYarnUrlFromPath(loc);}
public long getSize() {return -1L;}
public LocalResourceVisibility getVisibility() {return visibility;}
public String getPattern() {return pattern;}
public void setResource(URL resource) {throw new UnsupportedOperationException();}
public void setSize(long size) {throw new UnsupportedOperationException();}
public void setTimestamp(long timestamp) {throw new UnsupportedOperationException();}
public void setType(LocalResourceType type) {throw new UnsupportedOperationException();}
public void setVisibility(LocalResourceVisibility visibility) {throw new UnsupportedOperationException();}
public void setPattern(String pattern) {throw new UnsupportedOperationException();}
public String toString() {StringBuilder sb = new StringBuilder();sb.append("{ ");sb.append(getPath().toString()).append(", ");sb.append(getTimestamp()).append(", ");sb.append(getType()).append(", ");sb.append(getPattern()).append(" }");return sb.toString();}public void testHarUriWithHaUriWithNoPort() {Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).nnTopology(MiniDFSNNTopology.simpleHATopology()).build();cluster.transitionToActive(0);HATestUtil.setFailoverConfigurations(cluster, conf);createEmptyHarArchive(HATestUtil.configureFailoverFs(cluster, conf), TEST_HAR_PATH);URI failoverUri = FileSystem.getDefaultUri(conf);Path p = new Path("har://hdfs-" + failoverUri.getAuthority() + TEST_HAR_PATH);p.getFileSystem(conf);} finally {cluster.shutdown();}}
private static void createEmptyHarArchive(FileSystem fs, Path p) {fs.mkdirs(p);OutputStream out = fs.create(new Path(p, "_masterindex"));out.write(Integer.toString(HarFileSystem.VERSION).getBytes());out.close();fs.create(new Path(p, "_index")).close();}public void setWeight(float weight) {for (int i = 0; i < weights.length; i++) {weights[i] = weight;}}
public void setWeight(ResourceType resourceType, float weight) {weights[resourceType.ordinal()] = weight;}
public float getWeight(ResourceType resourceType) {return weights[resourceType.ordinal()];}
public String toString() {StringBuffer sb = new StringBuffer();sb.append("<");for (int i = 0; i < ResourceType.values().length; i++) {if (i != 0) {sb.append(", ");}ResourceType resourceType = ResourceType.values()[i];sb.append(resourceType.name().toLowerCase());sb.append(String.format(" weight=%.1f", getWeight(resourceType)));}sb.append(">");return sb.toString();}public InputStream getDataIn() {return dataIn;}
public InputStream getChecksumIn() {return checksumIn;}
public void close() {IOUtils.closeStream(dataIn);IOUtils.closeStream(checksumIn);}public void setupConfAndServices() {conf = new Configuration();conf.set(ZKFailoverController.ZK_QUORUM_KEY, hostPort);this.cluster = new MiniZKFCCluster(conf, getServer(serverFactory));}
public void stopCluster() {if (cluster != null) {cluster.stop();}}
public void testExpireBackAndForth() {cluster.start();long st = Time.now();long runFor = STRESS_RUNTIME_SECS * 1000;int i = 0;while (Time.now() - st < runFor) {// flip flop the services back and forthint from = i % 2;int to = (i + 1) % 2;// Expire one service, it should fail over to the otherLOG.info("Failing over via expiration from " + from + " to " + to);cluster.expireAndVerifyFailover(from, to);i++;}}
public void testRandomExpirations() {cluster.start();long st = Time.now();long runFor = STRESS_RUNTIME_SECS * 1000;Random r = new Random();while (Time.now() - st < runFor) {cluster.getTestContext().checkException();int targetIdx = r.nextInt(2);ActiveStandbyElector target = cluster.getElector(targetIdx);long sessId = target.getZKSessionIdForTests();if (sessId != -1) {LOG.info(String.format("Expiring session %x for svc %d", sessId, targetIdx));getServer(serverFactory).closeSession(sessId);}Thread.sleep(r.nextInt(300));}}
public void testRandomHealthAndDisconnects() {long runFor = STRESS_RUNTIME_SECS * 1000;Mockito.doAnswer(new RandomlyThrow(0)).when(cluster.getService(0).proxy).monitorHealth();Mockito.doAnswer(new RandomlyThrow(1)).when(cluster.getService(1).proxy).monitorHealth();conf.setInt(CommonConfigurationKeys.HA_FC_ELECTOR_ZK_OP_RETRIES_KEY, 100);// setting up the mock.cluster.start();long st = Time.now();while (Time.now() - st < runFor) {cluster.getTestContext().checkException();serverFactory.closeAll();Thread.sleep(50);}}
public Object answer(InvocationOnMock invocation) {if (r.nextBoolean()) {LOG.info("Throwing an exception for svc " + svcIdx);throw new HealthCheckFailedException("random failure");}return invocation.callRealMethod();}public void setup() {test_root = new File(TEST_ROOT_DIR);test_root.mkdirs();}
public void after() {FileUtil.setWritable(test_root, true);FileUtil.fullyDelete(test_root);assertTrue(!test_root.exists());}
protected String[] getExecString() {return new String[] { "echo", "IGNORE\n", "/dev/sda3", "453115160", "53037920", "400077240", "11%", "/foo/bar\n" };}
public void testMount() {XXDF df = new XXDF();String expectedMount = Shell.WINDOWS ? df.getDirPath().substring(0, 2) : "/foo/bar";assertEquals("Invalid mount point", expectedMount, df.getMount());}
public void testFileSystem() {XXDF df = new XXDF();String expectedFileSystem = Shell.WINDOWS ? df.getDirPath().substring(0, 2) : "/dev/sda3";assertEquals("Invalid filesystem", expectedFileSystem, df.getFilesystem());}
public void testDFInvalidPath() {Random random = new Random(0xDEADBEEFl);File file = null;byte[] bytes = new byte[64];while (file == null) {random.nextBytes(bytes);final String invalid = new String("/" + bytes);final File invalidFile = new File(invalid);if (!invalidFile.exists()) {file = invalidFile;}}DF df = new DF(file, 0l);try {df.getMount();} catch (FileNotFoundException e) {GenericTestUtils.assertExceptionContains(file.getName(), e);}}
public void testDFMalformedOutput() {DF df = new DF(new File("/"), 0l);BufferedReader reader = new BufferedReader(new StringReader("Filesystem 1K-blocks Used Available Use% Mounted on\n" + "/dev/sda5   19222656 10597036   7649060  59% /"));df.parseExecResult(reader);df.parseOutput();reader = new BufferedReader(new StringReader("Filesystem 1K-blocks Used Available Use% Mounted on"));df.parseExecResult(reader);try {df.parseOutput();fail("Expected exception with missing line!");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Fewer lines of output than expected", e);System.out.println(e.toString());}reader = new BufferedReader(new StringReader("Filesystem 1K-blocks Used Available Use% Mounted on\n" + " "));df.parseExecResult(reader);try {df.parseOutput();fail("Expected exception with empty line!");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Unexpected empty line", e);System.out.println(e.toString());}reader = new BufferedReader(new StringReader("Filesystem 1K-blocks Used Available Use% Mounted on\n" + "   19222656 10597036   7649060  59% /"));df.parseExecResult(reader);try {df.parseOutput();fail("Expected exception with missing field!");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Could not parse line: ", e);System.out.println(e.toString());}}
public void testGetMountCurrentDirectory() {File currentDirectory = new File(".");String workingDir = currentDirectory.getAbsoluteFile().getCanonicalPath();DF df = new DF(new File(workingDir), 0L);String mountPath = df.getMount();File mountDir = new File(mountPath);assertTrue("Mount dir [" + mountDir.getAbsolutePath() + "] should exist.", mountDir.exists());assertTrue("Mount dir [" + mountDir.getAbsolutePath() + "] should be directory.", mountDir.isDirectory());assertTrue("Working dir [" + workingDir + "] should start with [" + mountPath + "].", workingDir.startsWith(mountPath));}public synchronized void reserveResource(SchedulerApplicationAttempt application, Priority priority, RMContainer container) {// Check if it's already reservedRMContainer reservedContainer = getReservedContainer();if (reservedContainer != null) {// Sanity checkif (!container.getContainer().getNodeId().equals(getNodeID())) {throw new IllegalStateException("Trying to reserve" + " container " + container + " on node " + container.getReservedNode() + " when currently" + " reserved resource " + reservedContainer + " on node " + reservedContainer.getReservedNode());}// Cannot reserve more than one application on a given node!if (!reservedContainer.getContainer().getId().getApplicationAttemptId().equals(container.getContainer().getId().getApplicationAttemptId())) {throw new IllegalStateException("Trying to reserve" + " container " + container + " for application " + application.getApplicationId() + " when currently" + " reserved container " + reservedContainer + " on node " + this);}LOG.info("Updated reserved container " + container.getContainer().getId() + " on node " + this + " for application " + application);} else {LOG.info("Reserved container " + container.getContainer().getId() + " on node " + this + " for application " + application);}setReservedContainer(container);this.reservedAppSchedulable = (FSAppAttempt) application;}
public synchronized void unreserveResource(SchedulerApplicationAttempt application) {// Cannot unreserve for wrong application...ApplicationAttemptId reservedApplication = getReservedContainer().getContainer().getId().getApplicationAttemptId();if (!reservedApplication.equals(application.getApplicationAttemptId())) {throw new IllegalStateException("Trying to unreserve " + " for application " + application.getApplicationId() + " when currently reserved " + " for application " + reservedApplication.getApplicationId() + " on node " + this);}setReservedContainer(null);this.reservedAppSchedulable = null;}
public synchronized FSAppAttempt getReservedAppSchedulable() {return reservedAppSchedulable;}protected AbstractFSContract createContract(Configuration conf) {return new NativeS3Contract(conf);}public String getContainerId() {return containerId;}
public int getAllocatedMB() {return allocatedMB;}
public int getAllocatedVCores() {return allocatedVCores;}
public String getAssignedNodeId() {return assignedNodeId;}
public int getPriority() {return priority;}
public long getStartedTime() {return startedTime;}
public long getFinishedTime() {return finishedTime;}
public long getElapsedTime() {return elapsedTime;}
public String getDiagnosticsInfo() {return diagnosticsInfo;}
public String getLogUrl() {return logUrl;}
public int getContainerExitStatus() {return containerExitStatus;}
public ContainerState getContainerState() {return containerState;}private InetSocketAddress getActiveNodeAddress() {Configuration activeConf = HAUtil.getConfForOtherNode(conf);return NameNode.getServiceAddress(activeConf, true);}
private NamenodeProtocol getActiveNodeProxy() {if (cachedActiveProxy == null) {int rpcTimeout = conf.getInt(DFSConfigKeys.DFS_HA_LOGROLL_RPC_TIMEOUT_KEY, DFSConfigKeys.DFS_HA_LOGROLL_RPC_TIMEOUT_DEFAULT);NamenodeProtocolPB proxy = RPC.waitForProxy(NamenodeProtocolPB.class, RPC.getProtocolVersion(NamenodeProtocolPB.class), activeAddr, conf, rpcTimeout, Long.MAX_VALUE);cachedActiveProxy = new NamenodeProtocolTranslatorPB(proxy);}assert cachedActiveProxy != null;return cachedActiveProxy;}
public void start() {tailerThread.start();}
public void stop() {tailerThread.setShouldRun(false);tailerThread.interrupt();try {tailerThread.join();} catch (InterruptedException e) {LOG.warn("Edit log tailer thread exited with an exception");throw new IOException(e);}}
FSEditLog getEditLog() {return editLog;}
public void setEditLog(FSEditLog editLog) {this.editLog = editLog;}
public void catchupDuringFailover() {Preconditions.checkState(tailerThread == null || !tailerThread.isAlive(), "Tailer thread should not be running once failover starts");// on security credentials to access the logs (eg QuorumJournalManager).SecurityUtil.doAsLoginUser(new PrivilegedExceptionAction<Void>() {
@Overridepublic Void run() throws Exception {try {doTailEdits();} catch (InterruptedException e) {throw new IOException(e);}return null;}});}
public Void run() {try {doTailEdits();} catch (InterruptedException e) {throw new IOException(e);}return null;}
void doTailEdits() {// deadlock.namesystem.writeLockInterruptibly();try {FSImage image = namesystem.getFSImage();long lastTxnId = image.getLastAppliedTxId();if (LOG.isDebugEnabled()) {LOG.debug("lastTxnId: " + lastTxnId);}Collection<EditLogInputStream> streams;try {streams = editLog.selectInputStreams(lastTxnId + 1, 0, null, false);} catch (IOException ioe) {LOG.warn("Edits tailer failed to find any streams. Will try again " + "later.", ioe);return;}if (LOG.isDebugEnabled()) {LOG.debug("edit streams to load from: " + streams.size());}// disk are ignored.long editsLoaded = 0;try {editsLoaded = image.loadEdits(streams, namesystem);} catch (EditLogInputException elie) {editsLoaded = elie.getNumEditsLoaded();throw elie;} finally {if (editsLoaded > 0 || LOG.isDebugEnabled()) {LOG.info(String.format("Loaded %d edits starting from txid %d ", editsLoaded, lastTxnId));}}if (editsLoaded > 0) {lastLoadTimestamp = now();}lastLoadedTxnId = image.getLastAppliedTxId();} finally {namesystem.writeUnlock();}}
public long getLastLoadTimestamp() {return lastLoadTimestamp;}
private boolean tooLongSinceLastLoad() {return logRollPeriodMs >= 0 && (now() - lastLoadTimestamp) > logRollPeriodMs;}
private void triggerActiveLogRoll() {LOG.info("Triggering log roll on remote NameNode " + activeAddr);try {getActiveNodeProxy().rollEditLog();lastRollTriggerTxId = lastLoadedTxnId;} catch (IOException ioe) {LOG.warn("Unable to trigger a roll of the active NN", ioe);}}
private void setShouldRun(boolean shouldRun) {this.shouldRun = shouldRun;}
public void run() {SecurityUtil.doAsLoginUserOrFatal(new PrivilegedAction<Object>() {
@Overridepublic Object run() {doWork();return null;}});}
public Object run() {doWork();return null;}
private void doWork() {while (shouldRun) {try {// triggered. if (tooLongSinceLastLoad() && lastRollTriggerTxId < lastLoadedTxnId) {triggerActiveLogRoll();}/**   * Check again in case someone calls {@link EditLogTailer#stop} while   * we're triggering an edit log roll, since ipc.Client catches and   * ignores {@link InterruptedException} in a few places. This fixes   * the bug described in HDFS-2823.   */if (!shouldRun) {break;}doTailEdits();} catch (EditLogInputException elie) {LOG.warn("Error while reading edits from disk. Will try again.", elie);} catch (InterruptedException ie) {continue;} catch (Throwable t) {LOG.fatal("Unknown error encountered while tailing edits. " + "Shutting down standby NN.", t);terminate(1, t);}try {Thread.sleep(sleepTimeMs);} catch (InterruptedException e) {LOG.warn("Edit log tailer interrupted", e);}}}protected ResultSet executeQuery(String query) {this.statement = connection.prepareStatement(query, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);return statement.executeQuery();}
protected String getSelectQuery() {StringBuilder query = new StringBuilder();// Default codepath for MySQL, HSQLDB, etc. Relies on LIMIT/OFFSET for splits.if (dbConf.getInputQuery() == null) {query.append("SELECT ");for (int i = 0; i < fieldNames.length; i++) {query.append(fieldNames[i]);if (i != fieldNames.length - 1) {query.append(", ");}}query.append(" FROM ").append(tableName);//in hsqldb this is necessaryquery.append(" AS ").append(tableName);if (conditions != null && conditions.length() > 0) {query.append(" WHERE (").append(conditions).append(")");}String orderBy = dbConf.getInputOrderBy();if (orderBy != null && orderBy.length() > 0) {query.append(" ORDER BY ").append(orderBy);}} else {//PREBUILT QUERYquery.append(dbConf.getInputQuery());}try {query.append(" LIMIT ").append(split.getLength());query.append(" OFFSET ").append(split.getStart());} catch (IOException ex) {}return query.toString();}
public void close() {try {if (null != results) {results.close();}if (null != statement) {statement.close();}if (null != connection) {connection.commit();connection.close();}} catch (SQLException e) {throw new IOException(e.getMessage());}}
public void initialize(InputSplit split, TaskAttemptContext context) {}
public LongWritable getCurrentKey() {return key;}
public T getCurrentValue() {return value;}
public T createValue() {return ReflectionUtils.newInstance(inputClass, conf);}
public long getPos() {return pos;}
public boolean next(LongWritable key, T value) {this.key = key;this.value = value;return nextKeyValue();}
public float getProgress() {return pos / (float) split.getLength();}
public boolean nextKeyValue() {try {if (key == null) {key = new LongWritable();}if (value == null) {value = createValue();}if (null == this.results) {// First time into this method, run the query.this.results = executeQuery(getSelectQuery());}if (!results.next())return false;// Set the key field value as the output key valuekey.set(pos + split.getStart());value.readFields(results);pos++;} catch (SQLException e) {throw new IOException("SQLException in nextKeyValue", e);}return true;}
protected DBInputFormat.DBInputSplit getSplit() {return split;}
protected String[] getFieldNames() {return fieldNames;}
protected String getTableName() {return tableName;}
protected String getConditions() {return conditions;}
protected DBConfiguration getDBConf() {return dbConf;}
protected Connection getConnection() {return connection;}
protected PreparedStatement getStatement() {return statement;}
protected void setStatement(PreparedStatement stmt) {this.statement = stmt;}void start(Configuration conf) {simulationStartTime = System.currentTimeMillis();}
private void processJobState(JobStats stats) {Job job = stats.getJob();try {if (job.isSuccessful()) {++totalSuccessfulJobs;} else {++totalFailedJobs;}} catch (Exception e) {++totalLostJobs;}}
private void processJobTasks(JobStats stats) {totalMapTasksLaunched += stats.getNoOfMaps();totalReduceTasksLaunched += stats.getNoOfReds();}
private void process(JobStats stats) {// process the job run stateprocessJobState(stats);// process the tasks informationprocessJobTasks(stats);}
public void update(JobStats item) {// process only if the simulation has startedif (simulationStartTime > 0) {process(item);totalSimulationTime = System.currentTimeMillis() - getSimulationStartTime();}}
protected static String getTraceSignature(String input) {Path inputPath = new Path(input);FileSystem fs = inputPath.getFileSystem(new Configuration());FileStatus status = fs.getFileStatus(inputPath);Path qPath = fs.makeQualified(status.getPath());String traceID = status.getModificationTime() + qPath.toString() + status.getOwner() + status.getLen();return MD5Hash.digest(traceID).toString();}
void finalize(JobFactory factory, String inputPath, long dataSize, UserResolver userResolver, DataStatistics stats, Configuration conf) {numJobsInInputTrace = factory.numJobsInTrace;endTime = System.currentTimeMillis();if ("-".equals(inputPath)) {inputTraceLocation = Summarizer.NA;inputTraceSignature = Summarizer.NA;} else {Path inputTracePath = new Path(inputPath);FileSystem fs = inputTracePath.getFileSystem(conf);inputTraceLocation = fs.makeQualified(inputTracePath).toString();inputTraceSignature = getTraceSignature(inputPath);}jobSubmissionPolicy = Gridmix.getJobSubmissionPolicy(conf).name();resolver = userResolver.getClass().getName();if (dataSize > 0) {expectedDataSize = StringUtils.humanReadableInt(dataSize);} else {expectedDataSize = Summarizer.NA;}dataStats = stats;totalRuntime = System.currentTimeMillis() - getStartTime();}
public String toString() {StringBuilder builder = new StringBuilder();builder.append("Execution Summary:-");builder.append("\nInput trace: ").append(getInputTraceLocation());builder.append("\nInput trace signature: ").append(getInputTraceSignature());builder.append("\nTotal number of jobs in trace: ").append(getNumJobsInTrace());builder.append("\nExpected input data size: ").append(getExpectedDataSize());builder.append("\nInput data statistics: ").append(getInputDataStatistics());builder.append("\nTotal number of jobs processed: ").append(getNumSubmittedJobs());builder.append("\nTotal number of successful jobs: ").append(getNumSuccessfulJobs());builder.append("\nTotal number of failed jobs: ").append(getNumFailedJobs());builder.append("\nTotal number of lost jobs: ").append(getNumLostJobs());builder.append("\nTotal number of map tasks launched: ").append(getNumMapTasksLaunched());builder.append("\nTotal number of reduce task launched: ").append(getNumReduceTasksLaunched());builder.append("\nGridmix start time: ").append(UTIL.format(getStartTime()));builder.append("\nGridmix end time: ").append(UTIL.format(getEndTime()));builder.append("\nGridmix simulation start time: ").append(UTIL.format(getStartTime()));builder.append("\nGridmix runtime: ").append(StringUtils.formatTime(getRuntime()));builder.append("\nTime spent in initialization (data-gen etc): ").append(StringUtils.formatTime(getInitTime()));builder.append("\nTime spent in simulation: ").append(StringUtils.formatTime(getSimulationTime()));builder.append("\nGridmix configuration parameters: ").append(getCommandLineArgsString());builder.append("\nGridmix job submission policy: ").append(getJobSubmissionPolicy());builder.append("\nGridmix resolver: ").append(getUserResolver());builder.append("\n\n");return builder.toString();}
static String stringifyDataStatistics(DataStatistics stats) {if (stats != null) {StringBuffer buffer = new StringBuffer();String compressionStatus = stats.isDataCompressed() ? "Compressed" : "Uncompressed";buffer.append(compressionStatus).append(" input data size: ");buffer.append(StringUtils.humanReadableInt(stats.getDataSize()));buffer.append(", ");buffer.append("Number of files: ").append(stats.getNumFiles());return buffer.toString();} else {return Summarizer.NA;}}
protected String getExpectedDataSize() {return expectedDataSize;}
protected String getUserResolver() {return resolver;}
protected String getInputDataStatistics() {return stringifyDataStatistics(dataStats);}
protected String getInputTraceSignature() {return inputTraceSignature;}
protected String getInputTraceLocation() {return inputTraceLocation;}
protected int getNumJobsInTrace() {return numJobsInInputTrace;}
protected int getNumSuccessfulJobs() {return totalSuccessfulJobs;}
protected int getNumFailedJobs() {return totalFailedJobs;}
protected int getNumLostJobs() {return totalLostJobs;}
protected int getNumSubmittedJobs() {return totalSuccessfulJobs + totalFailedJobs + totalLostJobs;}
protected int getNumMapTasksLaunched() {return totalMapTasksLaunched;}
protected int getNumReduceTasksLaunched() {return totalReduceTasksLaunched;}
protected long getStartTime() {return startTime;}
protected long getEndTime() {return endTime;}
protected long getInitTime() {return simulationStartTime - startTime;}
protected long getSimulationStartTime() {return simulationStartTime;}
protected long getSimulationTime() {return totalSimulationTime;}
protected long getRuntime() {return totalRuntime;}
protected String getCommandLineArgsString() {return commandLineArgs;}
protected String getJobSubmissionPolicy() {return jobSubmissionPolicy;}public void testDeleteEmptyDirNonRecursive() {Path path = path("testDeleteEmptyDirNonRecursive");mkdirs(path);assertDeleted(path, false);}
public void testDeleteEmptyDirRecursive() {Path path = path("testDeleteEmptyDirRecursive");mkdirs(path);assertDeleted(path, true);}
public void testDeleteNonexistentPathRecursive() {Path path = path("testDeleteNonexistentPathRecursive");ContractTestUtils.assertPathDoesNotExist(getFileSystem(), "leftover", path);ContractTestUtils.rejectRootOperation(path);assertFalse("Returned true attempting to delete" + " a nonexistent path " + path, getFileSystem().delete(path, false));}
public void testDeleteNonexistentPathNonRecursive() {Path path = path("testDeleteNonexistentPathNonRecursive");ContractTestUtils.assertPathDoesNotExist(getFileSystem(), "leftover", path);ContractTestUtils.rejectRootOperation(path);assertFalse("Returned true attempting to recursively delete" + " a nonexistent path " + path, getFileSystem().delete(path, false));}
public void testDeleteNonEmptyDirNonRecursive() {Path path = path("testDeleteNonEmptyDirNonRecursive");mkdirs(path);Path file = new Path(path, "childfile");ContractTestUtils.writeTextFile(getFileSystem(), file, "goodbye, world", true);try {ContractTestUtils.rejectRootOperation(path);boolean deleted = getFileSystem().delete(path, false);fail("non recursive delete should have raised an exception," + " but completed with exit code " + deleted);} catch (IOException expected) {handleExpectedException(expected);}ContractTestUtils.assertIsDirectory(getFileSystem(), path);}
public void testDeleteNonEmptyDirRecursive() {Path path = path("testDeleteNonEmptyDirNonRecursive");mkdirs(path);Path file = new Path(path, "childfile");ContractTestUtils.writeTextFile(getFileSystem(), file, "goodbye, world", true);assertDeleted(path, true);ContractTestUtils.assertPathDoesNotExist(getFileSystem(), "not deleted", file);}public String getMessage() {return this.message;}public void testDFSAddressConfig() {Configuration conf = new HdfsConfiguration();/*------------------------------------------------------------------------- * By default, the DataNode socket address should be localhost (127.0.0.1). *------------------------------------------------------------------------*/MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();ArrayList<DataNode> dns = cluster.getDataNodes();DataNode dn = dns.get(0);String selfSocketAddr = dn.getXferAddress().toString();System.out.println("DN Self Socket Addr == " + selfSocketAddr);assertTrue(selfSocketAddr.contains("/127.0.0.1:"));/*------------------------------------------------------------------------- * Shut down the datanodes, reconfigure, and bring them back up. * Even if told to use the configuration properties for dfs.datanode, * MiniDFSCluster.startDataNodes() should use localhost as the default if * the dfs.datanode properties are not set. *------------------------------------------------------------------------*/for (int i = 0; i < dns.size(); i++) {DataNodeProperties dnp = cluster.stopDataNode(i);assertNotNull("Should have been able to stop simulated datanode", dnp);}conf.unset(DFS_DATANODE_ADDRESS_KEY);conf.unset(DFS_DATANODE_HTTP_ADDRESS_KEY);conf.unset(DFS_DATANODE_IPC_ADDRESS_KEY);cluster.startDataNodes(conf, 1, true, StartupOption.REGULAR, null, null, null, false, true);dns = cluster.getDataNodes();dn = dns.get(0);selfSocketAddr = dn.getXferAddress().toString();System.out.println("DN Self Socket Addr == " + selfSocketAddr);// assert that default self socket address is 127.0.0.1assertTrue(selfSocketAddr.contains("/127.0.0.1:"));/*------------------------------------------------------------------------- * Shut down the datanodes, reconfigure, and bring them back up. * This time, modify the dfs.datanode properties and make sure that they * are used to configure sockets by MiniDFSCluster.startDataNodes(). *------------------------------------------------------------------------*/for (int i = 0; i < dns.size(); i++) {DataNodeProperties dnp = cluster.stopDataNode(i);assertNotNull("Should have been able to stop simulated datanode", dnp);}conf.set(DFS_DATANODE_ADDRESS_KEY, "0.0.0.0:0");conf.set(DFS_DATANODE_HTTP_ADDRESS_KEY, "0.0.0.0:0");conf.set(DFS_DATANODE_IPC_ADDRESS_KEY, "0.0.0.0:0");cluster.startDataNodes(conf, 1, true, StartupOption.REGULAR, null, null, null, false, true);dns = cluster.getDataNodes();dn = dns.get(0);selfSocketAddr = dn.getXferAddress().toString();System.out.println("DN Self Socket Addr == " + selfSocketAddr);// assert that default self socket address is 0.0.0.0assertTrue(selfSocketAddr.contains("/0.0.0.0:"));cluster.shutdown();}public void setUp() {System.setProperty("java.security.krb5.realm", KerberosTestUtils.getRealm());System.setProperty("java.security.krb5.kdc", "localhost:88");String rules = "RULE:[1:$1@$0](.*@YAHOO\\.COM)s/@.*//\n" + "RULE:[2:$1](johndoe)s/^.*$/guest/\n" + "RULE:[2:$1;$2](^.*;admin$)s/;admin$//\n" + "RULE:[2:$2](root)\n" + "DEFAULT";KerberosName.setRules(rules);KerberosName.printRules();}
private void checkTranslation(String from, String to) {System.out.println("Translate " + from);KerberosName nm = new KerberosName(from);String simple = nm.getShortName();System.out.println("to " + simple);Assert.assertEquals("short name incorrect", to, simple);}
public void testRules() {checkTranslation("omalley@" + KerberosTestUtils.getRealm(), "omalley");checkTranslation("hdfs/10.0.0.1@" + KerberosTestUtils.getRealm(), "hdfs");checkTranslation("oom@YAHOO.COM", "oom");checkTranslation("johndoe/zoo@FOO.COM", "guest");checkTranslation("joe/admin@FOO.COM", "joe");checkTranslation("joe/root@FOO.COM", "root");}
private void checkBadName(String name) {System.out.println("Checking " + name + " to ensure it is bad.");try {new KerberosName(name);Assert.fail("didn't get exception for " + name);} catch (IllegalArgumentException iae) {}}
private void checkBadTranslation(String from) {System.out.println("Checking bad translation for " + from);KerberosName nm = new KerberosName(from);try {nm.getShortName();Assert.fail("didn't get exception for " + from);} catch (IOException ie) {}}
public void testAntiPatterns() {checkBadName("owen/owen/owen@FOO.COM");checkBadName("owen@foo/bar.com");checkBadTranslation("foo@ACME.COM");checkBadTranslation("root/joe@FOO.COM");}
public void testToLowerCase() {String rules = "RULE:[1:$1]/L\n" + "RULE:[2:$1]/L\n" + "RULE:[2:$1;$2](^.*;admin$)s/;admin$///L\n" + "RULE:[2:$1;$2](^.*;guest$)s/;guest$//g/L\n" + "DEFAULT";KerberosName.setRules(rules);KerberosName.printRules();checkTranslation("Joe@FOO.COM", "joe");checkTranslation("Joe/root@FOO.COM", "joe");checkTranslation("Joe/admin@FOO.COM", "joe");checkTranslation("Joe/guestguest@FOO.COM", "joe");}
public void clear() {System.clearProperty("java.security.krb5.realm");System.clearProperty("java.security.krb5.kdc");}public MonitorHealthResponseProto monitorHealth(RpcController controller, MonitorHealthRequestProto request) {try {server.monitorHealth();return MONITOR_HEALTH_RESP;} catch (IOException e) {throw new ServiceException(e);}}
private StateChangeRequestInfo convert(HAStateChangeRequestInfoProto proto) {RequestSource src;switch(proto.getReqSource()) {case REQUEST_BY_USER:src = RequestSource.REQUEST_BY_USER;break;case REQUEST_BY_USER_FORCED:src = RequestSource.REQUEST_BY_USER_FORCED;break;case REQUEST_BY_ZKFC:src = RequestSource.REQUEST_BY_ZKFC;break;default:LOG.warn("Unknown request source: " + proto.getReqSource());src = null;}return new StateChangeRequestInfo(src);}
public TransitionToActiveResponseProto transitionToActive(RpcController controller, TransitionToActiveRequestProto request) {try {server.transitionToActive(convert(request.getReqInfo()));return TRANSITION_TO_ACTIVE_RESP;} catch (IOException e) {throw new ServiceException(e);}}
public TransitionToStandbyResponseProto transitionToStandby(RpcController controller, TransitionToStandbyRequestProto request) {try {server.transitionToStandby(convert(request.getReqInfo()));return TRANSITION_TO_STANDBY_RESP;} catch (IOException e) {throw new ServiceException(e);}}
public GetServiceStatusResponseProto getServiceStatus(RpcController controller, GetServiceStatusRequestProto request) {HAServiceStatus s;try {s = server.getServiceStatus();} catch (IOException e) {throw new ServiceException(e);}HAServiceStateProto retState;switch(s.getState()) {case ACTIVE:retState = HAServiceStateProto.ACTIVE;break;case STANDBY:retState = HAServiceStateProto.STANDBY;break;case INITIALIZING:default:retState = HAServiceStateProto.INITIALIZING;break;}GetServiceStatusResponseProto.Builder ret = GetServiceStatusResponseProto.newBuilder().setState(retState).setReadyToBecomeActive(s.isReadyToBecomeActive());if (!s.isReadyToBecomeActive()) {ret.setNotReadyReason(s.getNotReadyReason());}return ret.build();}
public long getProtocolVersion(String protocol, long clientVersion) {return RPC.getProtocolVersion(HAServiceProtocolPB.class);}
public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) {if (!protocol.equals(RPC.getProtocolName(HAServiceProtocolPB.class))) {throw new IOException("Serverside implements " + RPC.getProtocolName(HAServiceProtocolPB.class) + ". The following requested protocol is unknown: " + protocol);}return ProtocolSignature.getProtocolSignature(clientMethodsHash, RPC.getProtocolVersion(HAServiceProtocolPB.class), HAServiceProtocolPB.class);}protected TestConfigFileParser getConfigParser() {return new TestConfigFileParserDFS();}
public void endElement(String uri, String localName, String qName) {if (qName.equals("dfs-admin-command")) {if (testCommands != null) {testCommands.add(new CLITestCmdDFS(charString, new CLICommandDFSAdmin()));} else if (cleanupCommands != null) {cleanupCommands.add(new CLITestCmdDFS(charString, new CLICommandDFSAdmin()));}} else {super.endElement(uri, localName, qName);}}public void setUp() {fc.mkdir(fileContextTestHelper.getTestRootPath(fc), FileContext.DEFAULT_PERM, true);}
public void tearDown() {fc.delete(fileContextTestHelper.getTestRootPath(fc), true);}
public void testFcCopy() {final String ts = "some random text";Path file1 = fileContextTestHelper.getTestRootPath(fc, "file1");Path file2 = fileContextTestHelper.getTestRootPath(fc, "file2");writeFile(fc, file1, ts.getBytes());assertTrue(fc.util().exists(file1));fc.util().copy(file1, file2);// verify that newly copied file2 existsassertTrue("Failed to copy file2  ", fc.util().exists(file2));// verify that file2 contains test stringassertTrue("Copied files does not match ", Arrays.equals(ts.getBytes(), readFile(fc, file2, ts.getBytes().length)));}
public void testRecursiveFcCopy() {final String ts = "some random text";Path dir1 = fileContextTestHelper.getTestRootPath(fc, "dir1");Path dir2 = fileContextTestHelper.getTestRootPath(fc, "dir2");Path file1 = new Path(dir1, "file1");fc.mkdir(dir1, null, false);writeFile(fc, file1, ts.getBytes());assertTrue(fc.util().exists(file1));Path file2 = new Path(dir2, "file1");fc.util().copy(dir1, dir2);// verify that newly copied file2 existsassertTrue("Failed to copy file2  ", fc.util().exists(file2));// verify that file2 contains test stringassertTrue("Copied files does not match ", Arrays.equals(ts.getBytes(), readFile(fc, file2, ts.getBytes().length)));}public void run() {publishMetricsFromQueue();}
boolean putMetrics(MetricsBuffer buffer, long logicalTime) {if (logicalTime % period == 0) {LOG.debug("enqueue, logicalTime=" + logicalTime);if (queue.enqueue(buffer))return true;dropped.incr();return false;}// OKreturn true;}
public boolean putMetricsImmediate(MetricsBuffer buffer) {WaitableMetricsBuffer waitableBuffer = new WaitableMetricsBuffer(buffer);if (!queue.enqueue(waitableBuffer)) {LOG.warn(name + " has a full queue and can't consume the given metrics.");dropped.incr();return false;}if (!waitableBuffer.waitTillNotified(oobPutTimeout)) {LOG.warn(name + " couldn't fulfill an immediate putMetrics request in time." + " Abandoning.");return false;}return true;}
void publishMetricsFromQueue() {int retryDelay = firstRetryDelay;int n = retryCount;// millisint minDelay = Math.min(500, retryDelay * 1000);Random rng = new Random(System.nanoTime());while (!stopping) {try {queue.consumeAll(this);retryDelay = firstRetryDelay;n = retryCount;inError = false;} catch (InterruptedException e) {LOG.info(name + " thread interrupted.");} catch (Exception e) {if (n > 0) {int retryWindow = Math.max(0, 1000 / 2 * retryDelay - minDelay);int awhile = rng.nextInt(retryWindow) + minDelay;if (!inError) {LOG.error("Got sink exception, retry in " + awhile + "ms", e);}retryDelay *= retryBackoff;try {Thread.sleep(awhile);} catch (InterruptedException e2) {LOG.info(name + " thread interrupted while waiting for retry", e2);}--n;} else {if (!inError) {LOG.error("Got sink exception and over retry limit, " + "suppressing further error messages", e);}queue.clear();inError = true;}}}}
public void consume(MetricsBuffer buffer) {long ts = 0;for (MetricsBuffer.Entry entry : buffer) {if (sourceFilter == null || sourceFilter.accepts(entry.name())) {for (MetricsRecordImpl record : entry.records()) {if ((context == null || context.equals(record.context())) && (recordFilter == null || recordFilter.accepts(record))) {if (LOG.isDebugEnabled()) {LOG.debug("Pushing record " + entry.name() + "." + record.context() + "." + record.name() + " to " + name);}sink.putMetrics(metricFilter == null ? record : new MetricsRecordFiltered(record, metricFilter));if (ts == 0)ts = record.timestamp();}}}}if (ts > 0) {sink.flush();latency.add(Time.now() - ts);}if (buffer instanceof WaitableMetricsBuffer) {((WaitableMetricsBuffer) buffer).notifyAnyWaiters();}LOG.debug("Done");}
void start() {sinkThread.start();LOG.info("Sink " + name + " started");}
void stop() {stopping = true;sinkThread.interrupt();try {sinkThread.join();} catch (InterruptedException e) {LOG.warn("Stop interrupted", e);}if (sink instanceof Closeable) {IOUtils.cleanup(LOG, (Closeable) sink);}}
String name() {return name;}
String description() {return description;}
void snapshot(MetricsRecordBuilder rb, boolean all) {registry.snapshot(rb, all);}
MetricsSink sink() {return sink;}
public boolean waitTillNotified(long millisecondsToWait) {try {return notificationSemaphore.tryAcquire(millisecondsToWait, TimeUnit.MILLISECONDS);} catch (InterruptedException e) {return false;}}
public void notifyAnyWaiters() {notificationSemaphore.release();}public void testResourceDecreaseContext() {ContainerId containerId = ContainerId.newInstance(ApplicationAttemptId.newInstance(ApplicationId.newInstance(1234, 3), 3), 7);Resource resource = Resource.newInstance(1023, 3);ContainerResourceDecrease ctx = ContainerResourceDecrease.newInstance(containerId, resource);// get proto and recover to ctxContainerResourceDecreaseProto proto = ((ContainerResourceDecreasePBImpl) ctx).getProto();ctx = new ContainerResourceDecreasePBImpl(proto);// check valuesAssert.assertEquals(ctx.getCapability(), resource);Assert.assertEquals(ctx.getContainerId(), containerId);}
public void testResourceDecreaseContextWithNull() {ContainerResourceDecrease ctx = ContainerResourceDecrease.newInstance(null, null);// get proto and recover to ctx;ContainerResourceDecreaseProto proto = ((ContainerResourceDecreasePBImpl) ctx).getProto();ctx = new ContainerResourceDecreasePBImpl(proto);// check valuesAssert.assertNull(ctx.getCapability());Assert.assertNull(ctx.getContainerId());}public void uncaughtException(Thread t, Throwable e) {if (ShutdownHookManager.get().isShutdownInProgress()) {LOG.error("Thread " + t + " threw an Throwable, but we are shutting " + "down, so ignoring this", e);} else if (e instanceof Error) {try {LOG.fatal("Thread " + t + " threw an Error.  Shutting down now...", e);} catch (Throwable err) {}if (e instanceof OutOfMemoryError) {//even try to clean up or we can get stuck on shutdown.try {System.err.println("Halting due to Out Of Memory Error...");} catch (Throwable err) {}ExitUtil.halt(-1);} else {ExitUtil.terminate(-1);}} else {LOG.error("Thread " + t + " threw an Exception.", e);}}private FilterConfig mockConfig(String username) {FilterConfig mock = Mockito.mock(FilterConfig.class);Mockito.doReturn(username).when(mock).getInitParameter(CommonConfigurationKeys.HADOOP_HTTP_STATIC_USER);return mock;}
public void testFilter() {FilterConfig config = mockConfig("myuser");StaticUserFilter suf = new StaticUserFilter();suf.init(config);ArgumentCaptor<HttpServletRequestWrapper> wrapperArg = ArgumentCaptor.forClass(HttpServletRequestWrapper.class);FilterChain chain = mock(FilterChain.class);suf.doFilter(mock(HttpServletRequest.class), mock(ServletResponse.class), chain);Mockito.verify(chain).doFilter(wrapperArg.capture(), Mockito.<ServletResponse>anyObject());HttpServletRequestWrapper wrapper = wrapperArg.getValue();assertEquals("myuser", wrapper.getUserPrincipal().getName());assertEquals("myuser", wrapper.getRemoteUser());suf.destroy();}
public void testOldStyleConfiguration() {Configuration conf = new Configuration();conf.set("dfs.web.ugi", "joe,group1,group2");assertEquals("joe", StaticUserWebFilter.getUsernameFromConf(conf));}
public void testConfiguration() {Configuration conf = new Configuration();conf.set(CommonConfigurationKeys.HADOOP_HTTP_STATIC_USER, "joe");assertEquals("joe", StaticUserWebFilter.getUsernameFromConf(conf));}public final String getAclName() {return aclName;}public static long getFolderUsage(String folder) {return new DUHelper().calculateFolderSize(folder);}
private long calculateFolderSize(String folder) {if (folder == null)throw new IllegalArgumentException("folder");File f = new File(folder);return folderSize = getFileSize(f);}
public String check(String folder) {if (folder == null)throw new IllegalArgumentException("folder");File f = new File(folder);folderSize = getFileSize(f);usage = 1.0 * (f.getTotalSpace() - f.getFreeSpace()) / f.getTotalSpace();return String.format("used %d files %d disk in use %f", folderSize, fileCount, usage);}
public long getFileCount() {return fileCount;}
public double getUsage() {return usage;}
private long getFileSize(File folder) {folderCount++;long foldersize = 0;if (folder.isFile())return folder.length();File[] filelist = folder.listFiles();if (filelist == null) {return 0;}for (int i = 0; i < filelist.length; i++) {if (filelist[i].isDirectory()) {foldersize += getFileSize(filelist[i]);} else {//Counting the total filesfileCount++;foldersize += filelist[i].length();}}return foldersize;}
public static void main(String[] args) {if (Shell.WINDOWS)System.out.println("Windows: " + DUHelper.getFolderUsage(args[0]));elseSystem.out.println("Other: " + DUHelper.getFolderUsage(args[0]));}public void setUp() {}
public void tearDown() {// Delete test files after running testsEXCLUDES_FILE.delete();INCLUDES_FILE.delete();}
public void testHostsFileReader() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.write("#DFS-Hosts-excluded\n");efw.write("somehost1\n");efw.write("#This-is-comment\n");efw.write("somehost2\n");efw.write("somehost3 # host3\n");efw.write("somehost4\n");efw.write("somehost4 somehost5\n");efw.close();ifw.write("#Hosts-in-DFS\n");ifw.write("somehost1\n");ifw.write("somehost2\n");ifw.write("somehost3\n");ifw.write("#This-is-comment\n");ifw.write("somehost4 # host4\n");ifw.write("somehost4 somehost5\n");ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);int includesLen = hfp.getHosts().size();int excludesLen = hfp.getExcludedHosts().size();assertEquals(5, includesLen);assertEquals(5, excludesLen);assertTrue(hfp.getHosts().contains("somehost5"));assertFalse(hfp.getHosts().contains("host3"));assertTrue(hfp.getExcludedHosts().contains("somehost5"));assertFalse(hfp.getExcludedHosts().contains("host4"));}
public void testCreateHostFileReaderWithNonexistentFile() {try {new HostsFileReader(HOSTS_TEST_DIR + "/doesnt-exist", HOSTS_TEST_DIR + "/doesnt-exist");Assert.fail("Should throw FileNotFoundException");} catch (FileNotFoundException ex) {}}
public void testRefreshHostFileReaderWithNonexistentFile() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.close();ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);assertTrue(INCLUDES_FILE.delete());try {hfp.refresh();Assert.fail("Should throw FileNotFoundException");} catch (FileNotFoundException ex) {}}
public void testHostFileReaderWithNull() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.close();ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);int includesLen = hfp.getHosts().size();int excludesLen = hfp.getExcludedHosts().size();// TestCase1: Check if lines beginning with # are ignoredassertEquals(0, includesLen);assertEquals(0, excludesLen);// getExcludedHostsassertFalse(hfp.getHosts().contains("somehost5"));assertFalse(hfp.getExcludedHosts().contains("somehost5"));}
public void testHostFileReaderWithCommentsOnly() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.write("#DFS-Hosts-excluded\n");efw.close();ifw.write("#Hosts-in-DFS\n");ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);int includesLen = hfp.getHosts().size();int excludesLen = hfp.getExcludedHosts().size();assertEquals(0, includesLen);assertEquals(0, excludesLen);assertFalse(hfp.getHosts().contains("somehost5"));assertFalse(hfp.getExcludedHosts().contains("somehost5"));}
public void testHostFileReaderWithSpaces() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.write("#DFS-Hosts-excluded\n");efw.write("   somehost somehost2");efw.write("   somehost3 # somehost4");efw.close();ifw.write("#Hosts-in-DFS\n");ifw.write("   somehost somehost2");ifw.write("   somehost3 # somehost4");ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);int includesLen = hfp.getHosts().size();int excludesLen = hfp.getExcludedHosts().size();assertEquals(3, includesLen);assertEquals(3, excludesLen);assertTrue(hfp.getHosts().contains("somehost3"));assertFalse(hfp.getHosts().contains("somehost5"));assertFalse(hfp.getHosts().contains("somehost4"));assertTrue(hfp.getExcludedHosts().contains("somehost3"));assertFalse(hfp.getExcludedHosts().contains("somehost5"));assertFalse(hfp.getExcludedHosts().contains("somehost4"));}
public void testHostFileReaderWithTabs() {FileWriter efw = new FileWriter(excludesFile);FileWriter ifw = new FileWriter(includesFile);efw.write("#DFS-Hosts-excluded\n");efw.write(" \n");efw.write("   somehost \t somehost2 \n somehost4");efw.write("   somehost3 \t # somehost5");efw.close();ifw.write("#Hosts-in-DFS\n");ifw.write(" \n");ifw.write("   somehost \t  somehost2 \n somehost4");ifw.write("   somehost3 \t # somehost5");ifw.close();HostsFileReader hfp = new HostsFileReader(includesFile, excludesFile);int includesLen = hfp.getHosts().size();int excludesLen = hfp.getExcludedHosts().size();assertEquals(4, includesLen);assertEquals(4, excludesLen);assertTrue(hfp.getHosts().contains("somehost2"));assertFalse(hfp.getHosts().contains("somehost5"));assertTrue(hfp.getExcludedHosts().contains("somehost2"));assertFalse(hfp.getExcludedHosts().contains("somehost5"));}static String formatDescription(String usage, String... desciptions) {StringBuilder b = new StringBuilder(usage + ": " + desciptions[0]);for (int i = 1; i < desciptions.length; i++) {b.append("\n\t\t" + desciptions[i]);}return b.toString();}public ZooKeeper getNewZooKeeper() {return client;}
public String getVersionNode() {return znodeWorkingPath + "/" + ROOT_ZNODE_NAME + "/" + VERSION_NODE;}
public Version getCurrentVersion() {return CURRENT_VERSION_INFO;}
public String getAppNode(String appId) {return workingZnode + "/" + ROOT_ZNODE_NAME + "/" + RM_APP_ROOT + "/" + appId;}
public RMStateStore getRMStateStore() {YarnConfiguration conf = new YarnConfiguration();workingZnode = "/Test";conf.set(YarnConfiguration.RM_ZK_ADDRESS, hostPort);conf.set(YarnConfiguration.ZK_RM_STATE_STORE_PARENT_PATH, workingZnode);this.client = createClient();this.store = new TestZKRMStateStoreInternal(conf, workingZnode);return this.store;}
public boolean isFinalStateValid() {List<String> nodes = client.getChildren(store.znodeWorkingPath, false);return nodes.size() == 1;}
public void writeVersion(Version version) {client.setData(store.getVersionNode(), ((VersionPBImpl) version).getProto().toByteArray(), -1);}
public Version getCurrentVersion() {return store.getCurrentVersion();}
public boolean appExists(RMApp app) {Stat node = client.exists(store.getAppNode(app.getApplicationId().toString()), false);return node != null;}
public void testZKRMStateStoreRealZK() {TestZKRMStateStoreTester zkTester = new TestZKRMStateStoreTester();testRMAppStateStore(zkTester);testRMDTSecretManagerStateStore(zkTester);testCheckVersion(zkTester);testEpoch(zkTester);testAppDeletion(zkTester);testDeleteStore(zkTester);testAMRMTokenSecretManagerStateStore(zkTester);}
private Configuration createHARMConf(String rmIds, String rmId, int adminPort) {Configuration conf = new YarnConfiguration();conf.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);conf.set(YarnConfiguration.RM_HA_IDS, rmIds);conf.setBoolean(YarnConfiguration.RECOVERY_ENABLED, true);conf.set(YarnConfiguration.RM_STORE, ZKRMStateStore.class.getName());conf.set(YarnConfiguration.RM_ZK_ADDRESS, hostPort);conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);conf.set(YarnConfiguration.RM_HA_ID, rmId);conf.set(YarnConfiguration.RM_WEBAPP_ADDRESS, "localhost:0");for (String rpcAddress : YarnConfiguration.getServiceAddressConfKeys(conf)) {for (String id : HAUtil.getRMHAIds(conf)) {conf.set(HAUtil.addSuffix(rpcAddress, id), "localhost:0");}}conf.set(HAUtil.addSuffix(YarnConfiguration.RM_ADMIN_ADDRESS, rmId), "localhost:" + adminPort);return conf;}
public void testFencing() {StateChangeRequestInfo req = new StateChangeRequestInfo(HAServiceProtocol.RequestSource.REQUEST_BY_USER);Configuration conf1 = createHARMConf("rm1,rm2", "rm1", 1234);conf1.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);ResourceManager rm1 = new ResourceManager();rm1.init(conf1);rm1.start();rm1.getRMContext().getRMAdminService().transitionToActive(req);assertEquals("RM with ZKStore didn't start", Service.STATE.STARTED, rm1.getServiceState());assertEquals("RM should be Active", HAServiceProtocol.HAServiceState.ACTIVE, rm1.getRMContext().getRMAdminService().getServiceStatus().getState());Configuration conf2 = createHARMConf("rm1,rm2", "rm2", 5678);conf2.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);ResourceManager rm2 = new ResourceManager();rm2.init(conf2);rm2.start();rm2.getRMContext().getRMAdminService().transitionToActive(req);assertEquals("RM with ZKStore didn't start", Service.STATE.STARTED, rm2.getServiceState());assertEquals("RM should be Active", HAServiceProtocol.HAServiceState.ACTIVE, rm2.getRMContext().getRMAdminService().getServiceStatus().getState());for (int i = 0; i < ZK_TIMEOUT_MS / 50; i++) {if (HAServiceProtocol.HAServiceState.ACTIVE == rm1.getRMContext().getRMAdminService().getServiceStatus().getState()) {Thread.sleep(100);}}assertEquals("RM should have been fenced", HAServiceProtocol.HAServiceState.STANDBY, rm1.getRMContext().getRMAdminService().getServiceStatus().getState());assertEquals("RM should be Active", HAServiceProtocol.HAServiceState.ACTIVE, rm2.getRMContext().getRMAdminService().getServiceStatus().getState());}public void setup() {conf = new HdfsConfiguration();conf.setBoolean(DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY, true);conf.setInt(DFSConfigKeys.DFS_CLIENT_TEST_DROP_NAMENODE_RESPONSE_NUM_KEY, 2);cluster = new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(3).build();cluster.waitActive();cluster.transitionToActive(namenodeId);HATestUtil.setFailoverConfigurations(cluster, conf);filesystem = (DistributedFileSystem) HATestUtil.configureFailoverFs(cluster, conf);namesystem = cluster.getNamesystem(namenodeId);metrics = namesystem.getRetryCache().getMetricsForTests();}
public void cleanup() {if (cluster != null) {cluster.shutdown();}}
public void testRetryCacheMetrics() {checkMetrics(0, 0, 0);// After that, 1 request will reach NameNode correctly.trySaveNamespace();checkMetrics(2, 0, 1);// RetryCache will be cleared after Namesystem#close()namesystem.close();checkMetrics(2, 1, 1);}
private void checkMetrics(long hit, long cleared, long updated) {assertEquals("CacheHit", hit, metrics.getCacheHit());assertEquals("CacheCleared", cleared, metrics.getCacheCleared());assertEquals("CacheUpdated", updated, metrics.getCacheUpdated());}
private void trySaveNamespace() {filesystem.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);filesystem.saveNamespace();filesystem.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);}public void setUp() {config = new HdfsConfiguration();}
public void tearDown() {if (cluster.isClusterUp())cluster.shutdown();File data_dir = new File(cluster.getDataDirectory());if (data_dir.exists() && !FileUtil.fullyDelete(data_dir)) {throw new IOException("Could not delete hdfs directory in tearDown '" + data_dir + "'");}}
public void testNameNode() {int numDatanodes = 2;cluster = new MiniDFSCluster.Builder(config).numDataNodes(numDatanodes).build();cluster.waitActive();DFSTestUtil.createFile(cluster.getFileSystem(), new Path("/test1"), fileSize, fileSize, blockSize, (short) 2, seed);JMXGet jmx = new JMXGet();String serviceName = "NameNode";jmx.setService(serviceName);// default lists namenode mbeans onlyjmx.init();assertTrue("error printAllValues", checkPrintAllValues(jmx));//get some data from different sourceassertEquals(numDatanodes, Integer.parseInt(jmx.getValue("NumLiveDataNodes")));assertGauge("CorruptBlocks", Long.parseLong(jmx.getValue("CorruptBlocks")), getMetrics("FSNamesystem"));assertEquals(numDatanodes, Integer.parseInt(jmx.getValue("NumOpenConnections")));cluster.shutdown();MBeanServerConnection mbsc = ManagementFactory.getPlatformMBeanServer();ObjectName query = new ObjectName("Hadoop:service=" + serviceName + ",*");Set<ObjectName> names = mbsc.queryNames(query, null);assertTrue("No beans should be registered for " + serviceName, names.isEmpty());}
private static boolean checkPrintAllValues(JMXGet jmx) {int size = 0;byte[] bytes = null;String pattern = "List of all the available keys:";PipedOutputStream pipeOut = new PipedOutputStream();PipedInputStream pipeIn = new PipedInputStream(pipeOut);System.setErr(new PrintStream(pipeOut));jmx.printAllValues();if ((size = pipeIn.available()) != 0) {bytes = new byte[size];pipeIn.read(bytes, 0, bytes.length);}pipeOut.close();pipeIn.close();return bytes != null ? new String(bytes).contains(pattern) : false;}
public void testDataNode() {int numDatanodes = 2;cluster = new MiniDFSCluster.Builder(config).numDataNodes(numDatanodes).build();cluster.waitActive();DFSTestUtil.createFile(cluster.getFileSystem(), new Path("/test"), fileSize, fileSize, blockSize, (short) 2, seed);JMXGet jmx = new JMXGet();String serviceName = "DataNode";jmx.setService(serviceName);jmx.init();assertEquals(fileSize, Integer.parseInt(jmx.getValue("BytesWritten")));cluster.shutdown();MBeanServerConnection mbsc = ManagementFactory.getPlatformMBeanServer();ObjectName query = new ObjectName("Hadoop:service=" + serviceName + ",*");Set<ObjectName> names = mbsc.queryNames(query, null);assertTrue("No beans should be registered for " + serviceName, names.isEmpty());}protected void createInput() {DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));out.write(input.getBytes("UTF-8"));out.close();}
protected String[] genArgs() {return new String[] { "-input", INPUT_FILE.getAbsolutePath(), "-output", OUTPUT_DIR.getAbsolutePath(), "-mapper", map, "-reducer", reduce, "-jobconf", "mapreduce.task.files.preserve.failedtasks=true", "-jobconf", "stream.tmpdir=" + System.getProperty("test.build.data", "/tmp"), "-io", "typedbytes" };}
public void cleanupOutput() {FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());INPUT_FILE.delete();createInput();}
public void testCommandLine() {// So don't specify -config or -clusterStreamJob job = new StreamJob();job.setConf(new Configuration());job.run(genArgs());File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();String output = StreamUtil.slurp(outFile);outFile.delete();System.out.println("   map=" + map);System.out.println("reduce=" + reduce);System.err.println("outEx1=" + outputExpect);System.err.println("  out1=" + output);assertEquals(outputExpect, output);}public Double weight(int elapsed, int duration) {return weight;}
public Double weight(int elapsed, int duration) {double normalized = (double) elapsed / (double) duration;double result = (-2.0d * Math.pow(normalized - 0.5, 2)) + 0.5d;if (result < 0) {result = 0;}if (result > 1) {result = 1;}return result;}
public Double weight(int elapsed, int duration) {double normalized = (double) elapsed / (double) duration;double result = Math.pow(normalized, 2);if (result < 0) {result = 0;}if (result > 1) {result = 1;}return result;}
public Double weight(int elapsed, int duration) {double normalized = (double) elapsed / (double) duration;double result = Math.pow((normalized - 1), 2);if (result < 0) {result = 0;}if (result > 1) {result = 1;}return result;}private void writeFile1(Path name) {FSDataOutputStream stm = fileSys.create(name, true, fileSys.getConf().getInt(IO_FILE_BUFFER_SIZE_KEY, 4096), NUM_OF_DATANODES, BLOCK_SIZE);stm.write(expected);stm.close();checkFile(name);cleanupFile(name);}
private void writeFile2(Path name) {FSDataOutputStream stm = fileSys.create(name, true, fileSys.getConf().getInt(IO_FILE_BUFFER_SIZE_KEY, 4096), NUM_OF_DATANODES, BLOCK_SIZE);int i = 0;for (; i < FILE_SIZE - BYTES_PER_CHECKSUM; i += BYTES_PER_CHECKSUM) {stm.write(expected, i, BYTES_PER_CHECKSUM);}stm.write(expected, i, FILE_SIZE - 3 * BYTES_PER_CHECKSUM);stm.close();checkFile(name);cleanupFile(name);}
private void writeFile3(Path name) {FSDataOutputStream stm = fileSys.create(name, true, fileSys.getConf().getInt(IO_FILE_BUFFER_SIZE_KEY, 4096), NUM_OF_DATANODES, BLOCK_SIZE);stm.write(expected, 0, HALF_CHUNK_SIZE);stm.write(expected, HALF_CHUNK_SIZE, BYTES_PER_CHECKSUM + 2);stm.write(expected, HALF_CHUNK_SIZE + BYTES_PER_CHECKSUM + 2, 2);stm.write(expected, HALF_CHUNK_SIZE + BYTES_PER_CHECKSUM + 4, HALF_CHUNK_SIZE);stm.write(expected, BLOCK_SIZE + 4, BYTES_PER_CHECKSUM - 4);stm.write(expected, BLOCK_SIZE + BYTES_PER_CHECKSUM, FILE_SIZE - 3 * BYTES_PER_CHECKSUM);stm.close();checkFile(name);cleanupFile(name);}
private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {for (int idx = 0; idx < actual.length; idx++) {assertEquals(message + " byte " + (from + idx) + " differs. expected " + expected[from + idx] + " actual " + actual[idx], actual[idx], expected[from + idx]);actual[idx] = 0;}}
private void checkFile(Path name) {FSDataInputStream stm = fileSys.open(name);// do a sanity check. Read the filestm.readFully(0, actual);checkAndEraseData(actual, 0, expected, "Read Sanity Test");stm.close();// do a sanity check. Get the file checksumfileSys.getFileChecksum(name);}
private void cleanupFile(Path name) {assertTrue(fileSys.exists(name));fileSys.delete(name, true);assertTrue(!fileSys.exists(name));}
public void testFSOutputSummer() {doTestFSOutputSummer("CRC32");doTestFSOutputSummer("CRC32C");doTestFSOutputSummer("NULL");}
private void doTestFSOutputSummer(String checksumType) {Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, BYTES_PER_CHECKSUM);conf.set(DFSConfigKeys.DFS_CHECKSUM_TYPE_KEY, checksumType);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();fileSys = cluster.getFileSystem();try {Path file = new Path("try.dat");Random rand = new Random(seed);rand.nextBytes(expected);writeFile1(file);writeFile2(file);writeFile3(file);} finally {fileSys.close();cluster.shutdown();}}
public void TestDFSCheckSumType() {Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, BYTES_PER_CHECKSUM);conf.set(DFSConfigKeys.DFS_CHECKSUM_TYPE_KEY, "NULL");MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();fileSys = cluster.getFileSystem();try {Path file = new Path("try.dat");Random rand = new Random(seed);rand.nextBytes(expected);writeFile1(file);} finally {fileSys.close();cluster.shutdown();}}public static int getDefaultBlockSize() {return DEFAULT_BLOCK_SIZE;}
public static byte[] getFileData(int numOfBlocks, long blockSize) {byte[] data = new byte[(int) (numOfBlocks * blockSize)];for (int i = 0; i < data.length; i++) {data[i] = (byte) (i % 10);}return data;}
public Path getTestRootPath(FileContext fc) {return fc.makeQualified(new Path(testRootDir));}
public Path getTestRootPath(FileContext fc, String pathString) {return fc.makeQualified(new Path(testRootDir, pathString));}
public String getAbsoluteTestRootDir(FileContext fc) {if (absTestRootDir == null) {if (new Path(testRootDir).isAbsolute()) {absTestRootDir = testRootDir;} else {absTestRootDir = fc.getWorkingDirectory().toString() + "/" + testRootDir;}}return absTestRootDir;}
public Path getAbsoluteTestRootPath(FileContext fc) {return fc.makeQualified(new Path(getAbsoluteTestRootDir(fc)));}
public Path getDefaultWorkingDirectory(FileContext fc) {return getTestRootPath(fc, "/user/" + System.getProperty("user.name")).makeQualified(fc.getDefaultFileSystem().getUri(), fc.getWorkingDirectory());}
public static long createFile(FileContext fc, Path path, int numBlocks, CreateOpts... options) {BlockSize blockSizeOpt = CreateOpts.getOpt(CreateOpts.BlockSize.class, options);long blockSize = blockSizeOpt != null ? blockSizeOpt.getValue() : DEFAULT_BLOCK_SIZE;FSDataOutputStream out = fc.create(path, EnumSet.of(CreateFlag.CREATE), options);byte[] data = getFileData(numBlocks, blockSize);out.write(data, 0, data.length);out.close();return data.length;}
public static long createFile(FileContext fc, Path path, int numBlocks, int blockSize) {return createFile(fc, path, numBlocks, CreateOpts.blockSize(blockSize), CreateOpts.createParent());}
public static long createFile(FileContext fc, Path path) {return createFile(fc, path, DEFAULT_NUM_BLOCKS, CreateOpts.createParent());}
public long createFile(FileContext fc, String name) {Path path = getTestRootPath(fc, name);return createFile(fc, path);}
public long createFileNonRecursive(FileContext fc, String name) {Path path = getTestRootPath(fc, name);return createFileNonRecursive(fc, path);}
public static long createFileNonRecursive(FileContext fc, Path path) {return createFile(fc, path, DEFAULT_NUM_BLOCKS, CreateOpts.donotCreateParent());}
public static void appendToFile(FileContext fc, Path path, int numBlocks, CreateOpts... options) {BlockSize blockSizeOpt = CreateOpts.getOpt(CreateOpts.BlockSize.class, options);long blockSize = blockSizeOpt != null ? blockSizeOpt.getValue() : DEFAULT_BLOCK_SIZE;FSDataOutputStream out;out = fc.create(path, EnumSet.of(CreateFlag.APPEND));byte[] data = getFileData(numBlocks, blockSize);out.write(data, 0, data.length);out.close();}
public static boolean exists(FileContext fc, Path p) {return fc.util().exists(p);}
public static boolean isFile(FileContext fc, Path p) {try {return fc.getFileStatus(p).isFile();} catch (FileNotFoundException e) {return false;}}
public static boolean isDir(FileContext fc, Path p) {try {return fc.getFileStatus(p).isDirectory();} catch (FileNotFoundException e) {return false;}}
public static boolean isSymlink(FileContext fc, Path p) {try {return fc.getFileLinkStatus(p).isSymlink();} catch (FileNotFoundException e) {return false;}}
public static void writeFile(FileContext fc, Path path, byte b[]) {FSDataOutputStream out = fc.create(path, EnumSet.of(CreateFlag.CREATE), CreateOpts.createParent());out.write(b);out.close();}
public static byte[] readFile(FileContext fc, Path path, int len) {DataInputStream dis = fc.open(path);byte[] buffer = new byte[len];IOUtils.readFully(dis, buffer, 0, len);dis.close();return buffer;}
public FileStatus containsPath(FileContext fc, Path path, FileStatus[] dirList) {return containsPath(getTestRootPath(fc, path.toString()), dirList);}
public static FileStatus containsPath(Path path, FileStatus[] dirList) {for (int i = 0; i < dirList.length; i++) {if (path.equals(dirList[i].getPath()))return dirList[i];}return null;}
public FileStatus containsPath(FileContext fc, String path, FileStatus[] dirList) {return containsPath(fc, new Path(path), dirList);}
public static void checkFileStatus(FileContext aFc, String path, fileType expectedType) {FileStatus s = aFc.getFileStatus(new Path(path));Assert.assertNotNull(s);if (expectedType == fileType.isDir) {Assert.assertTrue(s.isDirectory());} else if (expectedType == fileType.isFile) {Assert.assertTrue(s.isFile());} else if (expectedType == fileType.isSymlink) {Assert.assertTrue(s.isSymlink());}Assert.assertEquals(aFc.makeQualified(new Path(path)), s.getPath());}
public static void checkFileLinkStatus(FileContext aFc, String path, fileType expectedType) {FileStatus s = aFc.getFileLinkStatus(new Path(path));Assert.assertNotNull(s);if (expectedType == fileType.isDir) {Assert.assertTrue(s.isDirectory());} else if (expectedType == fileType.isFile) {Assert.assertTrue(s.isFile());} else if (expectedType == fileType.isSymlink) {Assert.assertTrue(s.isSymlink());}Assert.assertEquals(aFc.makeQualified(new Path(path)), s.getPath());}protected final void initializeTables(LoggedDiscreteCDF cdf) {rankings[0] = 0.0;values[0] = cdf.getMinimum();rankings[rankings.length - 1] = 1.0;values[rankings.length - 1] = cdf.getMaximum();List<LoggedSingleRelativeRanking> subjects = cdf.getRankings();for (int i = 0; i < subjects.size(); ++i) {rankings[i + 1] = subjects.get(i).getRelativeRanking();values[i + 1] = subjects.get(i).getDatum();}}
protected int floorIndex(double probe) {int result = Arrays.binarySearch(rankings, probe);return Math.abs(result + 1) - 1;}
protected double getRankingAt(int index) {return rankings[index];}
protected long getDatumAt(int index) {return values[index];}
public long randomValue() {return valueAt(random.nextDouble());}public void testCheckpointCreate() {checkpointCreate(ByteBuffer.allocate(BUFSIZE));}
public void testCheckpointCreateDirect() {checkpointCreate(ByteBuffer.allocateDirect(BUFSIZE));}
public void checkpointCreate(ByteBuffer b) {int WRITES = 128;FileSystem fs = mock(FileSystem.class);DataOutputBuffer dob = new DataOutputBuffer();FSDataOutputStream hdfs = spy(new FSDataOutputStream(dob, null));// backed by array@SuppressWarnings("resource") DataOutputBuffer verif = new DataOutputBuffer();when(fs.create(isA(Path.class), eq((short) 1))).thenReturn(hdfs);when(fs.rename(isA(Path.class), isA(Path.class))).thenReturn(true);Path base = new Path("/chk");Path finalLoc = new Path("/chk/checkpoint_chk0");Path tmp = FSCheckpointService.tmpfile(finalLoc);FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService("chk0"), (short) 1);CheckpointWriteChannel out = chk.create();Random r = new Random();final byte[] randBytes = new byte[BUFSIZE];for (int i = 0; i < WRITES; ++i) {r.nextBytes(randBytes);int s = r.nextInt(BUFSIZE - 1);int e = r.nextInt(BUFSIZE - s) + 1;verif.write(randBytes, s, e);b.clear();b.put(randBytes).flip();b.position(s).limit(b.position() + e);out.write(b);}verify(fs, never()).rename(any(Path.class), eq(finalLoc));CheckpointID cid = chk.commit(out);verify(hdfs).close();verify(fs).rename(eq(tmp), eq(finalLoc));assertArrayEquals(Arrays.copyOfRange(verif.getData(), 0, verif.getLength()), Arrays.copyOfRange(dob.getData(), 0, dob.getLength()));}
public void testDelete() {FileSystem fs = mock(FileSystem.class);Path chkloc = new Path("/chk/chk0");when(fs.delete(eq(chkloc), eq(false))).thenReturn(true);Path base = new Path("/otherchk");FSCheckpointID id = new FSCheckpointID(chkloc);FSCheckpointService chk = new FSCheckpointService(fs, base, new SimpleNamingService("chk0"), (short) 1);assertTrue(chk.delete(id));verify(fs).delete(eq(chkloc), eq(false));}public void setTaskFound(boolean t) {taskFound = t;}
public boolean getTaskFound() {return taskFound;}
public void setPreemption(boolean preemption) {this.preemption = preemption;}
public boolean getPreemption() {return preemption;}
public void write(DataOutput out) {out.writeBoolean(taskFound);out.writeBoolean(preemption);}
public void readFields(DataInput in) {taskFound = in.readBoolean();preemption = in.readBoolean();}public FileHandle getObjFileHandle() {return objFileHandle;}
public Nfs3FileAttributes getObjAttr() {return objAttr;}
public WccData getDirWcc() {return dirWcc;}
public XDR writeHeaderAndResponse(XDR out, int xid, Verifier verifier) {super.writeHeaderAndResponse(out, xid, verifier);if (getStatus() == Nfs3Status.NFS3_OK) {// Handle followsout.writeBoolean(true);objFileHandle.serialize(out);// Attributes followout.writeBoolean(true);objAttr.serialize(out);}dirWcc.serialize(out);return out;}public static void setUp() {// start a clusterConfiguration conf = new HdfsConfiguration();// so that blocks remain under-replicatedconf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 1000);conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1L);conf.setLong(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY, 1L);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).build();cluster.waitActive();fileSys = cluster.getFileSystem();namesystem = cluster.getNamesystem();}
public void testMetaSave() {for (int i = 0; i < 2; i++) {Path file = new Path("/filestatus" + i);DFSTestUtil.createFile(fileSys, file, 1024, 1024, blockSize, (short) 2, seed);}cluster.stopDataNode(1);// wait for namenode to discover that a datanode is deadThread.sleep(15000);namesystem.setReplication("/filestatus0", (short) 4);namesystem.metaSave("metasave.out.txt");// VerificationFileInputStream fstream = new FileInputStream(getLogFile("metasave.out.txt"));DataInputStream in = new DataInputStream(fstream);BufferedReader reader = null;try {reader = new BufferedReader(new InputStreamReader(in));String line = reader.readLine();Assert.assertEquals("3 files and directories, 2 blocks = 5 total filesystem objects", line);line = reader.readLine();assertTrue(line.equals("Live Datanodes: 1"));line = reader.readLine();assertTrue(line.equals("Dead Datanodes: 1"));line = reader.readLine();line = reader.readLine();assertTrue(line.matches("^/filestatus[01]:.*"));} finally {if (reader != null)reader.close();}}
public void testMetasaveAfterDelete() {for (int i = 0; i < 2; i++) {Path file = new Path("/filestatus" + i);DFSTestUtil.createFile(fileSys, file, 1024, 1024, blockSize, (short) 2, seed);}cluster.stopDataNode(1);// wait for namenode to discover that a datanode is deadThread.sleep(15000);namesystem.setReplication("/filestatus0", (short) 4);namesystem.delete("/filestatus0", true);namesystem.delete("/filestatus1", true);namesystem.metaSave("metasaveAfterDelete.out.txt");// VerificationBufferedReader reader = null;try {FileInputStream fstream = new FileInputStream(getLogFile("metasaveAfterDelete.out.txt"));DataInputStream in = new DataInputStream(fstream);reader = new BufferedReader(new InputStreamReader(in));reader.readLine();String line = reader.readLine();assertTrue(line.equals("Live Datanodes: 1"));line = reader.readLine();assertTrue(line.equals("Dead Datanodes: 1"));line = reader.readLine();assertTrue(line.equals("Metasave: Blocks waiting for replication: 0"));line = reader.readLine();assertTrue(line.equals("Mis-replicated blocks that have been postponed:"));line = reader.readLine();assertTrue(line.equals("Metasave: Blocks being replicated: 0"));} finally {if (reader != null)reader.close();}}
public void testMetaSaveOverwrite() {// metaSave twice.namesystem.metaSave("metaSaveOverwrite.out.txt");namesystem.metaSave("metaSaveOverwrite.out.txt");// Read output file.FileInputStream fis = null;InputStreamReader isr = null;BufferedReader rdr = null;try {fis = new FileInputStream(getLogFile("metaSaveOverwrite.out.txt"));isr = new InputStreamReader(fis);rdr = new BufferedReader(isr);// presence of only one "Live Datanodes" line.boolean foundLiveDatanodesLine = false;String line = rdr.readLine();while (line != null) {if (line.startsWith("Live Datanodes")) {if (foundLiveDatanodesLine) {fail("multiple Live Datanodes lines, output file not overwritten");}foundLiveDatanodesLine = true;}line = rdr.readLine();}} finally {IOUtils.cleanup(null, rdr, isr, fis);}}
public static void tearDown() {if (fileSys != null)fileSys.close();if (cluster != null)cluster.shutdown();}
private static File getLogFile(String name) {return new File(System.getProperty("hadoop.log.dir"), name);}public void run(String argv[]) {int exitCode = -1;try {exitCode = pgd.run(argv);} catch (Throwable e) {e.printStackTrace();}System.exit(exitCode);}
public static void main(String argv[]) {new HdfsTestDriver().run(argv);}public String getMessage() {return identifier;}public void setup() {conf.setInt(DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_KEY, KEEPALIVE_TIMEOUT);conf.setInt(DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_KEY, 0);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();dn = cluster.getDataNodes().get(0);}
public void teardown() {cluster.shutdown();}
public void testDatanodeRespectsKeepAliveTimeout() {Configuration clientConf = new Configuration(conf);// the datanode-side expiration time.final long CLIENT_EXPIRY_MS = 60000L;clientConf.setLong(DFS_CLIENT_SOCKET_CACHE_EXPIRY_MSEC_KEY, CLIENT_EXPIRY_MS);clientConf.set(DFS_CLIENT_CONTEXT, "testDatanodeRespectsKeepAliveTimeout");DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(cluster.getURI(), clientConf);PeerCache peerCache = ClientContext.getFromConf(clientConf).getPeerCache();DFSTestUtil.createFile(fs, TEST_FILE, 1L, (short) 1, 0L);// Clients that write aren't currently re-used.assertEquals(0, peerCache.size());assertXceiverCount(0);// cached socket, and should have an xceiver on the other side.DFSTestUtil.readFile(fs, TEST_FILE);assertEquals(1, peerCache.size());assertXceiverCount(1);// and make sure the xceiver died.Thread.sleep(DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_DEFAULT + 1);assertXceiverCount(0);// from it again.assertEquals(1, peerCache.size());// give an EOF.Peer peer = peerCache.get(dn.getDatanodeId(), false);assertNotNull(peer);assertEquals(-1, peer.getInputStream().read());}
public void testClientResponsesKeepAliveTimeout() {Configuration clientConf = new Configuration(conf);// the datanode-side expiration time.final long CLIENT_EXPIRY_MS = 10L;clientConf.setLong(DFS_CLIENT_SOCKET_CACHE_EXPIRY_MSEC_KEY, CLIENT_EXPIRY_MS);clientConf.set(DFS_CLIENT_CONTEXT, "testClientResponsesKeepAliveTimeout");DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(cluster.getURI(), clientConf);PeerCache peerCache = ClientContext.getFromConf(clientConf).getPeerCache();DFSTestUtil.createFile(fs, TEST_FILE, 1L, (short) 1, 0L);// Clients that write aren't currently re-used.assertEquals(0, peerCache.size());assertXceiverCount(0);// cached socket, and should have an xceiver on the other side.DFSTestUtil.readFile(fs, TEST_FILE);assertEquals(1, peerCache.size());assertXceiverCount(1);// Sleep for a bit longer than the client keepalive timeout.Thread.sleep(CLIENT_EXPIRY_MS + 1);// Taking out a peer which is expired should give a null.Peer peer = peerCache.get(dn.getDatanodeId(), false);assertTrue(peer == null);// The socket cache is now empty.assertEquals(0, peerCache.size());}
public void testSlowReader() {// the datanode-side expiration time.final long CLIENT_EXPIRY_MS = 600000L;Configuration clientConf = new Configuration(conf);clientConf.setLong(DFS_CLIENT_SOCKET_CACHE_EXPIRY_MSEC_KEY, CLIENT_EXPIRY_MS);clientConf.set(DFS_CLIENT_CONTEXT, "testSlowReader");DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(cluster.getURI(), clientConf);// Restart the DN with a shorter write timeout.DataNodeProperties props = cluster.stopDataNode(0);props.conf.setInt(DFS_DATANODE_SOCKET_WRITE_TIMEOUT_KEY, WRITE_TIMEOUT);props.conf.setInt(DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_KEY, 120000);assertTrue(cluster.restartDataNode(props, true));dn = cluster.getDataNodes().get(0);// try to write the block while the DN is still starting.cluster.triggerHeartbeats();DFSTestUtil.createFile(fs, TEST_FILE, 1024 * 1024 * 8L, (short) 1, 0L);FSDataInputStream stm = fs.open(TEST_FILE);stm.read();assertXceiverCount(1);GenericTestUtils.waitFor(new Supplier<Boolean>() {
public Boolean get() {// the xceiver to exit.return getXceiverCountWithoutServer() == 0;}}, 500, 50000);IOUtils.closeStream(stm);}
public Boolean get() {// the xceiver to exit.return getXceiverCountWithoutServer() == 0;}
public void testManyClosedSocketsInCache() {Configuration clientConf = new Configuration(conf);clientConf.set(DFS_CLIENT_CONTEXT, "testManyClosedSocketsInCache");DistributedFileSystem fs = (DistributedFileSystem) FileSystem.get(cluster.getURI(), clientConf);PeerCache peerCache = ClientContext.getFromConf(clientConf).getPeerCache();DFSTestUtil.createFile(fs, TEST_FILE, 1L, (short) 1, 0L);// and then closing them.InputStream[] stms = new InputStream[5];try {for (int i = 0; i < stms.length; i++) {stms[i] = fs.open(TEST_FILE);}for (InputStream stm : stms) {IOUtils.copyBytes(stm, new NullOutputStream(), 1024);}} finally {IOUtils.cleanup(null, stms);}assertEquals(5, peerCache.size());// Let all the xceivers timeoutThread.sleep(1500);assertXceiverCount(0);// Client side still has the sockets cachedassertEquals(5, peerCache.size());// Reading should not throw an exception.DFSTestUtil.readFile(fs, TEST_FILE);}
private void assertXceiverCount(int expected) {int count = getXceiverCountWithoutServer();if (count != expected) {ReflectionUtils.printThreadInfo(new PrintWriter(System.err), "Thread dumps");fail("Expected " + expected + " xceivers, found " + count);}}
private int getXceiverCountWithoutServer() {return dn.getXceiverCount() - 1;}public ReadableByteChannel getInputStreamChannel() {return channel;}
public void setReadTimeout(int timeoutMs) {socket.setAttribute(DomainSocket.RECEIVE_TIMEOUT, timeoutMs);}
public int getReceiveBufferSize() {return socket.getAttribute(DomainSocket.RECEIVE_BUFFER_SIZE);}
public boolean getTcpNoDelay() {/* No TCP, no TCP_NODELAY. */return false;}
public void setWriteTimeout(int timeoutMs) {socket.setAttribute(DomainSocket.SEND_TIMEOUT, timeoutMs);}
public boolean isClosed() {return !socket.isOpen();}
public void close() {socket.close();}
public String getRemoteAddressString() {return "unix:" + socket.getPath();}
public String getLocalAddressString() {return "<local>";}
public InputStream getInputStream() {return in;}
public OutputStream getOutputStream() {return out;}
public boolean isLocal() {/* UNIX domain sockets can only be used for local communication. */return true;}
public String toString() {return "DomainPeer(" + getRemoteAddressString() + ")";}
public DomainSocket getDomainSocket() {return socket;}
public boolean hasSecureChannel() {//return true;}public void logAuditEvent(boolean succeeded, String userName, InetAddress addr, String cmd, String src, String dst, FileStatus status) {logAuditEvent(succeeded, userName, addr, cmd, src, dst, status, null, null);}private static Op valueOf(byte code) {final int i = (code & 0xff) - FIRST_CODE;return i < 0 || i >= values().length ? null : values()[i];}
public static Op read(DataInput in) {return valueOf(in.readByte());}
public void write(DataOutput out) {out.write(code);}public String getName() {return name;}
public String getDescription() {return description;}public ContainerId getContainerId() {return containerId;}public String generateEdits() {CheckpointSignature signature = runOperations();return getEditsFilename(signature);}
private String getEditsFilename(CheckpointSignature sig) {FSImage image = cluster.getNameNode().getFSImage();// it was set up to only have ONE StorageDirectoryIterator<StorageDirectory> it = image.getStorage().dirIterator(NameNodeDirType.EDITS);StorageDirectory sd = it.next();File ret = NNStorage.getFinalizedEditsFile(sd, 1, sig.curSegmentTxId - 1);assert ret.exists() : "expected " + ret + " exists";return ret.getAbsolutePath();}
public void startCluster(String dfsDir) {// same as manageDfsDirs but only one edits file instead of twoconfig.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, Util.fileAsURI(new File(dfsDir, "name")).toString());config.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY, Util.fileAsURI(new File(dfsDir, "namesecondary1")).toString());// blocksize for concat (file size must be multiple of blocksize)config.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, blockSize);// for security to work (fake JobTracker user)config.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTH_TO_LOCAL, "RULE:[2:$1@$0](JobTracker@.*FOO.COM)s/@.*//" + "DEFAULT");config.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY, true);config.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);cluster = new MiniDFSCluster.Builder(config).manageNameDfsDirs(false).build();cluster.waitClusterUp();}
public void shutdownCluster() {if (cluster != null) {cluster.shutdown();}}
private CheckpointSignature runOperations() {LOG.info("Creating edits by performing fs operations");// no check, if it's not it throws an exception which is what we wantDistributedFileSystem dfs = cluster.getFileSystem();DFSTestUtil.runOperations(cluster, dfs, cluster.getConfiguration(0), dfs.getDefaultBlockSize(), 0);// OP_ROLLING_UPGRADE_STARTcluster.getNamesystem().getEditLog().logStartRollingUpgrade(Time.now());// OP_ROLLING_UPGRADE_FINALIZEcluster.getNamesystem().getEditLog().logFinalizeRollingUpgrade(Time.now());// Force a roll so we get an OP_END_LOG_SEGMENT txnreturn cluster.getNameNodeRpc().rollEditLog();}public ContainerId getContainerId() {if (this.containerId != null) {return this.containerId;}ContainerFinishDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasContainerId()) {return null;}this.containerId = convertFromProtoFormat(p.getContainerId());return this.containerId;}
public void setContainerId(ContainerId containerId) {maybeInitBuilder();if (containerId == null) {builder.clearContainerId();}this.containerId = containerId;}
public long getFinishTime() {ContainerFinishDataProtoOrBuilder p = viaProto ? proto : builder;return p.getFinishTime();}
public void setFinishTime(long finishTime) {maybeInitBuilder();builder.setFinishTime(finishTime);}
public String getDiagnosticsInfo() {ContainerFinishDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasDiagnosticsInfo()) {return null;}return p.getDiagnosticsInfo();}
public void setDiagnosticsInfo(String diagnosticsInfo) {maybeInitBuilder();if (diagnosticsInfo == null) {builder.clearDiagnosticsInfo();return;}builder.setDiagnosticsInfo(diagnosticsInfo);}
public int getContainerExitStatus() {ContainerFinishDataProtoOrBuilder p = viaProto ? proto : builder;return p.getContainerExitStatus();}
public ContainerState getContainerState() {ContainerFinishDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasContainerState()) {return null;}return convertFromProtoFormat(p.getContainerState());}
public void setContainerState(ContainerState state) {maybeInitBuilder();if (state == null) {builder.clearContainerState();return;}builder.setContainerState(convertToProtoFormat(state));}
public void setContainerExitStatus(int containerExitStatus) {maybeInitBuilder();builder.setContainerExitStatus(containerExitStatus);}
public ContainerFinishDataProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
private void mergeLocalToBuilder() {if (this.containerId != null && !((ContainerIdPBImpl) this.containerId).getProto().equals(builder.getContainerId())) {builder.setContainerId(convertToProtoFormat(this.containerId));}}
private void mergeLocalToProto() {if (viaProto) {maybeInitBuilder();}mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ContainerFinishDataProto.newBuilder(proto);}viaProto = false;}
private ContainerIdProto convertToProtoFormat(ContainerId containerId) {return ((ContainerIdPBImpl) containerId).getProto();}
private ContainerIdPBImpl convertFromProtoFormat(ContainerIdProto containerId) {return new ContainerIdPBImpl(containerId);}
private ContainerStateProto convertToProtoFormat(ContainerState state) {return ProtoUtils.convertToProtoFormat(state);}
private ContainerState convertFromProtoFormat(ContainerStateProto containerState) {return ProtoUtils.convertFromProtoFormat(containerState);}protected Object doExecute(Object... arguments) {if (++retryCount < succeedAfter)throw new Exception("Transient failure#" + retryCount);return 0;}
public void testRetriableCommand() {try {new MyRetriableCommand(5).execute(0);Assert.assertTrue(false);} catch (Exception e) {Assert.assertTrue(true);}try {new MyRetriableCommand(3).execute(0);Assert.assertTrue(true);} catch (Exception e) {Assert.assertTrue(false);}try {new MyRetriableCommand(5, RetryPolicies.retryUpToMaximumCountWithFixedSleep(5, 0, TimeUnit.MILLISECONDS)).execute(0);Assert.assertTrue(true);} catch (Exception e) {Assert.assertTrue(false);}}public long getProtocolVersion(String protocol, long clientVersion) {return Foo0.versionID;}
public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) {Class<? extends VersionedProtocol> inter;try {inter = (Class<? extends VersionedProtocol>) getClass().getGenericInterfaces()[0];} catch (Exception e) {throw new IOException(e);}return ProtocolSignature.getProtocolSignature(clientMethodsHash, getProtocolVersion(protocol, clientVersion), inter);}
public String ping() {return "Foo0";}
public long getProtocolVersion(String protocol, long clientVersion) {return Foo1.versionID;}
public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) {Class<? extends VersionedProtocol> inter;try {inter = (Class<? extends VersionedProtocol>) getClass().getGenericInterfaces()[0];} catch (Exception e) {throw new IOException(e);}return ProtocolSignature.getProtocolSignature(clientMethodsHash, getProtocolVersion(protocol, clientVersion), inter);}
public String ping() {return "Foo1";}
public String ping2() {return "Foo1";}
public long getProtocolVersion(String protocol, long clientVersion) {return Bar.versionID;}
public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) {Class<? extends VersionedProtocol> inter;try {inter = (Class<? extends VersionedProtocol>) getClass().getGenericInterfaces()[0];} catch (Exception e) {throw new IOException(e);}return ProtocolSignature.getProtocolSignature(clientMethodsHash, getProtocolVersion(protocol, clientVersion), inter);}
public int echo(int i) {return i;}
public void hello() {}
public void setUp() {// create a server with two handlersserver = new RPC.Builder(conf).setProtocol(Foo0.class).setInstance(new Foo0Impl()).setBindAddress(ADDRESS).setPort(0).setNumHandlers(2).setVerbose(false).build();server.addProtocol(RPC.RpcKind.RPC_WRITABLE, Foo1.class, new Foo1Impl());server.addProtocol(RPC.RpcKind.RPC_WRITABLE, Bar.class, new BarImpl());server.addProtocol(RPC.RpcKind.RPC_WRITABLE, Mixin.class, new BarImpl());// Create server side implementationPBServerImpl pbServerImpl = new PBServerImpl();BlockingService service = TestProtobufRpcProto.newReflectiveBlockingService(pbServerImpl);server.addProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER, TestRpcService.class, service);server.start();addr = NetUtils.getConnectAddress(server);}
public void tearDown() {server.stop();}
public void test1() {ProtocolProxy<?> proxy;proxy = RPC.getProtocolProxy(Foo0.class, Foo0.versionID, addr, conf);Foo0 foo0 = (Foo0) proxy.getProxy();Assert.assertEquals("Foo0", foo0.ping());proxy = RPC.getProtocolProxy(Foo1.class, Foo1.versionID, addr, conf);Foo1 foo1 = (Foo1) proxy.getProxy();Assert.assertEquals("Foo1", foo1.ping());Assert.assertEquals("Foo1", foo1.ping());proxy = RPC.getProtocolProxy(Bar.class, Foo1.versionID, addr, conf);Bar bar = (Bar) proxy.getProxy();Assert.assertEquals(99, bar.echo(99));Mixin mixin = bar;mixin.hello();}
public void testNonExistingProtocol() {ProtocolProxy<?> proxy;proxy = RPC.getProtocolProxy(FooUnimplemented.class, FooUnimplemented.versionID, addr, conf);FooUnimplemented foo = (FooUnimplemented) proxy.getProxy();foo.ping();}
public void testNonExistingProtocol2() {ProtocolProxy<?> proxy;proxy = RPC.getProtocolProxy(FooUnimplemented.class, FooUnimplemented.versionID, addr, conf);FooUnimplemented foo = (FooUnimplemented) proxy.getProxy();Assert.assertEquals(Foo1.versionID, foo.getProtocolVersion(RPC.getProtocolName(FooUnimplemented.class), FooUnimplemented.versionID));foo.getProtocolSignature(RPC.getProtocolName(FooUnimplemented.class), FooUnimplemented.versionID, 0);}
public void testIncorrectServerCreation() {new RPC.Builder(conf).setProtocol(Foo1.class).setInstance(new Foo0Impl()).setBindAddress(ADDRESS).setPort(0).setNumHandlers(2).setVerbose(false).build();}
public void testPBService() {// Set RPC engine to protobuf RPC engineConfiguration conf2 = new Configuration();RPC.setProtocolEngine(conf2, TestRpcService.class, ProtobufRpcEngine.class);TestRpcService client = RPC.getProxy(TestRpcService.class, 0, addr, conf2);TestProtoBufRpc.testProtoBufRpc(client);}private void throwExceptionOnError(String tag) {if (stream.checkError()) {throw new IOException("Error serializing " + tag);}}
private void printCommaUnlessFirst() {if (!isFirst) {stream.print(",");}isFirst = false;}
public void writeByte(byte b, String tag) {writeLong((long) b, tag);}
public void writeBool(boolean b, String tag) {printCommaUnlessFirst();String val = b ? "T" : "F";stream.print(val);throwExceptionOnError(tag);}
public void writeInt(int i, String tag) {writeLong((long) i, tag);}
public void writeLong(long l, String tag) {printCommaUnlessFirst();stream.print(l);throwExceptionOnError(tag);}
public void writeFloat(float f, String tag) {writeDouble((double) f, tag);}
public void writeDouble(double d, String tag) {printCommaUnlessFirst();stream.print(d);throwExceptionOnError(tag);}
public void writeString(String s, String tag) {printCommaUnlessFirst();stream.print(Utils.toCSVString(s));throwExceptionOnError(tag);}
public void writeBuffer(Buffer buf, String tag) {printCommaUnlessFirst();stream.print(Utils.toCSVBuffer(buf));throwExceptionOnError(tag);}
public void startRecord(Record r, String tag) {if (tag != null && !tag.isEmpty()) {printCommaUnlessFirst();stream.print("s{");isFirst = true;}}
public void endRecord(Record r, String tag) {if (tag == null || tag.isEmpty()) {stream.print("\n");isFirst = true;} else {stream.print("}");isFirst = false;}}
public void startVector(ArrayList v, String tag) {printCommaUnlessFirst();stream.print("v{");isFirst = true;}
public void endVector(ArrayList v, String tag) {stream.print("}");isFirst = false;}
public void startMap(TreeMap v, String tag) {printCommaUnlessFirst();stream.print("m{");isFirst = true;}
public void endMap(TreeMap v, String tag) {stream.print("}");isFirst = false;}private void checkRange(final Long min, final Long max) {if (value == null) {return;}if (min != null && value < min) {throw new IllegalArgumentException("Invalid parameter range: " + getName() + " = " + domain.toString(value) + " < " + domain.toString(min));}if (max != null && value > max) {throw new IllegalArgumentException("Invalid parameter range: " + getName() + " = " + domain.toString(value) + " > " + domain.toString(max));}}
public String toString() {return getName() + "=" + domain.toString(getValue());}
public String getValueString() {return domain.toString(getValue());}
public String getDomain() {return "<" + NULL + " | long in radix " + radix + ">";}
Long parse(final String str) {try {return NULL.equals(str) ? null : Long.parseLong(str, radix);} catch (NumberFormatException e) {throw new IllegalArgumentException("Failed to parse \"" + str + "\" as a radix-" + radix + " long integer.", e);}}
String toString(final Long n) {return n == null ? NULL : Long.toString(n, radix);}private StorageCredentials getCredentials() {return storageCreds;}
private boolean isOutOfBandIoAllowed() {return allowConcurrentOOBIo;}
private OperationContext getOperationContext() {return opContext;}
public static void bind(StorageCredentials storageCreds, OperationContext opContext, boolean allowConcurrentOOBIo) {SendRequestIntercept sendListener = new SendRequestIntercept(storageCreds, allowConcurrentOOBIo, opContext);opContext.getSendingRequestEventHandler().addListener(sendListener);}
public void eventOccurred(SendingRequestEvent sendEvent) {if (!(sendEvent.getConnectionObject() instanceof HttpURLConnection)) {// request.return;}// the request.HttpURLConnection urlConnection = (HttpURLConnection) sendEvent.getConnectionObject();// is a "GET" operation.if (urlConnection.getRequestMethod().equalsIgnoreCase("GET") && isOutOfBandIoAllowed()) {// condition on the conditional header.urlConnection.setRequestProperty(HeaderConstants.IF_MATCH, ALLOW_ALL_REQUEST_PRECONDITIONS);// packet, therefore the packet has to be re-signed.try {// zero.getCredentials().signBlobAndQueueRequest(urlConnection, -1L, getOperationContext());} catch (InvalidKeyException e) {String errString = String.format("Received invalid key exception when attempting sign packet." + " Cause: %s", e.getCause().toString());LOG.error(errString);} catch (StorageException e) {String errString = String.format("Received storage exception when attempting to sign packet." + " Cause: %s", e.getCause().toString());LOG.error(errString);}}}public void init(FilterConfig filterConfig) {}
public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) {try {filterChain.doFilter(servletRequest, servletResponse);} finally {FileSystem fs = FILE_SYSTEM_TL.get();if (fs != null) {FILE_SYSTEM_TL.remove();getFileSystemAccess().releaseFileSystem(fs);}}}
public void destroy() {}
public static void setFileSystem(FileSystem fs) {FILE_SYSTEM_TL.set(fs);}public Block getBlock() {return block;}
public String[] getDatanodeUuids() {return datanodeUuids;}
public String[] getStorageIDs() {return storageIDs;}
public StorageType[] getStorageTypes() {return storageTypes;}
public String toString() {final StringBuilder b = new StringBuilder();b.append(block);if (datanodeUuids.length == 0) {return b.append("[]").toString();}appendString(0, b.append("["));for (int i = 1; i < datanodeUuids.length; i++) {appendString(i, b.append(","));}return b.append("]").toString();}
private StringBuilder appendString(int i, StringBuilder b) {return b.append("[").append(storageTypes[i]).append("]").append(storageIDs[i]).append("@").append(datanodeUuids[i]);}
public BlockWithLocations[] getBlocks() {return blocks;}protected MessageDigest initialValue() {try {return MessageDigest.getInstance("MD5");} catch (NoSuchAlgorithmException e) {throw new RuntimeException(e);}}
public void readFields(DataInput in) {in.readFully(digest);}
public static MD5Hash read(DataInput in) {MD5Hash result = new MD5Hash();result.readFields(in);return result;}
public void write(DataOutput out) {out.write(digest);}
public void set(MD5Hash that) {System.arraycopy(that.digest, 0, this.digest, 0, MD5_LEN);}
public byte[] getDigest() {return digest;}
public static MD5Hash digest(byte[] data) {return digest(data, 0, data.length);}
public static MessageDigest getDigester() {MessageDigest digester = DIGESTER_FACTORY.get();digester.reset();return digester;}
public static MD5Hash digest(InputStream in) {final byte[] buffer = new byte[4 * 1024];final MessageDigest digester = getDigester();for (int n; (n = in.read(buffer)) != -1; ) {digester.update(buffer, 0, n);}return new MD5Hash(digester.digest());}
public static MD5Hash digest(byte[] data, int start, int len) {byte[] digest;MessageDigest digester = getDigester();digester.update(data, start, len);digest = digester.digest();return new MD5Hash(digest);}
public static MD5Hash digest(String string) {return digest(UTF8.getBytes(string));}
public static MD5Hash digest(UTF8 utf8) {return digest(utf8.getBytes(), 0, utf8.getLength());}
public long halfDigest() {long value = 0;for (int i = 0; i < 8; i++) value |= ((digest[i] & 0xffL) << (8 * (7 - i)));return value;}
public int quarterDigest() {int value = 0;for (int i = 0; i < 4; i++) value |= ((digest[i] & 0xff) << (8 * (3 - i)));return value;}
public boolean equals(Object o) {if (!(o instanceof MD5Hash))return false;MD5Hash other = (MD5Hash) o;return Arrays.equals(this.digest, other.digest);}
public int hashCode() {return quarterDigest();}
public int compareTo(MD5Hash that) {return WritableComparator.compareBytes(this.digest, 0, MD5_LEN, that.digest, 0, MD5_LEN);}
public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {return compareBytes(b1, s1, MD5_LEN, b2, s2, MD5_LEN);}
public String toString() {StringBuilder buf = new StringBuilder(MD5_LEN * 2);for (int i = 0; i < MD5_LEN; i++) {int b = digest[i];buf.append(HEX_DIGITS[(b >> 4) & 0xf]);buf.append(HEX_DIGITS[b & 0xf]);}return buf.toString();}
public void setDigest(String hex) {if (hex.length() != MD5_LEN * 2)throw new IllegalArgumentException("Wrong length: " + hex.length());byte[] digest = new byte[MD5_LEN];for (int i = 0; i < MD5_LEN; i++) {int j = i << 1;digest[i] = (byte) (charToNibble(hex.charAt(j)) << 4 | charToNibble(hex.charAt(j + 1)));}this.digest = digest;}
private static final int charToNibble(char c) {if (c >= '0' && c <= '9') {return c - '0';} else if (c >= 'a' && c <= 'f') {return 0xa + (c - 'a');} else if (c >= 'A' && c <= 'F') {return 0xA + (c - 'A');} else {throw new RuntimeException("Not a hex character: " + c);}}public static Class getClassByName(String className) {Class retv = null;try {ClassLoader classLoader = Thread.currentThread().getContextClassLoader();retv = Class.forName(className, true, classLoader);} catch (Exception e) {throw new RuntimeException(e);}return retv;}
public static JobConf createDataJoinJob(String args[]) {String inputDir = args[0];String outputDir = args[1];Class inputFormat = SequenceFileInputFormat.class;if (args[2].compareToIgnoreCase("text") != 0) {System.out.println("Using SequenceFileInputFormat: " + args[2]);} else {System.out.println("Using TextInputFormat: " + args[2]);inputFormat = TextInputFormat.class;}int numOfReducers = Integer.parseInt(args[3]);Class mapper = getClassByName(args[4]);Class reducer = getClassByName(args[5]);Class mapoutputValueClass = getClassByName(args[6]);Class outputFormat = TextOutputFormat.class;Class outputValueClass = Text.class;if (args[7].compareToIgnoreCase("text") != 0) {System.out.println("Using SequenceFileOutputFormat: " + args[7]);outputFormat = SequenceFileOutputFormat.class;outputValueClass = getClassByName(args[7]);} else {System.out.println("Using TextOutputFormat: " + args[7]);}long maxNumOfValuesPerGroup = 100;String jobName = "";if (args.length > 8) {maxNumOfValuesPerGroup = Long.parseLong(args[8]);}if (args.length > 9) {jobName = args[9];}Configuration defaults = new Configuration();JobConf job = new JobConf(defaults, DataJoinJob.class);job.setJobName("DataJoinJob: " + jobName);FileSystem fs = FileSystem.get(defaults);fs.delete(new Path(outputDir), true);FileInputFormat.setInputPaths(job, inputDir);job.setInputFormat(inputFormat);job.setMapperClass(mapper);FileOutputFormat.setOutputPath(job, new Path(outputDir));job.setOutputFormat(outputFormat);SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);job.setMapOutputKeyClass(Text.class);job.setMapOutputValueClass(mapoutputValueClass);job.setOutputKeyClass(Text.class);job.setOutputValueClass(outputValueClass);job.setReducerClass(reducer);job.setNumMapTasks(1);job.setNumReduceTasks(numOfReducers);job.setLong("datajoin.maxNumOfValuesPerGroup", maxNumOfValuesPerGroup);return job;}
public static boolean runJob(JobConf job) {JobClient jc = new JobClient(job);boolean sucess = true;RunningJob running = null;try {running = jc.submitJob(job);JobID jobId = running.getID();System.out.println("Job " + jobId + " is submitted");while (!running.isComplete()) {System.out.println("Job " + jobId + " is still running.");try {Thread.sleep(60000);} catch (InterruptedException e) {}running = jc.getJob(jobId);}sucess = running.isSuccessful();} finally {if (!sucess && (running != null)) {running.killJob();}jc.close();}return sucess;}
public static void main(String[] args) {boolean success;if (args.length < 8 || args.length > 10) {System.out.println("usage: DataJoinJob " + "inputdirs outputdir map_input_file_format " + "numofParts " + "mapper_class " + "reducer_class " + "map_output_value_class " + "output_value_class [maxNumOfValuesPerGroup [descriptionOfJob]]]");System.exit(-1);}try {JobConf job = DataJoinJob.createDataJoinJob(args);success = DataJoinJob.runJob(job);if (!success) {System.out.println("Job failed");}} catch (IOException ioe) {ioe.printStackTrace();}}public void stateChanged(Service service) {log.info("Entry to state " + service.getServiceState() + " for " + service.getName());}public void testNoIntern() {String literalABC = "ABC";String substringABC = "ABCDE".substring(0, 3);String heapABC = new String("ABC");assertNotSame(literalABC, substringABC);assertNotSame(literalABC, heapABC);assertNotSame(substringABC, heapABC);}
public void testStrongIntern() {String strongInternLiteralABC = strongIntern("ABC");String strongInternSubstringABC = strongIntern("ABCDE".substring(0, 3));String strongInternHeapABC = strongIntern(new String("ABC"));assertSame(strongInternLiteralABC, strongInternSubstringABC);assertSame(strongInternLiteralABC, strongInternHeapABC);assertSame(strongInternSubstringABC, strongInternHeapABC);}
public void testWeakIntern() {String weakInternLiteralABC = weakIntern("ABC");String weakInternSubstringABC = weakIntern("ABCDE".substring(0, 3));String weakInternHeapABC = weakIntern(new String("ABC"));assertSame(weakInternLiteralABC, weakInternSubstringABC);assertSame(weakInternLiteralABC, weakInternHeapABC);assertSame(weakInternSubstringABC, weakInternHeapABC);}public void testScaledWriteThenRead() {Path dir = new Path("/test/manysmallfiles");Duration rm1 = new Duration();fs.delete(dir, true);rm1.finished();fs.mkdirs(dir);Duration ls1 = new Duration();fs.listStatus(dir);ls1.finished();long count = getOperationCount();SwiftTestUtils.noteAction("Beginning Write of " + count + " files ");DurationStats writeStats = new DurationStats("write");DurationStats readStats = new DurationStats("read");String format = "%08d";for (long l = 0; l < count; l++) {String name = String.format(format, l);Path p = new Path(dir, "part-" + name);Duration d = new Duration();SwiftTestUtils.writeTextFile(fs, p, name, false);d.finished();writeStats.add(d);Thread.sleep(1000);}SwiftTestUtils.noteAction("Beginning ls");Duration ls2 = new Duration();FileStatus[] status2 = (FileStatus[]) fs.listStatus(dir);ls2.finished();assertEquals("Not enough entries in the directory", count, status2.length);SwiftTestUtils.noteAction("Beginning read");for (long l = 0; l < count; l++) {String name = String.format(format, l);Path p = new Path(dir, "part-" + name);Duration d = new Duration();String result = SwiftTestUtils.readBytesToString(fs, p, name.length());assertEquals(name, result);d.finished();readStats.add(d);}//do a recursive deleteSwiftTestUtils.noteAction("Beginning delete");Duration rm2 = new Duration();fs.delete(dir, true);rm2.finished();//print the statsLOG.info(String.format("'filesystem','%s'", fs.getUri()));LOG.info(writeStats.toString());LOG.info(readStats.toString());LOG.info(String.format("'rm1',%d,'ls1',%d", rm1.value(), ls1.value()));LOG.info(String.format("'rm2',%d,'ls2',%d", rm2.value(), ls2.value()));}public Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) {LOG.debug("Creating a HadoopYarnProtoRpc proxy for protocol " + protocol);return RpcFactoryProvider.getClientFactory(conf).getClient(protocol, 1, addr, conf);}
public void stopProxy(Object proxy, Configuration conf) {RpcFactoryProvider.getClientFactory(conf).stopClient(proxy);}
public Server getServer(Class protocol, Object instance, InetSocketAddress addr, Configuration conf, SecretManager<? extends TokenIdentifier> secretManager, int numHandlers, String portRangeConfig) {LOG.debug("Creating a HadoopYarnProtoRpc server for protocol " + protocol + " with " + numHandlers + " handlers");return RpcFactoryProvider.getServerFactory(conf).getServer(protocol, instance, addr, conf, secretManager, numHandlers, portRangeConfig);}public String getApplicationId() {return applicationId;}
public ResourceInfo getMaximumResourceCapability() {return maximumResourceCapability;}public static void setTokenServiceUseIp(boolean flag) {SecurityUtil.setTokenServiceUseIp(flag);}
public static boolean isExternalKdcRunning() {String externalKdc = System.getProperty("externalKdc");String krb5Conf = System.getProperty("java.security.krb5.conf");if (externalKdc == null || !externalKdc.equals("true") || krb5Conf == null) {return false;}return true;}public ApplicationId getApplicationId() {return this.appId;}public TaskAttemptStatus getReportedStatus() {return reportedStatus;}
public int containersNeededChange() {return containersNeededChange;}
public TaskId getTaskID() {return taskID;}
public JobId getJobID() {return jobID;}public void go(int seconds) {BufferedReader in = new BufferedReader(new InputStreamReader(System.in));String line;// task as failed even if all input was consumed).while ((line = in.readLine()) != null) {Thread.sleep(seconds * 1000L);System.out.println(line);}}
public static void main(String[] args) {int seconds = 5;if (args.length >= 1) {try {seconds = Integer.valueOf(args[0]);} catch (NumberFormatException e) {}}DelayEchoApp app = new DelayEchoApp();app.go(seconds);}public void start() {// Format the base dir, should succeedthrs = new DummyZKFCThread[2];thrs[0] = new DummyZKFCThread(ctx, svcs[0]);assertEquals(0, thrs[0].zkfc.run(new String[] { "-formatZK" }));ctx.addThread(thrs[0]);thrs[0].start();LOG.info("Waiting for svc0 to enter active state");waitForHAState(0, HAServiceState.ACTIVE);LOG.info("Adding svc1");thrs[1] = new DummyZKFCThread(ctx, svcs[1]);thrs[1].start();waitForHAState(1, HAServiceState.STANDBY);}
public void stop() {for (DummyZKFCThread thr : thrs) {if (thr != null) {thr.interrupt();}}if (ctx != null) {ctx.stop();}sharedResource.assertNoViolations();}
public TestContext getTestContext() {return ctx;}
public DummyHAService getService(int i) {return svcs[i];}
public ActiveStandbyElector getElector(int i) {return thrs[i].zkfc.getElectorForTests();}
public DummyZKFC getZkfc(int i) {return thrs[i].zkfc;}
public void setHealthy(int idx, boolean healthy) {svcs[idx].isHealthy = healthy;}
public void setFailToBecomeActive(int idx, boolean doFail) {svcs[idx].failToBecomeActive = doFail;}
public void setFailToBecomeStandby(int idx, boolean doFail) {svcs[idx].failToBecomeStandby = doFail;}
public void setFailToFence(int idx, boolean doFail) {svcs[idx].failToFence = doFail;}
public void setUnreachable(int idx, boolean unreachable) {svcs[idx].actUnreachable = unreachable;}
public void waitForHAState(int idx, HAServiceState state) {DummyHAService svc = getService(idx);while (svc.state != state) {ctx.checkException();Thread.sleep(50);}}
public void waitForHealthState(int idx, State state) {ZKFCTestUtil.waitForHealthState(thrs[idx].zkfc, state, ctx);}
public void waitForElectorState(int idx, ActiveStandbyElector.State state) {ActiveStandbyElectorTestUtil.waitForElectorState(ctx, getElector(idx), state);}
public void expireActiveLockHolder(int idx) {Stat stat = new Stat();byte[] data = zks.getZKDatabase().getData(DummyZKFC.LOCK_ZNODE, stat, null);assertArrayEquals(Ints.toByteArray(svcs[idx].index), data);long session = stat.getEphemeralOwner();LOG.info("Expiring svc " + idx + "'s zookeeper session " + session);zks.closeSession(session);}
public void waitForActiveLockHolder(Integer idx) {DummyHAService svc = idx == null ? null : svcs[idx];ActiveStandbyElectorTestUtil.waitForActiveLockData(ctx, zks, DummyZKFC.SCOPED_PARENT_ZNODE, (idx == null) ? null : Ints.toByteArray(svc.index));}
public void expireAndVerifyFailover(int fromIdx, int toIdx) {Preconditions.checkArgument(fromIdx != toIdx);getElector(fromIdx).preventSessionReestablishmentForTests();try {expireActiveLockHolder(fromIdx);waitForHAState(fromIdx, HAServiceState.STANDBY);waitForHAState(toIdx, HAServiceState.ACTIVE);} finally {getElector(fromIdx).allowSessionReestablishmentForTests();}}
public void doWork() {try {assertEquals(0, zkfc.run(new String[0]));} catch (InterruptedException ie) {}}
protected byte[] targetToData(HAServiceTarget target) {return Ints.toByteArray(((DummyHAService) target).index);}
protected HAServiceTarget dataToTarget(byte[] data) {int index = Ints.fromByteArray(data);return DummyHAService.getInstance(index);}
protected void loginAsFCUser() {}
protected String getScopeInsideParentNode() {return DUMMY_CLUSTER;}
protected void checkRpcAdminAccess() {}
protected InetSocketAddress getRpcAddressToBindTo() {return new InetSocketAddress(0);}
protected void initRPC() {super.initRPC();localTarget.zkfcProxy = this.getRpcServerForTests();}
protected PolicyProvider getPolicyProvider() {return null;}long getBytesWritten() {return bytes;}
long getTimeTaken() {return time;}
public String toString() {return "Wrote " + getBytesWritten() + " bytes " + " which took " + getTimeTaken() + " milliseconds";}
long getOffset() {return offset;}
ByteBuffer getBuffer() {return buffer;}
long getHashValue() {return hashValue;}
long getTimeTaken() {return timeTaken;}
long getBytesWritten() {return bytesWritten;}
private GenerateResult generatePartialSegment(int byteAm, long offset, DataHasher hasher) {if (byteAm > BYTES_PER_LONG) {throw new IllegalArgumentException("Partial bytes must be less or equal to " + BYTES_PER_LONG);}if (byteAm <= 0) {throw new IllegalArgumentException("Partial bytes must be greater than zero and not " + byteAm);}ByteBuffer buf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);buf.putLong(hasher.generate(offset));ByteBuffer allBytes = ByteBuffer.wrap(new byte[byteAm]);buf.rewind();for (int i = 0; i < byteAm; ++i) {allBytes.put(buf.get());}allBytes.rewind();return new GenerateResult(offset, allBytes);}
private GenerateResult generateFullSegment(int byteAm, long startOffset, DataHasher hasher) {if (byteAm <= 0) {throw new IllegalArgumentException("Byte amount must be greater than zero and not " + byteAm);}if ((byteAm % BYTES_PER_LONG) != 0) {throw new IllegalArgumentException("Byte amount " + byteAm + " must be a multiple of " + BYTES_PER_LONG);}// generate all the segmentsByteBuffer allBytes = ByteBuffer.wrap(new byte[byteAm]);long offset = startOffset;ByteBuffer buf = ByteBuffer.wrap(new byte[BYTES_PER_LONG]);for (long i = 0; i < byteAm; i += BYTES_PER_LONG) {buf.rewind();buf.putLong(hasher.generate(offset));buf.rewind();allBytes.put(buf);offset += BYTES_PER_LONG;}allBytes.rewind();return new GenerateResult(offset, allBytes);}
private GenerateOutput writePieces(long byteAm, long startPos, DataHasher hasher, OutputStream out) {if (byteAm <= 0) {return new GenerateOutput(0, 0);}if (startPos < 0) {startPos = 0;}int leftOver = (int) (byteAm % bufferSize);long fullPieces = byteAm / bufferSize;long offset = startPos;long bytesWritten = 0;long timeTaken = 0;// write the full pieces that fit in the buffer sizefor (long i = 0; i < fullPieces; ++i) {GenerateResult genData = generateFullSegment(bufferSize, offset, hasher);offset = genData.getOffset();ByteBuffer gBuf = genData.getBuffer();{byte[] buf = gBuf.array();long startTime = Timer.now();out.write(buf);if (Constants.FLUSH_WRITES) {out.flush();}timeTaken += Timer.elapsed(startTime);bytesWritten += buf.length;}}if (leftOver > 0) {ByteBuffer leftOverBuf = ByteBuffer.wrap(new byte[leftOver]);int bytesLeft = leftOver % BYTES_PER_LONG;leftOver = leftOver - bytesLeft;// also greater or eq than BYTES_PER_LONG and a multiple of itif (leftOver > 0) {GenerateResult genData = generateFullSegment(leftOver, offset, hasher);offset = genData.getOffset();leftOverBuf.put(genData.getBuffer());}// collect any single partial byte segmentif (bytesLeft > 0) {GenerateResult genData = generatePartialSegment(bytesLeft, offset, hasher);offset = genData.getOffset();leftOverBuf.put(genData.getBuffer());}// do the write of bothleftOverBuf.rewind();{byte[] buf = leftOverBuf.array();long startTime = Timer.now();out.write(buf);if (Constants.FLUSH_WRITES) {out.flush();}timeTaken += Timer.elapsed(startTime);bytesWritten += buf.length;}}return new GenerateOutput(bytesWritten, timeTaken);}
GenerateOutput writeSegment(long byteAm, OutputStream out) {long headerLen = getHeaderLength();if (byteAm < headerLen) {// not enough bytes to write even the headerreturn new GenerateOutput(0, 0);}// adjust for header lengthbyteAm -= headerLen;if (byteAm < 0) {byteAm = 0;}WriteInfo header = writeHeader(out, byteAm);DataHasher hasher = new DataHasher(header.getHashValue());GenerateOutput pRes = writePieces(byteAm, 0, hasher, out);long bytesWritten = pRes.getBytesWritten() + header.getBytesWritten();long timeTaken = header.getTimeTaken() + pRes.getTimeTaken();return new GenerateOutput(bytesWritten, timeTaken);}
static int getHeaderLength() {return HEADER_LENGTH;}
WriteInfo writeHeader(OutputStream os, long fileSize) {int headerLen = getHeaderLength();ByteBuffer buf = ByteBuffer.wrap(new byte[headerLen]);long hash = rnd.nextLong();buf.putLong(hash);buf.putLong(fileSize);buf.rewind();byte[] headerData = buf.array();long elapsed = 0;{long startTime = Timer.now();os.write(headerData);if (Constants.FLUSH_WRITES) {os.flush();}elapsed += Timer.elapsed(startTime);}return new WriteInfo(hash, headerLen, elapsed);}public void testQueueConfigurationParser() {JobQueueInfo info = new JobQueueInfo("root", "rootInfo");JobQueueInfo infoChild1 = new JobQueueInfo("child1", "child1Info");JobQueueInfo infoChild2 = new JobQueueInfo("child2", "child1Info");info.addChild(infoChild1);info.addChild(infoChild2);DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance();DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();Document document = builder.newDocument();// test QueueConfigurationParser.getQueueElement Element e = QueueConfigurationParser.getQueueElement(document, info);// transform result to string for checkDOMSource domSource = new DOMSource(e);StringWriter writer = new StringWriter();StreamResult result = new StreamResult(writer);TransformerFactory tf = TransformerFactory.newInstance();Transformer transformer = tf.newTransformer();transformer.transform(domSource, result);String str = writer.toString();assertTrue(str.endsWith("<queue><name>root</name><properties/><state>running</state><queue><name>child1</name><properties/><state>running</state></queue><queue><name>child2</name><properties/><state>running</state></queue></queue>"));}public static void setUp() {JobConf conf = new JobConf();fileSys = FileSystem.get(conf);fileSys.delete(new Path(TEST_ROOT_DIR), true);conf.set("mapred.job.tracker.handler.count", "1");conf.set("mapred.job.tracker", "127.0.0.1:0");conf.set("mapred.job.tracker.http.address", "127.0.0.1:0");conf.set("mapred.task.tracker.http.address", "127.0.0.1:0");conf.set(JHAdminConfig.MR_HISTORY_INTERMEDIATE_DONE_DIR, TEST_ROOT_DIR + "/intermediate");conf.set(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, "true");mr = new MiniMRCluster(1, "file:///", 1, null, null, conf);inDir = new Path(TEST_ROOT_DIR, "test-input");String input = "The quick brown fox\n" + "has many silly\n" + "red fox sox\n";DataOutputStream file = fileSys.create(new Path(inDir, "part-" + 0));file.writeBytes(input);file.close();emptyInDir = new Path(TEST_ROOT_DIR, "empty-input");fileSys.mkdirs(emptyInDir);}
public static void tearDown() {if (fileSys != null) {// fileSys.delete(new Path(TEST_ROOT_DIR), true);fileSys.close();}if (mr != null) {mr.shutdown();}}
public void cleanupJob(JobContext context) {System.err.println("---- HERE ----");JobConf conf = context.getJobConf();Path outputPath = FileOutputFormat.getOutputPath(conf);FileSystem fs = outputPath.getFileSystem(conf);fs.create(new Path(outputPath, CUSTOM_CLEANUP_FILE_NAME)).close();}
public void commitJob(JobContext context) {cleanupJob(context);}
public void abortJob(JobContext context, int i) {cleanupJob(context);}
public void abortJob(JobContext context, int state) {JobConf conf = context.getJobConf();;Path outputPath = FileOutputFormat.getOutputPath(conf);FileSystem fs = outputPath.getFileSystem(conf);String fileName = (state == JobStatus.FAILED) ? TestJobCleanup.ABORT_FAILED_FILE_NAME : TestJobCleanup.ABORT_KILLED_FILE_NAME;fs.create(new Path(outputPath, fileName)).close();}
private Path getNewOutputDir() {return new Path(TEST_ROOT_DIR, "output-" + outDirs++);}
private void configureJob(JobConf jc, String jobName, int maps, int reds, Path outDir) {jc.setJobName(jobName);jc.setInputFormat(TextInputFormat.class);jc.setOutputKeyClass(LongWritable.class);jc.setOutputValueClass(Text.class);FileInputFormat.setInputPaths(jc, inDir);FileOutputFormat.setOutputPath(jc, outDir);jc.setMapperClass(IdentityMapper.class);jc.setReducerClass(IdentityReducer.class);jc.setNumMapTasks(maps);jc.setNumReduceTasks(reds);}
private void testSuccessfulJob(String filename, Class<? extends OutputCommitter> committer, String[] exclude) {JobConf jc = mr.createJobConf();Path outDir = getNewOutputDir();configureJob(jc, "job with cleanup()", 1, 0, outDir);jc.setOutputCommitter(committer);JobClient jobClient = new JobClient(jc);RunningJob job = jobClient.submitJob(jc);JobID id = job.getID();job.waitForCompletion();LOG.info("Job finished : " + job.isComplete());Path testFile = new Path(outDir, filename);assertTrue("Done file \"" + testFile + "\" missing for job " + id, fileSys.exists(testFile));// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for successful job " + id, fileSys.exists(file));}}
private void testFailedJob(String fileName, Class<? extends OutputCommitter> committer, String[] exclude) {JobConf jc = mr.createJobConf();Path outDir = getNewOutputDir();configureJob(jc, "fail job with abort()", 1, 0, outDir);jc.setMaxMapAttempts(1);// set the job to failjc.setMapperClass(UtilsForTests.FailMapper.class);jc.setOutputCommitter(committer);JobClient jobClient = new JobClient(jc);RunningJob job = jobClient.submitJob(jc);JobID id = job.getID();job.waitForCompletion();assertEquals("Job did not fail", JobStatus.FAILED, job.getJobState());if (fileName != null) {Path testFile = new Path(outDir, fileName);assertTrue("File " + testFile + " missing for failed job " + id, fileSys.exists(testFile));}// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for failed job " + id, fileSys.exists(file));}}
private void testKilledJob(String fileName, Class<? extends OutputCommitter> committer, String[] exclude) {JobConf jc = mr.createJobConf();Path outDir = getNewOutputDir();configureJob(jc, "kill job with abort()", 1, 0, outDir);// set the job to wait for longjc.setMapperClass(UtilsForTests.KillMapper.class);jc.setOutputCommitter(committer);JobClient jobClient = new JobClient(jc);RunningJob job = jobClient.submitJob(jc);JobID id = job.getID();Counters counters = job.getCounters();// wait for the map to be launchedwhile (true) {if (counters.getCounter(JobCounter.TOTAL_LAUNCHED_MAPS) == 1) {break;}LOG.info("Waiting for a map task to be launched");UtilsForTests.waitFor(100);counters = job.getCounters();}// kill the jobjob.killJob();// wait for the job to completejob.waitForCompletion();assertEquals("Job was not killed", JobStatus.KILLED, job.getJobState());if (fileName != null) {Path testFile = new Path(outDir, fileName);assertTrue("File " + testFile + " missing for job " + id, fileSys.exists(testFile));}// check if the files from the missing set existsfor (String ex : exclude) {Path file = new Path(outDir, ex);assertFalse("File " + file + " should not be present for killed job " + id, fileSys.exists(file));}}
public void testDefaultCleanupAndAbort() {// check with a successful jobtestSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, FileOutputCommitter.class, new String[] {});// check with a failed jobtestFailedJob(null, FileOutputCommitter.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });// check default abort job killtestKilledJob(null, FileOutputCommitter.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });}
public void testCustomAbort() {// check with a successful jobtestSuccessfulJob(FileOutputCommitter.SUCCEEDED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { ABORT_FAILED_FILE_NAME, ABORT_KILLED_FILE_NAME });// check with a failed jobtestFailedJob(ABORT_FAILED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_KILLED_FILE_NAME });// check with a killed jobtestKilledJob(ABORT_KILLED_FILE_NAME, CommitterWithCustomAbort.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME, ABORT_FAILED_FILE_NAME });}
public void testCustomCleanup() {// check with a successful jobtestSuccessfulJob(CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] {});// check with a failed jobtestFailedJob(CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });// check with a killed jobtestKilledJob(TestJobCleanup.CUSTOM_CLEANUP_FILE_NAME, CommitterWithCustomDeprecatedCleanup.class, new String[] { FileOutputCommitter.SUCCEEDED_FILE_NAME });}static void registerCommands(CommandFactory factory) {}
public void testSetup() {factory = new CommandFactory(conf);assertNotNull(factory);}
public void testRegistration() {assertArrayEquals(new String[] {}, factory.getNames());factory.registerCommands(TestRegistrar.class);String[] names = factory.getNames();assertArrayEquals(new String[] { "tc1", "tc2", "tc2.1" }, names);factory.addClass(TestCommand3.class, "tc3");names = factory.getNames();assertArrayEquals(new String[] { "tc1", "tc2", "tc2.1", "tc3" }, names);factory.addClass(TestCommand4.class, (new TestCommand4()).getName());names = factory.getNames();assertArrayEquals(new String[] { "tc1", "tc2", "tc2.1", "tc3", "tc4" }, names);}
public void testGetInstances() {factory.registerCommands(TestRegistrar.class);Command instance;instance = factory.getInstance("blarg");assertNull(instance);instance = factory.getInstance("tc1");assertNotNull(instance);assertEquals(TestCommand1.class, instance.getClass());assertEquals("tc1", instance.getCommandName());instance = factory.getInstance("tc2");assertNotNull(instance);assertEquals(TestCommand2.class, instance.getClass());assertEquals("tc2", instance.getCommandName());instance = factory.getInstance("tc2.1");assertNotNull(instance);assertEquals(TestCommand2.class, instance.getClass());assertEquals("tc2.1", instance.getCommandName());factory.addClass(TestCommand4.class, "tc4");instance = factory.getInstance("tc4");assertNotNull(instance);assertEquals(TestCommand4.class, instance.getClass());assertEquals("tc4", instance.getCommandName());String usage = instance.getUsage();assertEquals("-tc4 tc4_usage", usage);assertEquals("tc4_description", instance.getDescription());}
public static void registerCommands(CommandFactory factory) {factory.addClass(TestCommand1.class, "tc1");factory.addClass(TestCommand2.class, "tc2", "tc2.1");}protected void serviceInit(Configuration conf) {registerHeartbeatHandler(conf);commitWindowMs = conf.getLong(MRJobConfig.MR_AM_COMMIT_WINDOW_MS, MRJobConfig.DEFAULT_MR_AM_COMMIT_WINDOW_MS);super.serviceInit(conf);}
protected void serviceStart() {startRpcServer();super.serviceStart();}
protected void registerHeartbeatHandler(Configuration conf) {taskHeartbeatHandler = new TaskHeartbeatHandler(context.getEventHandler(), context.getClock(), conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT));addService(taskHeartbeatHandler);}
protected void startRpcServer() {Configuration conf = getConfig();try {server = new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class).setInstance(this).setBindAddress("0.0.0.0").setPort(0).setNumHandlers(conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT)).setVerbose(false).setSecretManager(jobTokenSecretManager).build();// Enable service authorization?if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, false)) {refreshServiceAcls(conf, new MRAMPolicyProvider());}server.start();this.address = NetUtils.createSocketAddrForHost(context.getNMHostname(), server.getListenerAddress().getPort());} catch (IOException e) {throw new YarnRuntimeException(e);}}
void refreshServiceAcls(Configuration configuration, PolicyProvider policyProvider) {this.server.refreshServiceAcl(configuration, policyProvider);}
protected void serviceStop() {stopRpcServer();super.serviceStop();}
protected void stopRpcServer() {if (server != null) {server.stop();}}
public InetSocketAddress getAddress() {return address;}
public boolean canCommit(TaskAttemptID taskAttemptID) {LOG.info("Commit go/no-go request from " + taskAttemptID.toString());// request there.org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);taskHeartbeatHandler.progressing(attemptID);// window to help avoid double-committing in a split-brain situationlong now = context.getClock().getTime();if (now - rmHeartbeatHandler.getLastHeartbeatTime() > commitWindowMs) {return false;}Job job = context.getJob(attemptID.getTaskId().getJobId());Task task = job.getTask(attemptID.getTaskId());return task.canCommit(attemptID);}
public void commitPending(TaskAttemptID taskAttemptID, TaskStatus taskStatsu) {LOG.info("Commit-pending state update from " + taskAttemptID.toString());// request there.org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);taskHeartbeatHandler.progressing(attemptID);//Ignorable TaskStatus? - since a task will send a LastStatusUpdatecontext.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_COMMIT_PENDING));}
public void preempted(TaskAttemptID taskAttemptID, TaskStatus taskStatus) {LOG.info("Preempted state update from " + taskAttemptID.toString());// An attempt is telling us that it got preempted.org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);preemptionPolicy.reportSuccessfulPreemption(attemptID);taskHeartbeatHandler.progressing(attemptID);context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_PREEMPTED));}
public void done(TaskAttemptID taskAttemptID) {LOG.info("Done acknowledgement from " + taskAttemptID.toString());org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);taskHeartbeatHandler.progressing(attemptID);context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));}
public void fatalError(TaskAttemptID taskAttemptID, String msg) {// This happens only in Child and in the Task.LOG.fatal("Task: " + taskAttemptID + " - exited : " + msg);reportDiagnosticInfo(taskAttemptID, "Error: " + msg);org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);// handling checkpointspreemptionPolicy.handleFailedContainer(attemptID);context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));}
public void fsError(TaskAttemptID taskAttemptID, String message) {// This happens only in Child.LOG.fatal("Task: " + taskAttemptID + " - failed due to FSError: " + message);reportDiagnosticInfo(taskAttemptID, "FSError: " + message);org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);// handling checkpointspreemptionPolicy.handleFailedContainer(attemptID);context.getEventHandler().handle(new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));}
public void shuffleError(TaskAttemptID taskAttemptID, String message) {}
public MapTaskCompletionEventsUpdate getMapCompletionEvents(JobID jobIdentifier, int startIndex, int maxEvents, TaskAttemptID taskAttemptID) {LOG.info("MapCompletionEvents request from " + taskAttemptID.toString() + ". startIndex " + startIndex + " maxEvents " + maxEvents);boolean shouldReset = false;org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);TaskCompletionEvent[] events = context.getJob(attemptID.getTaskId().getJobId()).getMapAttemptCompletionEvents(startIndex, maxEvents);taskHeartbeatHandler.progressing(attemptID);return new MapTaskCompletionEventsUpdate(events, shouldReset);}
public void reportDiagnosticInfo(TaskAttemptID taskAttemptID, String diagnosticInfo) {diagnosticInfo = StringInterner.weakIntern(diagnosticInfo);LOG.info("Diagnostics report from " + taskAttemptID.toString() + ": " + diagnosticInfo);org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = TypeConverter.toYarn(taskAttemptID);taskHeartbeatHandler.progressing(attemptID);// call but not diagnosticInformation.context.getEventHandler().handle(new TaskAttemptDiagnosticsUpdateEvent(attemptID, diagnosticInfo));}
public AMFeedback statusUpdate(TaskAttemptID taskAttemptID, TaskStatus taskStatus) {org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID = TypeConverter.toYarn(taskAttemptID);AMFeedback feedback = new AMFeedback();feedback.setTaskFound(true);// Propagating preemption to the task if TASK_PREEMPTION is enabledif (getConfig().getBoolean(MRJobConfig.TASK_PREEMPTION, false) && preemptionPolicy.isPreempted(yarnAttemptID)) {feedback.setPreemption(true);LOG.info("Setting preemption bit for task: " + yarnAttemptID + " of type " + yarnAttemptID.getTaskId().getTaskType());}if (taskStatus == null) {//We are using statusUpdate only as a simple pingif (LOG.isDebugEnabled()) {LOG.debug("Ping from " + taskAttemptID.toString());}return feedback;}taskHeartbeatHandler.progressing(yarnAttemptID);TaskAttemptStatus taskAttemptStatus = new TaskAttemptStatus();taskAttemptStatus.id = yarnAttemptID;// Task sends the updated progress to the TT.taskAttemptStatus.progress = taskStatus.getProgress();LOG.info("Progress of TaskAttempt " + taskAttemptID + " is : " + taskStatus.getProgress());// Task sends the updated state-string to the TT.taskAttemptStatus.stateString = taskStatus.getStateString();// Task sends the updated phase to the TT.taskAttemptStatus.phase = TypeConverter.toYarn(taskStatus.getPhase());// conversions and unnecessary heap usage.taskAttemptStatus.counters = new org.apache.hadoop.mapreduce.Counters(taskStatus.getCounters());// Map Finish time set by the task (map only)if (taskStatus.getIsMap() && taskStatus.getMapFinishTime() != 0) {taskAttemptStatus.mapFinishTime = taskStatus.getMapFinishTime();}// Shuffle Finish time set by the task (reduce only).if (!taskStatus.getIsMap() && taskStatus.getShuffleFinishTime() != 0) {taskAttemptStatus.shuffleFinishTime = taskStatus.getShuffleFinishTime();}// Sort finish time set by the task (reduce only).if (!taskStatus.getIsMap() && taskStatus.getSortFinishTime() != 0) {taskAttemptStatus.sortFinishTime = taskStatus.getSortFinishTime();}//set the fetch failuresif (taskStatus.getFetchFailedMaps() != null && taskStatus.getFetchFailedMaps().size() > 0) {taskAttemptStatus.fetchFailedMaps = new ArrayList<org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId>();for (TaskAttemptID failedMapId : taskStatus.getFetchFailedMaps()) {taskAttemptStatus.fetchFailedMaps.add(TypeConverter.toYarn(failedMapId));}}context.getEventHandler().handle(new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id, taskAttemptStatus));return feedback;}
public long getProtocolVersion(String arg0, long arg1) {return TaskUmbilicalProtocol.versionID;}
public void reportNextRecordRange(TaskAttemptID taskAttemptID, Range range) {// call but not the next record range information.throw new IOException("Not yet implemented.");}
public JvmTask getTask(JvmContext context) {JVMId jvmId = context.jvmId;LOG.info("JVM with ID : " + jvmId + " asked for a task");JvmTask jvmTask = null;WrappedJvmID wJvmID = new WrappedJvmID(jvmId.getJobId(), jvmId.isMap, jvmId.getId());// multiple tasks to a JVMif (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {LOG.info("JVM with ID: " + jvmId + " is invalid and will be killed.");jvmTask = TASK_FOR_INVALID_JVM;} else {if (!launchedJVMs.contains(wJvmID)) {jvmTask = null;LOG.info("JVM with ID: " + jvmId + " asking for task before AM launch registered. Given null task");} else {// longer pending, and further request should ask it to exit.org.apache.hadoop.mapred.Task task = jvmIDToActiveAttemptMap.remove(wJvmID);launchedJVMs.remove(wJvmID);LOG.info("JVM with ID: " + jvmId + " given task: " + task.getTaskID());jvmTask = new JvmTask(task, false);}}return jvmTask;}
public void registerPendingTask(org.apache.hadoop.mapred.Task task, WrappedJvmID jvmID) {// A JVM not present in this map is an illegal task/JVM.jvmIDToActiveAttemptMap.put(jvmID, task);}
public void registerLaunchedTask(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID, WrappedJvmID jvmId) {// The JVM will only be given a task after this registartion.launchedJVMs.add(jvmId);taskHeartbeatHandler.register(attemptID);}
public void unregister(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID, WrappedJvmID jvmID) {// remove the mappings if not already removedlaunchedJVMs.remove(jvmID);jvmIDToActiveAttemptMap.remove(jvmID);//unregister this attempttaskHeartbeatHandler.unregister(attemptID);}
public ProtocolSignature getProtocolSignature(String protocol, long clientVersion, int clientMethodsHash) {return ProtocolSignature.getProtocolSignature(this, protocol, clientVersion, clientMethodsHash);}
public TaskCheckpointID getCheckpointID(TaskID taskId) {TaskId tid = TypeConverter.toYarn(taskId);return preemptionPolicy.getCheckpointID(tid);}
public void setCheckpointID(TaskID taskId, TaskCheckpointID cid) {TaskId tid = TypeConverter.toYarn(taskId);preemptionPolicy.setCheckpointID(tid, cid);}public static boolean isAvailable() {return NativeCodeLoader.isNativeCodeLoaded();}
public static void verifyChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data, String fileName, long basePos) {nativeComputeChunkedSums(bytesPerSum, checksumType, sums, sums.position(), data, data.position(), data.remaining(), fileName, basePos, true);}
public static void verifyChunkedSumsByteArray(int bytesPerSum, int checksumType, byte[] sums, int sumsOffset, byte[] data, int dataOffset, int dataLength, String fileName, long basePos) {nativeComputeChunkedSumsByteArray(bytesPerSum, checksumType, sums, sumsOffset, data, dataOffset, dataLength, fileName, basePos, true);}
public static void calculateChunkedSums(int bytesPerSum, int checksumType, ByteBuffer sums, ByteBuffer data) {nativeComputeChunkedSums(bytesPerSum, checksumType, sums, sums.position(), data, data.position(), data.remaining(), "", 0, false);}
public static void calculateChunkedSumsByteArray(int bytesPerSum, int checksumType, byte[] sums, int sumsOffset, byte[] data, int dataOffset, int dataLength) {nativeComputeChunkedSumsByteArray(bytesPerSum, checksumType, sums, sumsOffset, data, dataOffset, dataLength, "", 0, false);}public void setVerbose(boolean v) {this.verbose = v;}
public void merge(List srcNames, List srcUnjar, String dstJar) {String source = null;JarOutputStream jarOut = null;JarFile jarSource = null;jarOut = new JarOutputStream(new FileOutputStream(dstJar));boolean throwing = false;try {if (srcNames != null) {Iterator iter = srcNames.iterator();while (iter.hasNext()) {source = (String) iter.next();File fsource = new File(source);String base = getBasePathInJarOut(source);if (!fsource.exists()) {throwing = true;throw new FileNotFoundException(fsource.getAbsolutePath());}if (fsource.isDirectory()) {addDirectory(jarOut, base, fsource, 0);} else {addFileStream(jarOut, base, fsource);}}}if (srcUnjar != null) {Iterator iter = srcUnjar.iterator();while (iter.hasNext()) {source = (String) iter.next();jarSource = new JarFile(source);addJarEntries(jarOut, jarSource);jarSource.close();}}} finally {try {jarOut.close();} catch (ZipException z) {if (!throwing) {throw new IOException(z.toString());}}}}
protected String fileExtension(String file) {int leafPos = file.lastIndexOf('/');if (leafPos == file.length() - 1)return "";String leafName = file.substring(leafPos + 1);int dotPos = leafName.lastIndexOf('.');if (dotPos == -1)return "";String ext = leafName.substring(dotPos + 1);return ext;}
protected String getBasePathInJarOut(String sourceFile) {// TaskRunner will unjar and append to classpath: .:classes/:lib/*String ext = fileExtension(sourceFile);if (ext.equals("class")) {// or ""return "classes/";} else if (ext.equals("jar") || ext.equals("zip")) {return "lib/";} else {return "";}}
private void addJarEntries(JarOutputStream dst, JarFile src) {Enumeration entries = src.entries();JarEntry entry = null;while (entries.hasMoreElements()) {entry = (JarEntry) entries.nextElement();//if (entry.getName().startsWith("META-INF/")) continue; InputStream in = src.getInputStream(entry);addNamedStream(dst, entry.getName(), in);}}
void addNamedStream(JarOutputStream dst, String name, InputStream in) {if (verbose) {System.err.println("JarBuilder.addNamedStream " + name);}try {dst.putNextEntry(new JarEntry(name));int bytesRead = 0;while ((bytesRead = in.read(buffer, 0, BUFF_SIZE)) != -1) {dst.write(buffer, 0, bytesRead);}} catch (ZipException ze) {if (ze.getMessage().indexOf("duplicate entry") >= 0) {if (verbose) {System.err.println(ze + " Skip duplicate entry " + name);}} else {throw ze;}} finally {in.close();dst.flush();dst.closeEntry();}}
void addFileStream(JarOutputStream dst, String jarBaseName, File file) {FileInputStream in = new FileInputStream(file);try {String name = jarBaseName + file.getName();addNamedStream(dst, name, in);} finally {in.close();}}
void addDirectory(JarOutputStream dst, String jarBaseName, File dir, int depth) {File[] contents = dir.listFiles();if (contents != null) {for (int i = 0; i < contents.length; i++) {File f = contents[i];String fBaseName = (depth == 0) ? "" : dir.getName();if (jarBaseName.length() > 0) {fBaseName = jarBaseName + "/" + fBaseName;}if (f.isDirectory()) {addDirectory(dst, fBaseName, f, depth + 1);} else {addFileStream(dst, fBaseName + "/", f);}}}}
public static void main(String args[]) {// args = new String[] { "C:/Temp/merged.jar", "C:/jdk1.5.0/jre/lib/ext/dnsns.jar",  "/Temp/addtojar2.log", "C:/jdk1.5.0/jre/lib/ext/mtest.jar", "C:/Temp/base"};if (args.length < 2) {System.err.println("Usage: JarFiles merged.jar [src.jar | dir | file ]+");} else {JarBuilder jarFiles = new JarBuilder();List names = new ArrayList();List unjar = new ArrayList();for (int i = 1; i < args.length; i++) {String f = args[i];String ext = jarFiles.fileExtension(f);boolean expandAsJar = ext.equals("jar") || ext.equals("zip");if (expandAsJar) {unjar.add(f);} else {names.add(f);}}try {jarFiles.merge(names, unjar, args[0]);Date lastMod = new Date(new File(args[0]).lastModified());System.out.println("Merge done to " + args[0] + " " + lastMod);} catch (Exception ge) {ge.printStackTrace(System.err);}}}public FsDatasetImpl newInstance(DataNode datanode, DataStorage storage, Configuration conf) {return new FsDatasetImpl(datanode, storage, conf);}protected void serviceStart() {Configuration conf = getConfig();YarnRPC rpc = YarnRPC.create(conf);InetSocketAddress address = conf.getSocketAddr(YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, YarnConfiguration.TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_PORT);server = rpc.getServer(ApplicationHistoryProtocol.class, protocolHandler, address, conf, null, conf.getInt(YarnConfiguration.TIMELINE_SERVICE_HANDLER_THREAD_COUNT, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_CLIENT_THREAD_COUNT));server.start();this.bindAddress = conf.updateConnectAddr(YarnConfiguration.TIMELINE_SERVICE_BIND_HOST, YarnConfiguration.TIMELINE_SERVICE_ADDRESS, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ADDRESS, server.getListenerAddress());LOG.info("Instantiated ApplicationHistoryClientService at " + this.bindAddress);super.serviceStart();}
protected void serviceStop() {if (server != null) {server.stop();}super.serviceStop();}
public ApplicationHistoryProtocol getClientHandler() {return this.protocolHandler;}
public InetSocketAddress getBindAddress() {return this.bindAddress;}
public CancelDelegationTokenResponse cancelDelegationToken(CancelDelegationTokenRequest request) {// TODO Auto-generated method stubreturn null;}
public GetApplicationAttemptReportResponse getApplicationAttemptReport(GetApplicationAttemptReportRequest request) {try {GetApplicationAttemptReportResponse response = GetApplicationAttemptReportResponse.newInstance(history.getApplicationAttempt(request.getApplicationAttemptId()));return response;} catch (IOException e) {throw new ApplicationAttemptNotFoundException(e.getMessage());}}
public GetApplicationAttemptsResponse getApplicationAttempts(GetApplicationAttemptsRequest request) {GetApplicationAttemptsResponse response = GetApplicationAttemptsResponse.newInstance(new ArrayList<ApplicationAttemptReport>(history.getApplicationAttempts(request.getApplicationId()).values()));return response;}
public GetApplicationReportResponse getApplicationReport(GetApplicationReportRequest request) {try {ApplicationId applicationId = request.getApplicationId();GetApplicationReportResponse response = GetApplicationReportResponse.newInstance(history.getApplication(applicationId));return response;} catch (IOException e) {throw new ApplicationNotFoundException(e.getMessage());}}
public GetApplicationsResponse getApplications(GetApplicationsRequest request) {GetApplicationsResponse response = GetApplicationsResponse.newInstance(new ArrayList<ApplicationReport>(history.getAllApplications().values()));return response;}
public GetContainerReportResponse getContainerReport(GetContainerReportRequest request) {try {GetContainerReportResponse response = GetContainerReportResponse.newInstance(history.getContainer(request.getContainerId()));return response;} catch (IOException e) {throw new ContainerNotFoundException(e.getMessage());}}
public GetContainersResponse getContainers(GetContainersRequest request) {GetContainersResponse response = GetContainersResponse.newInstance(new ArrayList<ContainerReport>(history.getContainers(request.getApplicationAttemptId()).values()));return response;}
public GetDelegationTokenResponse getDelegationToken(GetDelegationTokenRequest request) {// TODO Auto-generated method stubreturn null;}
public RenewDelegationTokenResponse renewDelegationToken(RenewDelegationTokenRequest request) {// TODO Auto-generated method stubreturn null;}public void setup() {dataNodes = new DatanodeDescriptor[] { DFSTestUtil.getDatanodeDescriptor("1.1.1.1", "/d1/r1"), DFSTestUtil.getDatanodeDescriptor("2.2.2.2", "/d1/r1"), DFSTestUtil.getDatanodeDescriptor("3.3.3.3", "/d1/r2"), DFSTestUtil.getDatanodeDescriptor("3.3.3.3", 5021, "/d1/r2") };for (DatanodeDescriptor node : dataNodes) {map.add(node);}map.add(null);}
public void testContains() {DatanodeDescriptor nodeNotInMap = DFSTestUtil.getDatanodeDescriptor("3.3.3.3", "/d1/r4");for (int i = 0; i < dataNodes.length; i++) {assertTrue(map.contains(dataNodes[i]));}assertFalse(map.contains(null));assertFalse(map.contains(nodeNotInMap));}
public void testGetDatanodeByHost() {assertEquals(map.getDatanodeByHost("1.1.1.1"), dataNodes[0]);assertEquals(map.getDatanodeByHost("2.2.2.2"), dataNodes[1]);DatanodeDescriptor node = map.getDatanodeByHost("3.3.3.3");assertTrue(node == dataNodes[2] || node == dataNodes[3]);assertNull(map.getDatanodeByHost("4.4.4.4"));}
public void testRemove() {DatanodeDescriptor nodeNotInMap = DFSTestUtil.getDatanodeDescriptor("3.3.3.3", "/d1/r4");assertFalse(map.remove(nodeNotInMap));assertTrue(map.remove(dataNodes[0]));assertTrue(map.getDatanodeByHost("1.1.1.1.") == null);assertTrue(map.getDatanodeByHost("2.2.2.2") == dataNodes[1]);DatanodeDescriptor node = map.getDatanodeByHost("3.3.3.3");assertTrue(node == dataNodes[2] || node == dataNodes[3]);assertNull(map.getDatanodeByHost("4.4.4.4"));assertTrue(map.remove(dataNodes[2]));assertNull(map.getDatanodeByHost("1.1.1.1"));assertEquals(map.getDatanodeByHost("2.2.2.2"), dataNodes[1]);assertEquals(map.getDatanodeByHost("3.3.3.3"), dataNodes[3]);assertTrue(map.remove(dataNodes[3]));assertNull(map.getDatanodeByHost("1.1.1.1"));assertEquals(map.getDatanodeByHost("2.2.2.2"), dataNodes[1]);assertNull(map.getDatanodeByHost("3.3.3.3"));assertFalse(map.remove(null));assertTrue(map.remove(dataNodes[1]));assertFalse(map.remove(dataNodes[1]));}public void setUp() {fc = FileContext.getLocalFSFileContext();super.setUp();}public void render(Page.HTML<_> html) {html.body().div()._(Sub1.class)._().div().i("inline text")._(Sub2.class)._()._()._();}
public void render(Block html) {html.div("#sub1")._("sub1 text")._();}
public void render(Block html) {html.pre()._("sub2 text")._();}
public void testSubView() {Injector injector = WebAppTests.createMockInjector(this);injector.getInstance(MainView.class).render();PrintWriter out = injector.getInstance(HttpServletResponse.class).getWriter();out.flush();verify(out).print("sub1 text");verify(out).print("sub2 text");// test inline transition across viewsverify(out, times(16)).println();}public int compareTo(BinaryComparable other) {if (this == other)return 0;return WritableComparator.compareBytes(getBytes(), 0, getLength(), other.getBytes(), 0, other.getLength());}
public int compareTo(byte[] other, int off, int len) {return WritableComparator.compareBytes(getBytes(), 0, getLength(), other, off, len);}
public boolean equals(Object other) {if (!(other instanceof BinaryComparable))return false;BinaryComparable that = (BinaryComparable) other;if (this.getLength() != that.getLength())return false;return this.compareTo(that) == 0;}
public int hashCode() {return WritableComparator.hashBytes(getBytes(), getLength());}public void close() {RPC.stopProxy(rpcProxy);}
public void journal(JournalInfo journalInfo, long epoch, long firstTxnId, int numTxns, byte[] records) {JournalRequestProto req = JournalRequestProto.newBuilder().setJournalInfo(PBHelper.convert(journalInfo)).setEpoch(epoch).setFirstTxnId(firstTxnId).setNumTxns(numTxns).setRecords(PBHelper.getByteString(records)).build();try {rpcProxy.journal(NULL_CONTROLLER, req);} catch (ServiceException e) {throw ProtobufHelper.getRemoteException(e);}}
public void startLogSegment(JournalInfo journalInfo, long epoch, long txid) {StartLogSegmentRequestProto req = StartLogSegmentRequestProto.newBuilder().setJournalInfo(PBHelper.convert(journalInfo)).setEpoch(epoch).setTxid(txid).build();try {rpcProxy.startLogSegment(NULL_CONTROLLER, req);} catch (ServiceException e) {throw ProtobufHelper.getRemoteException(e);}}
public FenceResponse fence(JournalInfo journalInfo, long epoch, String fencerInfo) {FenceRequestProto req = FenceRequestProto.newBuilder().setEpoch(epoch).setJournalInfo(PBHelper.convert(journalInfo)).build();try {FenceResponseProto resp = rpcProxy.fence(NULL_CONTROLLER, req);return new FenceResponse(resp.getPreviousEpoch(), resp.getLastTransactionId(), resp.getInSync());} catch (ServiceException e) {throw ProtobufHelper.getRemoteException(e);}}
public boolean isMethodSupported(String methodName) {return RpcClientUtil.isMethodSupported(rpcProxy, JournalProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(JournalProtocolPB.class), methodName);}public synchronized void setDisplayName(String displayName) {this.displayName = displayName;}
public synchronized void readFields(DataInput in) {name = StringInterner.weakIntern(Text.readString(in));displayName = in.readBoolean() ? StringInterner.weakIntern(Text.readString(in)) : name;value = WritableUtils.readVLong(in);}
public synchronized void write(DataOutput out) {Text.writeString(out, name);boolean distinctDisplayName = !name.equals(displayName);out.writeBoolean(distinctDisplayName);if (distinctDisplayName) {Text.writeString(out, displayName);}WritableUtils.writeVLong(out, value);}
public synchronized String getName() {return name;}
public synchronized String getDisplayName() {return displayName;}
public synchronized long getValue() {return value;}
public synchronized void setValue(long value) {this.value = value;}
public synchronized void increment(long incr) {value += incr;}
public Counter getUnderlyingCounter() {return this;}public void testCompInvalidate() {final Configuration conf = new HdfsConfiguration();final int NUM_OF_DATANODES = 3;final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();try {cluster.waitActive();final FSNamesystem namesystem = cluster.getNamesystem();final BlockManager bm = namesystem.getBlockManager();final int blockInvalidateLimit = bm.getDatanodeManager().blockInvalidateLimit;final DatanodeDescriptor[] nodes = bm.getDatanodeManager().getHeartbeatManager().getDatanodes();assertEquals(nodes.length, NUM_OF_DATANODES);namesystem.writeLock();try {for (int i = 0; i < nodes.length; i++) {for (int j = 0; j < 3 * blockInvalidateLimit + 1; j++) {Block block = new Block(i * (blockInvalidateLimit + 1) + j, 0, GenerationStamp.LAST_RESERVED_STAMP);bm.addToInvalidates(block, nodes[i]);}}assertEquals(blockInvalidateLimit * NUM_OF_DATANODES, bm.computeInvalidateWork(NUM_OF_DATANODES + 1));assertEquals(blockInvalidateLimit * NUM_OF_DATANODES, bm.computeInvalidateWork(NUM_OF_DATANODES));assertEquals(blockInvalidateLimit * (NUM_OF_DATANODES - 1), bm.computeInvalidateWork(NUM_OF_DATANODES - 1));int workCount = bm.computeInvalidateWork(1);if (workCount == 1) {assertEquals(blockInvalidateLimit + 1, bm.computeInvalidateWork(2));} else {assertEquals(workCount, blockInvalidateLimit);assertEquals(2, bm.computeInvalidateWork(2));}} finally {namesystem.writeUnlock();}} finally {cluster.shutdown();}}public static int compareVersions(String version1, String version2) {ComparableVersion v1 = new ComparableVersion(version1);ComparableVersion v2 = new ComparableVersion(version2);return v1.compareTo(v2);}public ApplicationStateDataProto getProto() {mergeLocalToProto();proto = viaProto ? proto : builder.build();viaProto = true;return proto;}
private void mergeLocalToBuilder() {if (this.applicationSubmissionContext != null) {builder.setApplicationSubmissionContext(((ApplicationSubmissionContextPBImpl) applicationSubmissionContext).getProto());}}
private void mergeLocalToProto() {if (viaProto)maybeInitBuilder();mergeLocalToBuilder();proto = builder.build();viaProto = true;}
private void maybeInitBuilder() {if (viaProto || builder == null) {builder = ApplicationStateDataProto.newBuilder(proto);}viaProto = false;}
public long getSubmitTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasSubmitTime()) {return -1;}return (p.getSubmitTime());}
public void setSubmitTime(long submitTime) {maybeInitBuilder();builder.setSubmitTime(submitTime);}
public long getStartTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;return p.getStartTime();}
public void setStartTime(long startTime) {maybeInitBuilder();builder.setStartTime(startTime);}
public String getUser() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasUser()) {return null;}return (p.getUser());}
public void setUser(String user) {maybeInitBuilder();builder.setUser(user);}
public ApplicationSubmissionContext getApplicationSubmissionContext() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (applicationSubmissionContext != null) {return applicationSubmissionContext;}if (!p.hasApplicationSubmissionContext()) {return null;}applicationSubmissionContext = new ApplicationSubmissionContextPBImpl(p.getApplicationSubmissionContext());return applicationSubmissionContext;}
public void setApplicationSubmissionContext(ApplicationSubmissionContext context) {maybeInitBuilder();if (context == null) {builder.clearApplicationSubmissionContext();}this.applicationSubmissionContext = context;}
public RMAppState getState() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasApplicationState()) {return null;}return convertFromProtoFormat(p.getApplicationState());}
public void setState(RMAppState finalState) {maybeInitBuilder();if (finalState == null) {builder.clearApplicationState();return;}builder.setApplicationState(convertToProtoFormat(finalState));}
public String getDiagnostics() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;if (!p.hasDiagnostics()) {return null;}return p.getDiagnostics();}
public void setDiagnostics(String diagnostics) {maybeInitBuilder();if (diagnostics == null) {builder.clearDiagnostics();return;}builder.setDiagnostics(diagnostics);}
public long getFinishTime() {ApplicationStateDataProtoOrBuilder p = viaProto ? proto : builder;return p.getFinishTime();}
public void setFinishTime(long finishTime) {maybeInitBuilder();builder.setFinishTime(finishTime);}
public int hashCode() {return getProto().hashCode();}
public boolean equals(Object other) {if (other == null)return false;if (other.getClass().isAssignableFrom(this.getClass())) {return this.getProto().equals(this.getClass().cast(other).getProto());}return false;}
public String toString() {return TextFormat.shortDebugString(getProto());}
public static RMAppStateProto convertToProtoFormat(RMAppState e) {return RMAppStateProto.valueOf(RM_APP_PREFIX + e.name());}
public static RMAppState convertFromProtoFormat(RMAppStateProto e) {return RMAppState.valueOf(e.name().replace(RM_APP_PREFIX, ""));}Boolean hasAccess(Job job, HttpServletRequest request) {String remoteUser = request.getRemoteUser();UserGroupInformation callerUGI = null;if (remoteUser != null) {callerUGI = UserGroupInformation.createRemoteUser(remoteUser);}if (callerUGI != null && !job.checkAccess(callerUGI, JobACL.VIEW_JOB)) {return false;}return true;}
private void init() {//clear content typeresponse.setContentType(null);}
public static Job getJobFromJobIdString(String jid, AppContext appCtx) {JobId jobId;Job job;try {jobId = MRApps.toJobID(jid);} catch (YarnRuntimeException e) {throw new NotFoundException(e.getMessage());} catch (IllegalArgumentException e) {throw new NotFoundException(e.getMessage());}if (jobId == null) {throw new NotFoundException("job, " + jid + ", is not found");}job = appCtx.getJob(jobId);if (job == null) {throw new NotFoundException("job, " + jid + ", is not found");}return job;}
public static Task getTaskFromTaskIdString(String tid, Job job) {TaskId taskID;Task task;try {taskID = MRApps.toTaskID(tid);} catch (YarnRuntimeException e) {throw new NotFoundException(e.getMessage());} catch (NumberFormatException ne) {throw new NotFoundException(ne.getMessage());} catch (IllegalArgumentException e) {throw new NotFoundException(e.getMessage());}if (taskID == null) {throw new NotFoundException("taskid " + tid + " not found or invalid");}task = job.getTask(taskID);if (task == null) {throw new NotFoundException("task not found with id " + tid);}return task;}
public static TaskAttempt getTaskAttemptFromTaskAttemptString(String attId, Task task) {TaskAttemptId attemptId;TaskAttempt ta;try {attemptId = MRApps.toTaskAttemptID(attId);} catch (YarnRuntimeException e) {throw new NotFoundException(e.getMessage());} catch (NumberFormatException ne) {throw new NotFoundException(ne.getMessage());} catch (IllegalArgumentException e) {throw new NotFoundException(e.getMessage());}if (attemptId == null) {throw new NotFoundException("task attempt id " + attId + " not found or invalid");}ta = task.getAttempt(attemptId);if (ta == null) {throw new NotFoundException("Error getting info on task attempt id " + attId);}return ta;}
void checkAccess(Job job, HttpServletRequest request) {if (!hasAccess(job, request)) {throw new WebApplicationException(Status.UNAUTHORIZED);}}
public AppInfo get() {return getAppInfo();}
public AppInfo getAppInfo() {init();return new AppInfo(this.app, this.app.context);}
public BlacklistedNodesInfo getBlacklistedNodes() {init();return new BlacklistedNodesInfo(this.app.context);}
public JobsInfo getJobs(@Context HttpServletRequest hsr) {init();JobsInfo allJobs = new JobsInfo();for (Job job : appCtx.getAllJobs().values()) {// getAllJobs only gives you a partial we want a fullJob fullJob = appCtx.getJob(job.getID());if (fullJob == null) {continue;}allJobs.add(new JobInfo(fullJob, hasAccess(fullJob, hsr)));}return allJobs;}
public JobInfo getJob(@Context HttpServletRequest hsr, @PathParam("jobid") String jid) {init();Job job = getJobFromJobIdString(jid, appCtx);return new JobInfo(job, hasAccess(job, hsr));}
public AMAttemptsInfo getJobAttempts(@PathParam("jobid") String jid) {init();Job job = getJobFromJobIdString(jid, appCtx);AMAttemptsInfo amAttempts = new AMAttemptsInfo();for (AMInfo amInfo : job.getAMInfos()) {AMAttemptInfo attempt = new AMAttemptInfo(amInfo, MRApps.toString(job.getID()), job.getUserName());amAttempts.add(attempt);}return amAttempts;}
public JobCounterInfo getJobCounters(@Context HttpServletRequest hsr, @PathParam("jobid") String jid) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);return new JobCounterInfo(this.appCtx, job);}
public ConfInfo getJobConf(@Context HttpServletRequest hsr, @PathParam("jobid") String jid) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);ConfInfo info;try {info = new ConfInfo(job);} catch (IOException e) {throw new NotFoundException("unable to load configuration for job: " + jid);}return info;}
public TasksInfo getJobTasks(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @QueryParam("type") String type) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);TasksInfo allTasks = new TasksInfo();for (Task task : job.getTasks().values()) {TaskType ttype = null;if (type != null && !type.isEmpty()) {try {ttype = MRApps.taskType(type);} catch (YarnRuntimeException e) {throw new BadRequestException("tasktype must be either m or r");}}if (ttype != null && task.getType() != ttype) {continue;}allTasks.add(new TaskInfo(task));}return allTasks;}
public TaskInfo getJobTask(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @PathParam("taskid") String tid) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);Task task = getTaskFromTaskIdString(tid, job);return new TaskInfo(task);}
public JobTaskCounterInfo getSingleTaskCounters(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @PathParam("taskid") String tid) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);Task task = getTaskFromTaskIdString(tid, job);return new JobTaskCounterInfo(task);}
public TaskAttemptsInfo getJobTaskAttempts(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @PathParam("taskid") String tid) {init();TaskAttemptsInfo attempts = new TaskAttemptsInfo();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);Task task = getTaskFromTaskIdString(tid, job);for (TaskAttempt ta : task.getAttempts().values()) {if (ta != null) {if (task.getType() == TaskType.REDUCE) {attempts.add(new ReduceTaskAttemptInfo(ta, task.getType()));} else {attempts.add(new TaskAttemptInfo(ta, task.getType(), true));}}}return attempts;}
public TaskAttemptInfo getJobTaskAttemptId(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @PathParam("taskid") String tid, @PathParam("attemptid") String attId) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);Task task = getTaskFromTaskIdString(tid, job);TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);if (task.getType() == TaskType.REDUCE) {return new ReduceTaskAttemptInfo(ta, task.getType());} else {return new TaskAttemptInfo(ta, task.getType(), true);}}
public JobTaskAttemptCounterInfo getJobTaskAttemptIdCounters(@Context HttpServletRequest hsr, @PathParam("jobid") String jid, @PathParam("taskid") String tid, @PathParam("attemptid") String attId) {init();Job job = getJobFromJobIdString(jid, appCtx);checkAccess(job, hsr);Task task = getTaskFromTaskIdString(tid, job);TaskAttempt ta = getTaskAttemptFromTaskAttemptString(attId, task);return new JobTaskAttemptCounterInfo(ta);}public static void createCluster() {HDFSContract.createCluster();}
public static void teardownCluster() {HDFSContract.destroyCluster();}
protected AbstractFSContract createContract(Configuration conf) {return new HDFSContract(conf);}public static void setUp() {conf = new Configuration();cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();cluster.waitActive();fsn = cluster.getNamesystem();fs = cluster.getFileSystem();Path path1 = new Path(sub1, "dir1");assertTrue(fs.mkdirs(path1));Path path2 = new Path(sub2, "dir2");assertTrue(fs.mkdirs(path2));SnapshotTestHelper.createSnapshot(fs, sub1, "testSnapshot");objInSnapshot = SnapshotTestHelper.getSnapshotPath(sub1, "testSnapshot", "dir1");}
public static void tearDown() {if (cluster != null) {cluster.shutdown();}}
public void testSetReplication() {fs.setReplication(objInSnapshot, (short) 1);}
public void testSetPermission() {fs.setPermission(objInSnapshot, new FsPermission("777"));}
public void testSetOwner() {fs.setOwner(objInSnapshot, "username", "groupname");}
public void testRename() {try {fs.rename(objInSnapshot, new Path("/invalid/path"));fail("Didn't throw SnapshotAccessControlException");} catch (SnapshotAccessControlException e) {}try {fs.rename(sub2, objInSnapshot);fail("Didn't throw SnapshotAccessControlException");} catch (SnapshotAccessControlException e) {}try {fs.rename(sub2, objInSnapshot, (Options.Rename) null);fail("Didn't throw SnapshotAccessControlException");} catch (SnapshotAccessControlException e) {}}
public void testDelete() {fs.delete(objInSnapshot, true);}
public void testQuota() {fs.setQuota(objInSnapshot, 100, 100);}
public void testSetTime() {fs.setTimes(objInSnapshot, 100, 100);}
public void testCreate() {@SuppressWarnings("deprecation") DFSClient dfsclient = new DFSClient(conf);dfsclient.create(objInSnapshot.toString(), true);}
public void testAppend() {fs.append(objInSnapshot, 65535, null);}
public void testMkdir() {fs.mkdirs(objInSnapshot, new FsPermission("777"));}
public void testCreateSymlink() {@SuppressWarnings("deprecation") DFSClient dfsclient = new DFSClient(conf);dfsclient.createSymlink(sub2.toString(), "/TestSnapshot/sub1/.snapshot", false);}public void testMissingBlocksAlert() {MiniDFSCluster cluster = null;try {Configuration conf = new HdfsConfiguration();//minimize test delayconf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 0);conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);int fileLen = 10 * 1024;conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, fileLen / 2);//start a cluster with single datanodecluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();final BlockManager bm = cluster.getNamesystem().getBlockManager();DistributedFileSystem dfs = cluster.getFileSystem();// create a normal fileDFSTestUtil.createFile(dfs, new Path("/testMissingBlocksAlert/file1"), fileLen, (short) 3, 0);Path corruptFile = new Path("/testMissingBlocks/corruptFile");DFSTestUtil.createFile(dfs, corruptFile, fileLen, (short) 3, 0);// Corrupt the blockExtendedBlock block = DFSTestUtil.getFirstBlock(dfs, corruptFile);assertTrue(TestDatanodeBlockScanner.corruptReplica(block, 0));// read the file so that the corrupt block is reported to NNFSDataInputStream in = dfs.open(corruptFile);try {in.readFully(new byte[fileLen]);} catch (// checksum error is expected.  ChecksumException // checksum error is expected.  ignored) {}in.close();LOG.info("Waiting for missing blocks count to increase...");while (dfs.getMissingBlocksCount() <= 0) {Thread.sleep(100);}assertTrue(dfs.getMissingBlocksCount() == 1);assertEquals(4, dfs.getUnderReplicatedBlocksCount());assertEquals(3, bm.getUnderReplicatedNotMissingBlocks());MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();ObjectName mxbeanName = new ObjectName("Hadoop:service=NameNode,name=NameNodeInfo");Assert.assertEquals(1, (long) (Long) mbs.getAttribute(mxbeanName, "NumberOfMissingBlocks"));dfs.delete(corruptFile, true);LOG.info("Waiting for missing blocks count to be zero...");while (dfs.getMissingBlocksCount() > 0) {Thread.sleep(100);}assertEquals(2, dfs.getUnderReplicatedBlocksCount());assertEquals(2, bm.getUnderReplicatedNotMissingBlocks());Assert.assertEquals(0, (long) (Long) mbs.getAttribute(mxbeanName, "NumberOfMissingBlocks"));} finally {if (cluster != null) {cluster.shutdown();}}}public void startUpCluster() {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(REPL_FACTOR).build();fs = cluster.getFileSystem();}
public void shutDownCluster() {if (fs != null)fs.close();if (cluster != null) {cluster.shutdownDataNodes();cluster.shutdown();}}
public void pipeline_01() {final String METHOD_NAME = GenericTestUtils.getMethodName();if (LOG.isDebugEnabled()) {LOG.debug("Running " + METHOD_NAME);}Path filePath = new Path("/" + METHOD_NAME + ".dat");DFSTestUtil.createFile(fs, filePath, FILE_SIZE, REPL_FACTOR, rand.nextLong());if (LOG.isDebugEnabled()) {LOG.debug("Invoking append but doing nothing otherwise...");}FSDataOutputStream ofs = fs.append(filePath);ofs.writeBytes("Some more stuff to write");((DFSOutputStream) ofs.getWrappedStream()).hflush();List<LocatedBlock> lb = cluster.getNameNodeRpc().getBlockLocations(filePath.toString(), FILE_SIZE - 1, FILE_SIZE).getLocatedBlocks();String bpid = cluster.getNamesystem().getBlockPoolId();for (DataNode dn : cluster.getDataNodes()) {Replica r = DataNodeTestUtils.fetchReplicaInfo(dn, bpid, lb.get(0).getBlock().getBlockId());assertTrue("Replica on DN " + dn + " shouldn't be null", r != null);assertEquals("Should be RBW replica on " + dn + " after sequence of calls append()/write()/hflush()", HdfsServerConstants.ReplicaState.RBW, r.getState());}ofs.close();}
public void pipeline_02_03() {}
static byte[] writeData(final FSDataOutputStream out, final int length) {int bytesToWrite = length;byte[] ret = new byte[bytesToWrite];byte[] toWrite = new byte[1024];int written = 0;Random rb = new Random(rand.nextLong());while (bytesToWrite > 0) {rb.nextBytes(toWrite);int bytesToWriteNext = (1024 < bytesToWrite) ? 1024 : bytesToWrite;out.write(toWrite, 0, bytesToWriteNext);System.arraycopy(toWrite, 0, ret, (ret.length - bytesToWrite), bytesToWriteNext);written += bytesToWriteNext;if (LOG.isDebugEnabled()) {LOG.debug("Written: " + bytesToWriteNext + "; Total: " + written);}bytesToWrite -= bytesToWriteNext;}return ret;}
private static void setConfiguration() {conf = new Configuration();int customPerChecksumSize = 700;int customBlockSize = customPerChecksumSize * 3;conf.setInt(DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY, 100);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, customPerChecksumSize);conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, customBlockSize);conf.setInt(DFSConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY, customBlockSize / 2);conf.setInt(DFSConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY, 0);}
private static void initLoggers() {((Log4JLogger) NameNode.stateChangeLog).getLogger().setLevel(Level.ALL);((Log4JLogger) LogFactory.getLog(FSNamesystem.class)).getLogger().setLevel(Level.ALL);((Log4JLogger) DataNode.LOG).getLogger().setLevel(Level.ALL);((Log4JLogger) DFSClient.LOG).getLogger().setLevel(Level.ALL);}public static void setupCluster() {if (DomainSocket.getLoadingFailureReason() != null)return;DFSInputStream.tcpReadsDisabledForTesting = true;sockDir = new TemporarySocketDirectory();HdfsConfiguration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY, new File(sockDir.getDir(), "TestParallelLocalRead.%d.sock").getAbsolutePath());conf.setBoolean(DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_KEY, true);conf.setBoolean(DFSConfigKeys.DFS_CLIENT_READ_SHORTCIRCUIT_SKIP_CHECKSUM_KEY, true);DomainSocket.disableBindPathValidation();setupCluster(1, conf);}
public void before() {Assume.assumeThat(DomainSocket.getLoadingFailureReason(), equalTo(null));}
public static void teardownCluster() {if (DomainSocket.getLoadingFailureReason() != null)return;sockDir.close();TestParallelReadUtil.teardownCluster();}public static long getLongGaugeValue(AzureFileSystemInstrumentation instrumentation, String gaugeName) {return getLongGauge(gaugeName, getMetrics(instrumentation));}
public static long getLongCounterValue(AzureFileSystemInstrumentation instrumentation, String counterName) {return getLongCounter(counterName, getMetrics(instrumentation));}
public static long getCurrentBytesWritten(AzureFileSystemInstrumentation instrumentation) {return getLongGaugeValue(instrumentation, WASB_BYTES_WRITTEN);}
public static long getCurrentBytesRead(AzureFileSystemInstrumentation instrumentation) {return getLongGaugeValue(instrumentation, WASB_BYTES_READ);}
public static long getCurrentTotalBytesWritten(AzureFileSystemInstrumentation instrumentation) {return getLongCounterValue(instrumentation, WASB_RAW_BYTES_UPLOADED);}
public static long getCurrentTotalBytesRead(AzureFileSystemInstrumentation instrumentation) {return getLongCounterValue(instrumentation, WASB_RAW_BYTES_DOWNLOADED);}
public static long getCurrentWebResponses(AzureFileSystemInstrumentation instrumentation) {return getLongCounter(WASB_WEB_RESPONSES, getMetrics(instrumentation));}public static void start() {InetSocketAddress address = new InetSocketAddress(0);Configuration configuration = new Configuration();ResourceTracker instance = new ResourceTrackerTestImpl();server = RpcServerFactoryPBImpl.get().getServer(ResourceTracker.class, instance, address, configuration, null, 1);server.start();client = (ResourceTracker) RpcClientFactoryPBImpl.get().getClient(ResourceTracker.class, 1, NetUtils.getConnectAddress(server), configuration);}
public static void stop() {if (server != null) {server.stop();}}
public void testResourceTrackerPBClientImpl() {RegisterNodeManagerRequest request = recordFactory.newRecordInstance(RegisterNodeManagerRequest.class);assertNotNull(client.registerNodeManager(request));ResourceTrackerTestImpl.exception = true;try {client.registerNodeManager(request);fail("there  should be YarnException");} catch (YarnException e) {assertTrue(e.getMessage().startsWith("testMessage"));} finally {ResourceTrackerTestImpl.exception = false;}}
public void testNodeHeartbeat() {NodeHeartbeatRequest request = recordFactory.newRecordInstance(NodeHeartbeatRequest.class);assertNotNull(client.nodeHeartbeat(request));ResourceTrackerTestImpl.exception = true;try {client.nodeHeartbeat(request);fail("there  should be YarnException");} catch (YarnException e) {assertTrue(e.getMessage().startsWith("testMessage"));} finally {ResourceTrackerTestImpl.exception = false;}}
public RegisterNodeManagerResponse registerNodeManager(RegisterNodeManagerRequest request) {if (exception) {throw new YarnException("testMessage");}return recordFactory.newRecordInstance(RegisterNodeManagerResponse.class);}
public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) {if (exception) {throw new YarnException("testMessage");}return recordFactory.newRecordInstance(NodeHeartbeatResponse.class);}public static LanguageVersion languageVersion() {return LanguageVersion.JAVA_1_5;}
public static boolean start(RootDoc root) {System.out.println(ExcludePrivateAnnotationsStandardDoclet.class.getSimpleName());return Standard.start(RootDocProcessor.process(root));}
public static int optionLength(String option) {Integer length = StabilityOptions.optionLength(option);if (length != null) {return length;}return Standard.optionLength(option);}
public static boolean validOptions(String[][] options, DocErrorReporter reporter) {StabilityOptions.validOptions(options, reporter);String[][] filteredOptions = StabilityOptions.filterOptions(options);return Standard.validOptions(filteredOptions, reporter);}public StorageInfo getStorageInfo() {return storageInfo;}
public void setExportedKeys(ExportedBlockKeys keys) {this.exportedKeys = keys;}
public ExportedBlockKeys getExportedKeys() {return exportedKeys;}
public String getSoftwareVersion() {return softwareVersion;}
public int getVersion() {return storageInfo.getLayoutVersion();}
public String getRegistrationID() {return Storage.getRegistrationID(storageInfo);}
public String getAddress() {return getXferAddr();}
public String toString() {return getClass().getSimpleName() + "(" + getIpAddr() + ", datanodeUuid=" + getDatanodeUuid() + ", infoPort=" + getInfoPort() + ", ipcPort=" + getIpcPort() + ", storageInfo=" + storageInfo + ")";}
public boolean equals(Object to) {return super.equals(to);}
public int hashCode() {return super.hashCode();}public FileStatus getFileStatus() {run();return stat;}
public static boolean isAvailable() {if (Shell.LINUX || Shell.FREEBSD || Shell.MAC) {return true;}return false;}
FileStatus getFileStatusForTesting() {return stat;}
protected String[] getExecString() {String derefFlag = "-";if (dereference) {derefFlag = "-L";}if (Shell.LINUX) {return new String[] { "stat", derefFlag + "c", "%s,%F,%Y,%X,%a,%U,%G,%N", path.toString() };} else if (Shell.FREEBSD || Shell.MAC) {return new String[] { "stat", derefFlag + "f", "%z,%HT,%m,%a,%Op,%Su,%Sg,`link' -> `%Y'", path.toString() };} else {throw new UnsupportedOperationException("stat is not supported on this platform");}}
protected void parseExecResult(BufferedReader lines) {// Reset statstat = null;String line = lines.readLine();if (line == null) {throw new IOException("Unable to stat path: " + original);}if (line.endsWith("No such file or directory") || line.endsWith("Not a directory")) {throw new FileNotFoundException("File " + original + " does not exist");}if (line.endsWith("Too many levels of symbolic links")) {throw new IOException("Possible cyclic loop while following symbolic" + " link " + original);}// 6,symbolic link,6,1373584236,1373584236,lrwxrwxrwx,andrew,andrew,'link' -> 'target'StringTokenizer tokens = new StringTokenizer(line, ",");try {long length = Long.parseLong(tokens.nextToken());boolean isDir = tokens.nextToken().equalsIgnoreCase("directory") ? true : false;// Convert from seconds to millisecondslong modTime = Long.parseLong(tokens.nextToken()) * 1000;long accessTime = Long.parseLong(tokens.nextToken()) * 1000;String octalPerms = tokens.nextToken();// FreeBSD has extra digits beyond 4, truncate themif (octalPerms.length() > 4) {int len = octalPerms.length();octalPerms = octalPerms.substring(len - 4, len);}FsPermission perms = new FsPermission(Short.parseShort(octalPerms, 8));String owner = tokens.nextToken();String group = tokens.nextToken();String symStr = tokens.nextToken();// '' -> ''Path symlink = null;String parts[] = symStr.split(" -> ");try {String target = parts[1];target = target.substring(1, target.length() - 1);if (!target.isEmpty()) {symlink = new Path(target);}} catch (ArrayIndexOutOfBoundsException e) {}// Set statstat = new FileStatus(length, isDir, 1, blockSize, modTime, accessTime, perms, owner, group, symlink, qualified);} catch (NumberFormatException e) {throw new IOException("Unexpected stat output: " + line, e);} catch (NoSuchElementException e) {throw new IOException("Unexpected stat output: " + line, e);}}protected WebHdfsFileSystem createFileSystem() {return WebHdfsTestUtil.getWebHdfsFileSystem(conf, WebHdfsFileSystem.SCHEME);}public void incHedgedReadOps() {hedgedReadOps.incrementAndGet();}
public void incHedgedReadOpsInCurThread() {hedgedReadOpsInCurThread.incrementAndGet();}
public void incHedgedReadWins() {hedgedReadOpsWin.incrementAndGet();}
public long getHedgedReadOps() {return hedgedReadOps.longValue();}
public long getHedgedReadOpsInCurThread() {return hedgedReadOpsInCurThread.longValue();}
public long getHedgedReadWins() {return hedgedReadOpsWin.longValue();}private void writeOutput(RecordWriter theRecordWriter, Reporter reporter) {NullWritable nullWritable = NullWritable.get();try {theRecordWriter.write(key1, val1);theRecordWriter.write(null, nullWritable);theRecordWriter.write(null, val1);theRecordWriter.write(nullWritable, val2);theRecordWriter.write(key2, nullWritable);theRecordWriter.write(key1, null);theRecordWriter.write(null, null);theRecordWriter.write(key2, val2);} finally {theRecordWriter.close(reporter);}}
private void setConfForFileOutputCommitter(JobConf job) {job.set(JobContext.TASK_ATTEMPT_ID, attempt);job.setOutputCommitter(FileOutputCommitter.class);FileOutputFormat.setOutputPath(job, outDir);}
public void testCommitter() {JobConf job = new JobConf();setConfForFileOutputCommitter(job);JobContext jContext = new JobContextImpl(job, taskID.getJobID());TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);FileOutputCommitter committer = new FileOutputCommitter();FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));committer.setupJob(jContext);committer.setupTask(tContext);String file = "test.txt";// A reporter that does nothingReporter reporter = Reporter.NULL;// write outputFileSystem localFs = FileSystem.getLocal(job);TextOutputFormat theOutputFormat = new TextOutputFormat();RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);writeOutput(theRecordWriter, reporter);// do commitcommitter.commitTask(tContext);committer.commitJob(jContext);// validate outputFile expectedFile = new File(new Path(outDir, file).toString());StringBuffer expectedOutput = new StringBuffer();expectedOutput.append(key1).append('\t').append(val1).append("\n");expectedOutput.append(val1).append("\n");expectedOutput.append(val2).append("\n");expectedOutput.append(key2).append("\n");expectedOutput.append(key1).append("\n");expectedOutput.append(key2).append('\t').append(val2).append("\n");String output = UtilsForTests.slurp(expectedFile);assertEquals(output, expectedOutput.toString());FileUtil.fullyDelete(new File(outDir.toString()));}
public void testAbort() {JobConf job = new JobConf();setConfForFileOutputCommitter(job);JobContext jContext = new JobContextImpl(job, taskID.getJobID());TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);FileOutputCommitter committer = new FileOutputCommitter();FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));// do setupcommitter.setupJob(jContext);committer.setupTask(tContext);String file = "test.txt";// A reporter that does nothingReporter reporter = Reporter.NULL;// write outputFileSystem localFs = FileSystem.getLocal(job);TextOutputFormat theOutputFormat = new TextOutputFormat();RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, file, reporter);writeOutput(theRecordWriter, reporter);// do abortcommitter.abortTask(tContext);File expectedFile = new File(new Path(committer.getTaskAttemptPath(tContext), file).toString());assertFalse("task temp dir still exists", expectedFile.exists());committer.abortJob(jContext, JobStatus.State.FAILED);expectedFile = new File(new Path(outDir, FileOutputCommitter.TEMP_DIR_NAME).toString());assertFalse("job temp dir " + expectedFile + " still exists", expectedFile.exists());assertEquals("Output directory not empty", 0, new File(outDir.toString()).listFiles().length);FileUtil.fullyDelete(new File(outDir.toString()));}
public URI getUri() {return URI.create("faildel:///");}
public boolean delete(Path p, boolean recursive) {throw new IOException("fake delete failed");}
public void testFailAbort() {JobConf job = new JobConf();job.set(FileSystem.FS_DEFAULT_NAME_KEY, "faildel:///");job.setClass("fs.faildel.impl", FakeFileSystem.class, FileSystem.class);setConfForFileOutputCommitter(job);JobContext jContext = new JobContextImpl(job, taskID.getJobID());TaskAttemptContext tContext = new TaskAttemptContextImpl(job, taskID);FileOutputCommitter committer = new FileOutputCommitter();FileOutputFormat.setWorkOutputPath(job, committer.getTaskAttemptPath(tContext));// do setupcommitter.setupJob(jContext);committer.setupTask(tContext);String file = "test.txt";File jobTmpDir = new File(committer.getJobAttemptPath(jContext).toUri().getPath());File taskTmpDir = new File(committer.getTaskAttemptPath(tContext).toUri().getPath());File expectedFile = new File(taskTmpDir, file);// A reporter that does nothingReporter reporter = Reporter.NULL;// write outputFileSystem localFs = new FakeFileSystem();TextOutputFormat theOutputFormat = new TextOutputFormat();RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(localFs, job, expectedFile.getAbsolutePath(), reporter);writeOutput(theRecordWriter, reporter);// do abortThrowable th = null;try {committer.abortTask(tContext);} catch (IOException ie) {th = ie;}assertNotNull(th);assertTrue(th instanceof IOException);assertTrue(th.getMessage().contains("fake delete failed"));assertTrue(expectedFile + " does not exists", expectedFile.exists());th = null;try {committer.abortJob(jContext, JobStatus.State.FAILED);} catch (IOException ie) {th = ie;}assertNotNull(th);assertTrue(th instanceof IOException);assertTrue(th.getMessage().contains("fake delete failed"));assertTrue("job temp dir does not exists", jobTmpDir.exists());}public void testFixedVariableAndLocalWhiteList() {String[] fixedIps = { "10.119.103.112", "10.221.102.0/23" };TestFileBasedIPList.createFileWithEntries("fixedwhitelist.txt", fixedIps);String[] variableIps = { "10.222.0.0/16", "10.113.221.221" };TestFileBasedIPList.createFileWithEntries("variablewhitelist.txt", variableIps);Configuration conf = new Configuration();conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_FIXEDWHITELIST_FILE, "fixedwhitelist.txt");conf.setBoolean(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_ENABLE, true);conf.setLong(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_CACHE_SECS, 1);conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_FILE, "variablewhitelist.txt");WhitelistBasedResolver wqr = new WhitelistBasedResolver();wqr.setConf(conf);assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties(InetAddress.getByName("10.119.103.112")));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.119.103.113"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("10.221.103.121"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.221.104.0"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("10.222.103.121"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.223.104.0"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("10.113.221.221"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.113.221.222"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("127.0.0.1"));TestFileBasedIPList.removeFile("fixedwhitelist.txt");TestFileBasedIPList.removeFile("variablewhitelist.txt");}
public void testFixedAndLocalWhiteList() {String[] fixedIps = { "10.119.103.112", "10.221.102.0/23" };TestFileBasedIPList.createFileWithEntries("fixedwhitelist.txt", fixedIps);String[] variableIps = { "10.222.0.0/16", "10.113.221.221" };TestFileBasedIPList.createFileWithEntries("variablewhitelist.txt", variableIps);Configuration conf = new Configuration();conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_FIXEDWHITELIST_FILE, "fixedwhitelist.txt");conf.setBoolean(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_ENABLE, false);conf.setLong(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_CACHE_SECS, 100);conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_FILE, "variablewhitelist.txt");WhitelistBasedResolver wqr = new WhitelistBasedResolver();wqr.setConf(conf);assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties(InetAddress.getByName("10.119.103.112")));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.119.103.113"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("10.221.103.121"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.221.104.0"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.222.103.121"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.223.104.0"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.113.221.221"));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties("10.113.221.222"));assertEquals(wqr.getDefaultProperties(), wqr.getServerProperties("127.0.0.1"));;TestFileBasedIPList.removeFile("fixedwhitelist.txt");TestFileBasedIPList.removeFile("variablewhitelist.txt");}
public void testNullIPAddress() {String[] fixedIps = { "10.119.103.112", "10.221.102.0/23" };TestFileBasedIPList.createFileWithEntries("fixedwhitelist.txt", fixedIps);String[] variableIps = { "10.222.0.0/16", "10.113.221.221" };TestFileBasedIPList.createFileWithEntries("variablewhitelist.txt", variableIps);Configuration conf = new Configuration();conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_FIXEDWHITELIST_FILE, "fixedwhitelist.txt");conf.setBoolean(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_ENABLE, true);conf.setLong(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_CACHE_SECS, 100);conf.set(WhitelistBasedResolver.HADOOP_SECURITY_SASL_VARIABLEWHITELIST_FILE, "variablewhitelist.txt");WhitelistBasedResolver wqr = new WhitelistBasedResolver();wqr.setConf(conf);assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties((InetAddress) null));assertEquals(SASL_PRIVACY_PROPS, wqr.getServerProperties((String) null));TestFileBasedIPList.removeFile("fixedwhitelist.txt");TestFileBasedIPList.removeFile("variablewhitelist.txt");}public void testStartStopServer() {historyServer = new ApplicationHistoryServer();Configuration config = new YarnConfiguration();historyServer.init(config);assertEquals(STATE.INITED, historyServer.getServiceState());assertEquals(4, historyServer.getServices().size());ApplicationHistoryClientService historyService = historyServer.getClientService();assertNotNull(historyServer.getClientService());assertEquals(STATE.INITED, historyService.getServiceState());historyServer.start();assertEquals(STATE.STARTED, historyServer.getServiceState());assertEquals(STATE.STARTED, historyService.getServiceState());historyServer.stop();assertEquals(STATE.STOPPED, historyServer.getServiceState());}
public void testLaunch() {ExitUtil.disableSystemExit();try {historyServer = ApplicationHistoryServer.launchAppHistoryServer(new String[0]);} catch (ExitUtil.ExitException e) {assertEquals(0, e.status);ExitUtil.resetFirstExitException();fail();}}
public void testFilteOverrides() {String[] filterInitializers = { AuthenticationFilterInitializer.class.getName(), TimelineAuthenticationFilterInitializer.class.getName(), AuthenticationFilterInitializer.class.getName() + "," + TimelineAuthenticationFilterInitializer.class.getName(), AuthenticationFilterInitializer.class.getName() + ", " + TimelineAuthenticationFilterInitializer.class.getName() };for (String filterInitializer : filterInitializers) {historyServer = new ApplicationHistoryServer();Configuration config = new YarnConfiguration();config.set("hadoop.http.filter.initializers", filterInitializer);historyServer.init(config);historyServer.start();Configuration tmp = historyServer.getConfig();assertEquals(TimelineAuthenticationFilterInitializer.class.getName(), tmp.get("hadoop.http.filter.initializers"));historyServer.stop();AHSWebApp.resetInstance();}}
public void stop() {if (historyServer != null) {historyServer.stop();}AHSWebApp.resetInstance();}public ZooKeeper getNewZooKeeper() {return createClient(watcher, hostPort, ZK_TIMEOUT_MS);}
public synchronized void processWatchEvent(WatchedEvent event) {if (forExpire) {// a hack... couldn't find a way to trigger expired event.WatchedEvent expriredEvent = new WatchedEvent(Watcher.Event.EventType.None, Watcher.Event.KeeperState.Expired, null);super.processWatchEvent(expriredEvent);forExpire = false;syncBarrier.await();} else {super.processWatchEvent(event);}}
public void process(WatchedEvent event) {super.process(event);try {if (store != null) {store.processWatchEvent(event);}} catch (Throwable t) {LOG.error("Failed to process watcher event " + event + ": " + StringUtils.stringifyException(t));}}
public RMStateStore getRMStateStore(Configuration conf) {String workingZnode = "/Test";conf.set(YarnConfiguration.RM_ZK_ADDRESS, hostPort);conf.set(YarnConfiguration.ZK_RM_STATE_STORE_PARENT_PATH, workingZnode);watcher = new TestForwardingWatcher();this.store = new TestZKRMStateStore(conf, workingZnode);return this.store;}
public void testZKClientRetry() {TestZKClient zkClientTester = new TestZKClient();final String path = "/test";YarnConfiguration conf = new YarnConfiguration();conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);conf.setLong(YarnConfiguration.RM_ZK_RETRY_INTERVAL_MS, 100);final ZKRMStateStore store = (ZKRMStateStore) zkClientTester.getRMStateStore(conf);TestDispatcher dispatcher = new TestDispatcher();store.setRMDispatcher(dispatcher);final AtomicBoolean assertionFailedInThread = new AtomicBoolean(false);stopServer();Thread clientThread = new Thread() {
@Overridepublic void run() {try {store.getDataWithRetries(path, true);} catch (Exception e) {e.printStackTrace();assertionFailedInThread.set(true);}}};Thread.sleep(2000);startServer();clientThread.join();Assert.assertFalse(assertionFailedInThread.get());}
public void run() {try {store.getDataWithRetries(path, true);} catch (Exception e) {e.printStackTrace();assertionFailedInThread.set(true);}}
public void testZKClientDisconnectAndReconnect() {TestZKClient zkClientTester = new TestZKClient();String path = "/test";YarnConfiguration conf = new YarnConfiguration();conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);ZKRMStateStore store = (ZKRMStateStore) zkClientTester.getRMStateStore(conf);TestDispatcher dispatcher = new TestDispatcher();store.setRMDispatcher(dispatcher);store.createWithRetries(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);store.getDataWithRetries(path, true);store.setDataWithRetries(path, "newBytes".getBytes(), 0);stopServer();zkClientTester.watcher.waitForDisconnected(ZK_OP_WAIT_TIME);try {store.getDataWithRetries(path, true);fail("Expected ZKClient time out exception");} catch (Exception e) {assertTrue(e.getMessage().contains("Wait for ZKClient creation timed out"));}startServer();zkClientTester.watcher.waitForConnected(ZK_OP_WAIT_TIME);byte[] ret = null;try {ret = store.getDataWithRetries(path, true);} catch (Exception e) {String error = "ZKRMStateStore Session restore failed";LOG.error(error, e);fail(error);}assertEquals("newBytes", new String(ret));}
public void testZKSessionTimeout() {TestZKClient zkClientTester = new TestZKClient();String path = "/test";YarnConfiguration conf = new YarnConfiguration();conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);ZKRMStateStore store = (ZKRMStateStore) zkClientTester.getRMStateStore(conf);TestDispatcher dispatcher = new TestDispatcher();store.setRMDispatcher(dispatcher);// a hack to trigger expired eventzkClientTester.forExpire = true;// trigger watchstore.createWithRetries(path, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);store.getDataWithRetries(path, true);store.setDataWithRetries(path, "bytes".getBytes(), 0);zkClientTester.syncBarrier.await();try {byte[] ret = store.getDataWithRetries(path, false);assertEquals("bytes", new String(ret));} catch (Exception e) {String error = "New session creation failed";LOG.error(error, e);fail(error);}}
public void testSetZKAcl() {TestZKClient zkClientTester = new TestZKClient();YarnConfiguration conf = new YarnConfiguration();conf.set(YarnConfiguration.RM_ZK_ACL, "world:anyone:rwca");try {zkClientTester.store.zkClient.delete(zkClientTester.store.znodeWorkingPath, -1);fail("Shouldn't be able to delete path");} catch (Exception e) {}}
public void testInvalidZKAclConfiguration() {TestZKClient zkClientTester = new TestZKClient();YarnConfiguration conf = new YarnConfiguration();conf.set(YarnConfiguration.RM_ZK_ACL, "randomstring&*");try {zkClientTester.getRMStateStore(conf);fail("ZKRMStateStore created with bad ACL");} catch (ZKUtil.BadAclFormatException bafe) {} catch (Exception e) {String error = "Incorrect exception on BadAclFormat";LOG.error(error, e);fail(error);}}
public void testZKAuths() {TestZKClient zkClientTester = new TestZKClient();YarnConfiguration conf = new YarnConfiguration();conf.setInt(YarnConfiguration.RM_ZK_NUM_RETRIES, 1);conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);conf.set(YarnConfiguration.RM_ZK_ACL, TEST_ACL);conf.set(YarnConfiguration.RM_ZK_AUTH, TEST_AUTH_GOOD);zkClientTester.getRMStateStore(conf);}
public void testZKRetryInterval() {TestZKClient zkClientTester = new TestZKClient();YarnConfiguration conf = new YarnConfiguration();ZKRMStateStore store = (ZKRMStateStore) zkClientTester.getRMStateStore(conf);assertEquals(YarnConfiguration.DEFAULT_RM_ZK_RETRY_INTERVAL_MS, store.zkRetryInterval);store.stop();conf.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);store = (ZKRMStateStore) zkClientTester.getRMStateStore(conf);assertEquals(YarnConfiguration.DEFAULT_RM_ZK_TIMEOUT_MS / YarnConfiguration.DEFAULT_ZK_RM_NUM_RETRIES, store.zkRetryInterval);store.stop();}public static NMClientAsync createNMClientAsync(CallbackHandler callbackHandler) {return new NMClientAsyncImpl(callbackHandler);}
public NMClient getClient() {return client;}
public void setClient(NMClient client) {this.client = client;}
public CallbackHandler getCallbackHandler() {return callbackHandler;}
public void setCallbackHandler(CallbackHandler callbackHandler) {this.callbackHandler = callbackHandler;}static String[] getTrimmedStrings(String str) {if (null == str || "".equals(str.trim())) {return emptyStringArray;}return str.trim().split("\\s*,\\s*");}
static String toByteInfo(long bytes) {StringBuilder str = new StringBuilder();if (bytes < 0) {bytes = 0;}str.append(bytes);str.append(" bytes or ");str.append(bytes / 1024);str.append(" kilobytes or ");str.append(bytes / (1024 * 1024));str.append(" megabytes or ");str.append(bytes / (1024 * 1024 * 1024));str.append(" gigabytes");return str.toString();}
static String stringifyArray(Object[] args, String sep) {StringBuilder optStr = new StringBuilder();for (int i = 0; i < args.length; ++i) {optStr.append(args[i]);if ((i + 1) != args.length) {optStr.append(sep);}}return optStr.toString();}public boolean metricsAvailable() {return userMetricsAvailable;}
public int getAppsSubmitted() {return this.appsSubmitted;}
public int getAppsCompleted() {return appsCompleted;}
public int getAppsPending() {return appsPending;}
public int getAppsRunning() {return appsRunning;}
public int getAppsFailed() {return appsFailed;}
public int getAppsKilled() {return appsKilled;}
public long getReservedMB() {return this.reservedMB;}
public long getAllocatedMB() {return this.allocatedMB;}
public long getPendingMB() {return this.pendingMB;}
public long getReservedVirtualCores() {return this.reservedVirtualCores;}
public long getAllocatedVirtualCores() {return this.allocatedVirtualCores;}
public long getPendingVirtualCores() {return this.pendingVirtualCores;}
public int getReservedContainers() {return this.reservedContainers;}
public int getRunningContainers() {return this.runningContainers;}
public int getPendingContainers() {return this.pendingContainers;}public int run(String[] args) {int exitCode = 0;exitCode = init(args);if (exitCode != 0) {return exitCode;}genDirStructure();genFiles();return exitCode;}
private int init(String[] args) {try {// initialize file system handlefc = FileContext.getFileContext(getConf());} catch (IOException ioe) {System.err.println("Can not initialize the file system: " + ioe.getLocalizedMessage());return -1;}for (int i = 0; i < args.length; i++) {// parse command lineif (args[i].equals("-root")) {root = new Path(args[++i]);} else if (args[i].equals("-inDir")) {inDir = new File(args[++i]);} else {System.err.println(USAGE);ToolRunner.printGenericCommandUsage(System.err);System.exit(-1);}}return 0;}
private void genDirStructure() {BufferedReader in = new BufferedReader(new FileReader(new File(inDir, StructureGenerator.DIR_STRUCTURE_FILE_NAME)));String line;while ((line = in.readLine()) != null) {fc.mkdir(new Path(root + line), FileContext.DEFAULT_PERM, true);}}
private void genFiles() {BufferedReader in = new BufferedReader(new FileReader(new File(inDir, StructureGenerator.FILE_STRUCTURE_FILE_NAME)));String line;while ((line = in.readLine()) != null) {String[] tokens = line.split(" ");if (tokens.length != 2) {throw new IOException("Expect at most 2 tokens per line: " + line);}String fileName = root + tokens[0];long fileSize = (long) (BLOCK_SIZE * Double.parseDouble(tokens[1]));genFile(new Path(fileName), fileSize);}}
private void genFile(Path file, long fileSize) {FSDataOutputStream out = fc.create(file, EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE), CreateOpts.createParent(), CreateOpts.bufferSize(4096), CreateOpts.repFac((short) 3));for (long i = 0; i < fileSize; i++) {out.writeByte('a');}out.close();}
public static void main(String[] args) {int res = ToolRunner.run(new Configuration(), new DataGenerator(), args);System.exit(res);}public void testEviction() {NfsConfiguration conf = new NfsConfiguration();// Only two entries will be in the cacheconf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);DFSClient dfsClient = Mockito.mock(DFSClient.class);Nfs3FileAttributes attr = new Nfs3FileAttributes();HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);Mockito.when(fos.getPos()).thenReturn((long) 0);OpenFileCtx context1 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context2 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context3 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context4 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context5 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);boolean ret = cache.put(new FileHandle(1), context1);assertTrue(ret);Thread.sleep(1000);ret = cache.put(new FileHandle(2), context2);assertTrue(ret);ret = cache.put(new FileHandle(3), context3);assertFalse(ret);assertTrue(cache.size() == 2);// Wait for the oldest stream to be evict-able, insert againThread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);assertTrue(cache.size() == 2);ret = cache.put(new FileHandle(3), context3);assertTrue(ret);assertTrue(cache.size() == 2);assertTrue(cache.get(new FileHandle(1)) == null);// Test inactive entry is evicted immediatelycontext3.setActiveStatusForTest(false);ret = cache.put(new FileHandle(4), context4);assertTrue(ret);// Test eviction failure if all entries have pending work.context2.getPendingWritesForTest().put(new OffsetRange(0, 100), new WriteCtx(null, 0, 0, 0, null, null, null, 0, false, null));context4.getPendingCommitsForTest().put(new Long(100), new CommitCtx(0, null, 0, attr));Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);ret = cache.put(new FileHandle(5), context5);assertFalse(ret);}
public void testScan() {NfsConfiguration conf = new NfsConfiguration();// Only two entries will be in the cacheconf.setInt(NfsConfigKeys.DFS_NFS_MAX_OPEN_FILES_KEY, 2);DFSClient dfsClient = Mockito.mock(DFSClient.class);Nfs3FileAttributes attr = new Nfs3FileAttributes();HdfsDataOutputStream fos = Mockito.mock(HdfsDataOutputStream.class);Mockito.when(fos.getPos()).thenReturn((long) 0);OpenFileCtx context1 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context2 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context3 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtx context4 = new OpenFileCtx(fos, attr, "/dumpFilePath", dfsClient, new IdUserGroup(new NfsConfiguration()));OpenFileCtxCache cache = new OpenFileCtxCache(conf, 10 * 60 * 100);// Test cleaning expired entryboolean ret = cache.put(new FileHandle(1), context1);assertTrue(ret);ret = cache.put(new FileHandle(2), context2);assertTrue(ret);Thread.sleep(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT + 1);cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT);assertTrue(cache.size() == 0);// Test cleaning inactive entryret = cache.put(new FileHandle(3), context3);assertTrue(ret);ret = cache.put(new FileHandle(4), context4);assertTrue(ret);context3.setActiveStatusForTest(false);cache.scan(NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_DEFAULT);assertTrue(cache.size() == 1);assertTrue(cache.get(new FileHandle(3)) == null);assertTrue(cache.get(new FileHandle(4)) != null);}private void writeFile(FileSystem fileSys, Path name) {// create and write a file that contains three blocks of dataFSDataOutputStream stm = fileSys.create(name, new FsPermission((short) 0777), true, fileSys.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096), NUM_OF_DATANODES, BLOCK_SIZE, null);stm.write(expected);stm.close();}
private void checkAndEraseData(byte[] actual, int from, byte[] expected, String message) {for (int idx = 0; idx < actual.length; idx++) {assertEquals(message + " byte " + (from + idx) + " differs. expected " + expected[from + idx] + " actual " + actual[idx], actual[idx], expected[from + idx]);actual[idx] = 0;}}
private void checkReadAndGetPos() {actual = new byte[FILE_SIZE];// test reads that do not cross checksum boundarystm.seek(0);int offset;for (offset = 0; offset < BLOCK_SIZE + BYTES_PER_SUM; offset += BYTES_PER_SUM) {assertEquals(stm.getPos(), offset);stm.readFully(actual, offset, BYTES_PER_SUM);}stm.readFully(actual, offset, FILE_SIZE - BLOCK_SIZE - BYTES_PER_SUM);assertEquals(stm.getPos(), FILE_SIZE);checkAndEraseData(actual, 0, expected, "Read Sanity Test");// test reads that cross checksum boundarystm.seek(0L);assertEquals(stm.getPos(), 0L);stm.readFully(actual, 0, HALF_CHUNK_SIZE);assertEquals(stm.getPos(), HALF_CHUNK_SIZE);stm.readFully(actual, HALF_CHUNK_SIZE, BLOCK_SIZE - HALF_CHUNK_SIZE);assertEquals(stm.getPos(), BLOCK_SIZE);stm.readFully(actual, BLOCK_SIZE, BYTES_PER_SUM + HALF_CHUNK_SIZE);assertEquals(stm.getPos(), BLOCK_SIZE + BYTES_PER_SUM + HALF_CHUNK_SIZE);stm.readFully(actual, 2 * BLOCK_SIZE - HALF_CHUNK_SIZE, FILE_SIZE - (2 * BLOCK_SIZE - HALF_CHUNK_SIZE));assertEquals(stm.getPos(), FILE_SIZE);checkAndEraseData(actual, 0, expected, "Read Sanity Test");// test read that cross block boundarystm.seek(0L);stm.readFully(actual, 0, BYTES_PER_SUM + HALF_CHUNK_SIZE);assertEquals(stm.getPos(), BYTES_PER_SUM + HALF_CHUNK_SIZE);stm.readFully(actual, BYTES_PER_SUM + HALF_CHUNK_SIZE, BYTES_PER_SUM);assertEquals(stm.getPos(), BLOCK_SIZE + HALF_CHUNK_SIZE);stm.readFully(actual, BLOCK_SIZE + HALF_CHUNK_SIZE, FILE_SIZE - BLOCK_SIZE - HALF_CHUNK_SIZE);assertEquals(stm.getPos(), FILE_SIZE);checkAndEraseData(actual, 0, expected, "Read Sanity Test");}
private void testSeek1(int offset) {stm.seek(offset);assertEquals(offset, stm.getPos());stm.readFully(actual);checkAndEraseData(actual, offset, expected, "Read Sanity Test");}
private void checkSeek() {actual = new byte[HALF_CHUNK_SIZE];// test seeks to checksum boundarytestSeek1(0);testSeek1(BYTES_PER_SUM);testSeek1(BLOCK_SIZE);// test seek to non-checksum-boundary postestSeek1(BLOCK_SIZE + HALF_CHUNK_SIZE);testSeek1(HALF_CHUNK_SIZE);// test seek to a position at the same checksum chunktestSeek1(HALF_CHUNK_SIZE / 2);testSeek1(HALF_CHUNK_SIZE * 3 / 2);// test end of fileactual = new byte[1];testSeek1(FILE_SIZE - 1);String errMsg = null;try {stm.seek(FILE_SIZE);} catch (IOException e) {errMsg = e.getMessage();}assertTrue(errMsg == null);}
private void testSkip1(int skippedBytes) {long oldPos = stm.getPos();IOUtils.skipFully(stm, skippedBytes);long newPos = oldPos + skippedBytes;assertEquals(stm.getPos(), newPos);stm.readFully(actual);checkAndEraseData(actual, (int) newPos, expected, "Read Sanity Test");}
private void checkSkip() {actual = new byte[HALF_CHUNK_SIZE];// test skip to a checksum boundarystm.seek(0);testSkip1(BYTES_PER_SUM);testSkip1(HALF_CHUNK_SIZE);testSkip1(HALF_CHUNK_SIZE);// test skip to non-checksum-boundary posstm.seek(0);testSkip1(HALF_CHUNK_SIZE + 1);testSkip1(BYTES_PER_SUM);testSkip1(HALF_CHUNK_SIZE);// test skip to a position at the same checksum chunkstm.seek(0);testSkip1(1);testSkip1(1);// test skip to end of filestm.seek(0);actual = new byte[1];testSkip1(FILE_SIZE - 1);stm.seek(0);IOUtils.skipFully(stm, FILE_SIZE);try {IOUtils.skipFully(stm, 10);fail("expected to get a PrematureEOFException");} catch (EOFException e) {assertEquals(e.getMessage(), "Premature EOF from inputStream " + "after skipping 0 byte(s).");}stm.seek(0);try {IOUtils.skipFully(stm, FILE_SIZE + 10);fail("expected to get a PrematureEOFException");} catch (EOFException e) {assertEquals(e.getMessage(), "Premature EOF from inputStream " + "after skipping " + FILE_SIZE + " byte(s).");}stm.seek(10);try {IOUtils.skipFully(stm, FILE_SIZE);fail("expected to get a PrematureEOFException");} catch (EOFException e) {assertEquals(e.getMessage(), "Premature EOF from inputStream " + "after skipping " + (FILE_SIZE - 10) + " byte(s).");}}
private void cleanupFile(FileSystem fileSys, Path name) {assertTrue(fileSys.exists(name));fileSys.delete(name, true);assertTrue(!fileSys.exists(name));}
private void testChecker(FileSystem fileSys, boolean readCS) {Path file = new Path("try.dat");writeFile(fileSys, file);try {if (!readCS) {fileSys.setVerifyChecksum(false);}stm = fileSys.open(file);checkReadAndGetPos();checkSeek();checkSkip();//checkMarkassertFalse(stm.markSupported());stm.close();} finally {if (!readCS) {fileSys.setVerifyChecksum(true);}cleanupFile(fileSys, file);}}
private void testFileCorruption(LocalFileSystem fileSys) {String dir = PathUtils.getTestDirName(getClass());Path file = new Path(dir + "/corruption-test.dat");Path crcFile = new Path(dir + "/.corruption-test.dat.crc");writeFile(fileSys, file);int fileLen = (int) fileSys.getFileStatus(file).getLen();byte[] buf = new byte[fileLen];InputStream in = fileSys.open(file);IOUtils.readFully(in, buf, 0, buf.length);in.close();// check .crc corruptioncheckFileCorruption(fileSys, file, crcFile);fileSys.delete(file, true);writeFile(fileSys, file);// check data corrutpioncheckFileCorruption(fileSys, file, file);fileSys.delete(file, true);}
private void checkFileCorruption(LocalFileSystem fileSys, Path file, Path fileToCorrupt) {RandomAccessFile out = new RandomAccessFile(new File(fileToCorrupt.toString()), "rw");byte[] buf = new byte[(int) fileSys.getFileStatus(file).getLen()];int corruptFileLen = (int) fileSys.getFileStatus(fileToCorrupt).getLen();assertTrue(buf.length >= corruptFileLen);rand.nextBytes(buf);out.seek(corruptFileLen / 2);out.write(buf, 0, corruptFileLen / 4);out.close();boolean gotException = false;InputStream in = fileSys.open(file);try {IOUtils.readFully(in, buf, 0, buf.length);} catch (ChecksumException e) {gotException = true;}assertTrue(gotException);in.close();}
public void testFSInputChecker() {Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, BYTES_PER_SUM);rand.nextBytes(expected);// test DFSMiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();FileSystem fileSys = cluster.getFileSystem();try {testChecker(fileSys, true);testChecker(fileSys, false);testSeekAndRead(fileSys);} finally {fileSys.close();cluster.shutdown();}// test Local FSfileSys = FileSystem.getLocal(conf);try {testChecker(fileSys, true);testChecker(fileSys, false);testFileCorruption((LocalFileSystem) fileSys);testSeekAndRead(fileSys);} finally {fileSys.close();}}
private void testSeekAndRead(FileSystem fileSys) {Path file = new Path("try.dat");writeFile(fileSys, file);stm = fileSys.open(file, fileSys.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY, 4096));checkSeekAndRead();stm.close();cleanupFile(fileSys, file);}
private void checkSeekAndRead() {int position = 1;int len = 2 * BYTES_PER_SUM - position;readAndCompare(stm, position, len);position = BYTES_PER_SUM;len = BYTES_PER_SUM;readAndCompare(stm, position, len);}
private void readAndCompare(FSDataInputStream in, int position, int len) {byte[] b = new byte[len];in.seek(position);IOUtils.readFully(in, b, 0, b.length);for (int i = 0; i < b.length; i++) {assertEquals(expected[position + i], b[i]);}}public Quota.Counts getQuotaCounts() {return Quota.Counts.newInstance(-1, -1);}
public boolean metadataEquals(INodeDirectoryAttributes other) {return other != null && getQuotaCounts().equals(other.getQuotaCounts()) && getPermissionLong() == other.getPermissionLong() && getAclFeature() == other.getAclFeature() && getXAttrFeature() == other.getXAttrFeature();}
public Quota.Counts getQuotaCounts() {return Quota.Counts.newInstance(nsQuota, dsQuota);}public void init(String compression, String outputFile, int numRecords1stBlock, int numRecords2ndBlock) {this.compression = compression;this.outputFile = outputFile;this.records1stBlock = numRecords1stBlock;this.records2ndBlock = numRecords2ndBlock;}
public void setUp() {conf = new Configuration();path = new Path(ROOT, outputFile);fs = path.getFileSystem(conf);out = fs.create(path);writer = new Writer(out, BLOCK_SIZE, compression, null, conf);writer.append("keyZ".getBytes(), "valueZ".getBytes());writer.append("keyM".getBytes(), "valueM".getBytes());writer.append("keyN".getBytes(), "valueN".getBytes());writer.append("keyA".getBytes(), "valueA".getBytes());closeOutput();}
public void tearDown() {fs.delete(path, true);}
public void testFailureScannerWithKeys() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);try {Scanner scanner = reader.createScannerByKey("aaa".getBytes(), "zzz".getBytes());Assert.fail("Failed to catch creating scanner with keys on unsorted file.");} catch (RuntimeException e) {} finally {reader.close();}}
public void testScan() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);Scanner scanner = reader.createScanner();try {// read key and valuebyte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");scanner.advance();// now try get value firstvbuf = new byte[BUF_SIZE];vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");kbuf = new byte[BUF_SIZE];klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyM");} finally {scanner.close();reader.close();}}
public void testScanRange() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertFalse(reader.isSorted());Assert.assertEquals((int) reader.getEntryCount(), 4);Scanner scanner = reader.createScanner();try {// read key and valuebyte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");scanner.advance();// now try get value firstvbuf = new byte[BUF_SIZE];vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");kbuf = new byte[BUF_SIZE];klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), "keyM");} finally {scanner.close();reader.close();}}
public void testFailureSeek() {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {// can't find ceiltry {scanner.lowerBound("keyN".getBytes());Assert.fail("Cannot search in a unsorted TFile!");} catch (Exception e) {} finally {}// can't find highertry {scanner.upperBound("keyA".getBytes());Assert.fail("Cannot search higher in a unsorted TFile!");} catch (Exception e) {} finally {}// can't seektry {scanner.seekTo("keyM".getBytes());Assert.fail("Cannot search a unsorted TFile!");} catch (Exception e) {} finally {}} finally {scanner.close();reader.close();}}
private void closeOutput() {if (writer != null) {writer.close();writer = null;out.close();out = null;}}public void testProducesHistoryServerUriForAppId() {final String historyAddress = "example.net:424242";YarnConfiguration conf = new YarnConfiguration();conf.set(JHAdminConfig.MR_HISTORY_WEBAPP_ADDRESS, historyAddress);MapReduceTrackingUriPlugin plugin = new MapReduceTrackingUriPlugin();plugin.setConf(conf);ApplicationId id = ApplicationId.newInstance(6384623l, 5);String jobSuffix = id.toString().replaceFirst("^application_", "job_");URI expected = new URI("http://" + historyAddress + "/jobhistory/job/" + jobSuffix);URI actual = plugin.getTrackingUri(id);assertEquals(expected, actual);}public void init(String contextName, ContextFactory factory) {super.init(contextName, factory);int nKids;try {String sKids = getAttribute(ARITY_LABEL);nKids = Integer.valueOf(sKids);} catch (Exception e) {LOG.error("Unable to initialize composite metric " + contextName + ": could not init arity", e);return;}for (int i = 0; i < nKids; ++i) {MetricsContext ctxt = MetricsUtil.getContext(String.format(SUB_FMT, contextName, i), contextName);if (null != ctxt) {subctxt.add(ctxt);}}}
public MetricsRecord newRecord(String recordName) {return (MetricsRecord) Proxy.newProxyInstance(MetricsRecord.class.getClassLoader(), new Class[] { MetricsRecord.class }, new MetricsRecordDelegator(recordName, subctxt));}
protected void emitRecord(String contextName, String recordName, OutputRecord outRec) {for (MetricsContext ctxt : subctxt) {try {((AbstractMetricsContext) ctxt).emitRecord(contextName, recordName, outRec);if (contextName == null || recordName == null || outRec == null) {throw new IOException(contextName + ":" + recordName + ":" + outRec);}} catch (IOException e) {LOG.warn("emitRecord failed: " + ctxt.getContextName(), e);}}}
protected void flush() {for (MetricsContext ctxt : subctxt) {try {((AbstractMetricsContext) ctxt).flush();} catch (IOException e) {LOG.warn("flush failed: " + ctxt.getContextName(), e);}}}
public void startMonitoring() {for (MetricsContext ctxt : subctxt) {try {ctxt.startMonitoring();} catch (IOException e) {LOG.warn("startMonitoring failed: " + ctxt.getContextName(), e);}}}
public void stopMonitoring() {for (MetricsContext ctxt : subctxt) {ctxt.stopMonitoring();}}
public boolean isMonitoring() {boolean ret = true;for (MetricsContext ctxt : subctxt) {ret &= ctxt.isMonitoring();}return ret;}
public void close() {for (MetricsContext ctxt : subctxt) {ctxt.close();}}
public void registerUpdater(Updater updater) {for (MetricsContext ctxt : subctxt) {ctxt.registerUpdater(updater);}}
public void unregisterUpdater(Updater updater) {for (MetricsContext ctxt : subctxt) {ctxt.unregisterUpdater(updater);}}
private static Method initMethod() {try {return MetricsRecord.class.getMethod("getRecordName", new Class[0]);} catch (Exception e) {throw new RuntimeException("Internal error", e);}}
public Object invoke(Object p, Method m, Object[] args) {if (m_getRecordName.equals(m)) {return recordName;}assert Void.TYPE.equals(m.getReturnType());for (MetricsRecord rec : subrecs) {m.invoke(rec, args);}return null;}public String getName() {return NAME;}private static Path getOutputPath(JobContext context) {JobConf conf = context.getJobConf();return FileOutputFormat.getOutputPath(conf);}
private static Path getOutputPath(TaskAttemptContext context) {JobConf conf = context.getJobConf();return FileOutputFormat.getOutputPath(conf);}
private org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter getWrapped(JobContext context) {if (wrapped == null) {wrapped = new org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(getOutputPath(context), context);}return wrapped;}
private org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter getWrapped(TaskAttemptContext context) {if (wrapped == null) {wrapped = new org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(getOutputPath(context), context);}return wrapped;}
Path getJobAttemptPath(JobContext context) {Path out = getOutputPath(context);return out == null ? null : org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(context, out);}
public Path getTaskAttemptPath(TaskAttemptContext context) {Path out = getOutputPath(context);return out == null ? null : getTaskAttemptPath(context, out);}
private Path getTaskAttemptPath(TaskAttemptContext context, Path out) {Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());if (workPath == null && out != null) {return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(context, out);}return workPath;}
Path getCommittedTaskPath(TaskAttemptContext context) {Path out = getOutputPath(context);return out == null ? null : org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getCommittedTaskPath(context, out);}
public Path getWorkPath(TaskAttemptContext context, Path outputPath) {return outputPath == null ? null : getTaskAttemptPath(context, outputPath);}
public void setupJob(JobContext context) {getWrapped(context).setupJob(context);}
public void commitJob(JobContext context) {getWrapped(context).commitJob(context);}
public void cleanupJob(JobContext context) {getWrapped(context).cleanupJob(context);}
public void abortJob(JobContext context, int runState) {JobStatus.State state;if (runState == JobStatus.State.RUNNING.getValue()) {state = JobStatus.State.RUNNING;} else if (runState == JobStatus.State.SUCCEEDED.getValue()) {state = JobStatus.State.SUCCEEDED;} else if (runState == JobStatus.State.FAILED.getValue()) {state = JobStatus.State.FAILED;} else if (runState == JobStatus.State.PREP.getValue()) {state = JobStatus.State.PREP;} else if (runState == JobStatus.State.KILLED.getValue()) {state = JobStatus.State.KILLED;} else {throw new IllegalArgumentException(runState + " is not a valid runState.");}getWrapped(context).abortJob(context, state);}
public void setupTask(TaskAttemptContext context) {getWrapped(context).setupTask(context);}
public void commitTask(TaskAttemptContext context) {getWrapped(context).commitTask(context, getTaskAttemptPath(context));}
public void abortTask(TaskAttemptContext context) {getWrapped(context).abortTask(context, getTaskAttemptPath(context));}
public boolean needsTaskCommit(TaskAttemptContext context) {return getWrapped(context).needsTaskCommit(context, getTaskAttemptPath(context));}
public boolean isRecoverySupported() {return true;}
public boolean isRecoverySupported(JobContext context) {return getWrapped(context).isRecoverySupported(context);}
public void recoverTask(TaskAttemptContext context) {getWrapped(context).recoverTask(context);}public void close() {RPC.stopProxy(rpcProxy);}
public void refreshCallQueue() {try {rpcProxy.refreshCallQueue(NULL_CONTROLLER, VOID_REFRESH_CALL_QUEUE_REQUEST);} catch (ServiceException se) {throw ProtobufHelper.getRemoteException(se);}}
public boolean isMethodSupported(String methodName) {return RpcClientUtil.isMethodSupported(rpcProxy, RefreshCallQueueProtocolPB.class, RPC.RpcKind.RPC_PROTOCOL_BUFFER, RPC.getProtocolVersion(RefreshCallQueueProtocolPB.class), methodName);}public void setUp() {configuration.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);configuration.set(YarnConfiguration.RM_HA_IDS, RM1_NODE_ID + "," + RM2_NODE_ID);for (String confKey : YarnConfiguration.getServiceAddressConfKeys(configuration)) {configuration.set(HAUtil.addSuffix(confKey, RM1_NODE_ID), RM1_ADDRESS);configuration.set(HAUtil.addSuffix(confKey, RM2_NODE_ID), RM2_ADDRESS);configuration.set(HAUtil.addSuffix(confKey, RM3_NODE_ID), RM3_ADDRESS);}// Enable webapp to test web-services alsoconfiguration.setBoolean(MockRM.ENABLE_WEBAPP, true);configuration.setBoolean(YarnConfiguration.YARN_ACL_ENABLE, true);ClusterMetrics.destroy();QueueMetrics.clearQueueMetrics();DefaultMetricsSystem.shutdown();}
private void checkMonitorHealth() {try {rm.adminService.monitorHealth();} catch (HealthCheckFailedException e) {fail("The RM is in bad health: it is Active, but the active services " + "are not running");}}
private void checkStandbyRMFunctionality() {assertEquals(STATE_ERR, HAServiceState.STANDBY, rm.adminService.getServiceStatus().getState());assertFalse("Active RM services are started", rm.areActiveServicesRunning());assertTrue("RM is not ready to become active", rm.adminService.getServiceStatus().isReadyToBecomeActive());}
private void checkActiveRMFunctionality() {assertEquals(STATE_ERR, HAServiceState.ACTIVE, rm.adminService.getServiceStatus().getState());assertTrue("Active RM services aren't started", rm.areActiveServicesRunning());assertTrue("RM is not ready to become active", rm.adminService.getServiceStatus().isReadyToBecomeActive());try {rm.getNewAppId();rm.registerNode("127.0.0.1:0", 2048);app = rm.submitApp(1024);attempt = app.getCurrentAppAttempt();rm.waitForState(attempt.getAppAttemptId(), RMAppAttemptState.SCHEDULED);} catch (Exception e) {fail("Unable to perform Active RM functions");LOG.error("ActiveRM check failed", e);}checkActiveRMWebServices();}
private void checkActiveRMWebServices() {// Validate web-serviceClient webServiceClient = Client.create(new DefaultClientConfig());InetSocketAddress rmWebappAddr = NetUtils.getConnectAddress(rm.getWebapp().getListenerAddress());String webappURL = "http://" + rmWebappAddr.getHostName() + ":" + rmWebappAddr.getPort();WebResource webResource = webServiceClient.resource(webappURL);String path = app.getApplicationId().toString();ClientResponse response = webResource.path("ws").path("v1").path("cluster").path("apps").path(path).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);assertEquals("incorrect number of elements", 1, json.length());JSONObject appJson = json.getJSONObject("app");assertEquals("ACCEPTED", appJson.getString("state"));}
public void testFailoverAndTransitions() {configuration.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);Configuration conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);StateChangeRequestInfo requestInfo = new StateChangeRequestInfo(HAServiceProtocol.RequestSource.REQUEST_BY_USER);assertEquals(STATE_ERR, HAServiceState.INITIALIZING, rm.adminService.getServiceStatus().getState());assertFalse("RM is ready to become active before being started", rm.adminService.getServiceStatus().isReadyToBecomeActive());checkMonitorHealth();rm.start();checkMonitorHealth();checkStandbyRMFunctionality();verifyClusterMetrics(0, 0, 0, 0, 0, 0);// 1. Transition to Standby - must be a no-oprm.adminService.transitionToStandby(requestInfo);checkMonitorHealth();checkStandbyRMFunctionality();verifyClusterMetrics(0, 0, 0, 0, 0, 0);// 2. Transition to activerm.adminService.transitionToActive(requestInfo);checkMonitorHealth();checkActiveRMFunctionality();verifyClusterMetrics(1, 1, 1, 1, 2048, 1);// 3. Transition to active - no-oprm.adminService.transitionToActive(requestInfo);checkMonitorHealth();checkActiveRMFunctionality();verifyClusterMetrics(1, 2, 2, 2, 2048, 2);// 4. Transition to standbyrm.adminService.transitionToStandby(requestInfo);checkMonitorHealth();checkStandbyRMFunctionality();verifyClusterMetrics(0, 0, 0, 0, 0, 0);// 5. Transition to active to check Active->Standby->Active worksrm.adminService.transitionToActive(requestInfo);checkMonitorHealth();checkActiveRMFunctionality();verifyClusterMetrics(1, 1, 1, 1, 2048, 1);// become activerm.stop();assertEquals(STATE_ERR, HAServiceState.STOPPING, rm.adminService.getServiceStatus().getState());assertFalse("RM is ready to become active even after it is stopped", rm.adminService.getServiceStatus().isReadyToBecomeActive());assertFalse("Active RM services are started", rm.areActiveServicesRunning());checkMonitorHealth();}
public void testTransitionsWhenAutomaticFailoverEnabled() {final String ERR_UNFORCED_REQUEST = "User request succeeded even when " + "automatic failover is enabled";Configuration conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);rm.start();StateChangeRequestInfo requestInfo = new StateChangeRequestInfo(HAServiceProtocol.RequestSource.REQUEST_BY_USER);// Transition to standbytry {rm.adminService.transitionToStandby(requestInfo);fail(ERR_UNFORCED_REQUEST);} catch (AccessControlException e) {}checkMonitorHealth();checkStandbyRMFunctionality();// Transition to activetry {rm.adminService.transitionToActive(requestInfo);fail(ERR_UNFORCED_REQUEST);} catch (AccessControlException e) {}checkMonitorHealth();checkStandbyRMFunctionality();final String ERR_FORCED_REQUEST = "Forced request by user should work " + "even if automatic failover is enabled";requestInfo = new StateChangeRequestInfo(HAServiceProtocol.RequestSource.REQUEST_BY_USER_FORCED);// Transition to standbytry {rm.adminService.transitionToStandby(requestInfo);} catch (AccessControlException e) {fail(ERR_FORCED_REQUEST);}checkMonitorHealth();checkStandbyRMFunctionality();// Transition to activetry {rm.adminService.transitionToActive(requestInfo);} catch (AccessControlException e) {fail(ERR_FORCED_REQUEST);}checkMonitorHealth();checkActiveRMFunctionality();}
public void testRMDispatcherForHA() {String errorMessageForEventHandler = "Expect to get the same number of handlers";String errorMessageForService = "Expect to get the same number of services";configuration.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);Configuration conf = new YarnConfiguration(configuration);rm = new MockRM(conf) {
@Overrideprotected Dispatcher createDispatcher() {return new MyCountingDispatcher();}};rm.init(conf);int expectedEventHandlerCount = ((MyCountingDispatcher) rm.getRMContext().getDispatcher()).getEventHandlerCount();int expectedServiceCount = rm.getServices().size();assertTrue(expectedEventHandlerCount != 0);StateChangeRequestInfo requestInfo = new StateChangeRequestInfo(HAServiceProtocol.RequestSource.REQUEST_BY_USER);assertEquals(STATE_ERR, HAServiceState.INITIALIZING, rm.adminService.getServiceStatus().getState());assertFalse("RM is ready to become active before being started", rm.adminService.getServiceStatus().isReadyToBecomeActive());rm.start();//call transitions to standby and active a couple of timesrm.adminService.transitionToStandby(requestInfo);rm.adminService.transitionToActive(requestInfo);rm.adminService.transitionToStandby(requestInfo);rm.adminService.transitionToActive(requestInfo);rm.adminService.transitionToStandby(requestInfo);MyCountingDispatcher dispatcher = (MyCountingDispatcher) rm.getRMContext().getDispatcher();assertTrue(!dispatcher.isStopped());rm.adminService.transitionToActive(requestInfo);assertEquals(errorMessageForEventHandler, expectedEventHandlerCount, ((MyCountingDispatcher) rm.getRMContext().getDispatcher()).getEventHandlerCount());assertEquals(errorMessageForService, expectedServiceCount, rm.getServices().size());// Keep the dispatcher reference before transitioning to standbydispatcher = (MyCountingDispatcher) rm.getRMContext().getDispatcher();rm.adminService.transitionToStandby(requestInfo);assertEquals(errorMessageForEventHandler, expectedEventHandlerCount, ((MyCountingDispatcher) rm.getRMContext().getDispatcher()).getEventHandlerCount());assertEquals(errorMessageForService, expectedServiceCount, rm.getServices().size());assertTrue(dispatcher.isStopped());rm.stop();}
protected Dispatcher createDispatcher() {return new MyCountingDispatcher();}
public void testHAIDLookup() {//test implicitly lookup HA-IDConfiguration conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);assertEquals(conf.get(YarnConfiguration.RM_HA_ID), RM2_NODE_ID);//test explicitly lookup HA-IDconfiguration.set(YarnConfiguration.RM_HA_ID, RM1_NODE_ID);conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);assertEquals(conf.get(YarnConfiguration.RM_HA_ID), RM1_NODE_ID);//test if RM_HA_ID can not be foundconfiguration.set(YarnConfiguration.RM_HA_IDS, RM1_NODE_ID + "," + RM3_NODE_ID);configuration.unset(YarnConfiguration.RM_HA_ID);conf = new YarnConfiguration(configuration);try {rm = new MockRM(conf);rm.init(conf);fail("Should get an exception here.");} catch (Exception ex) {Assert.assertTrue(ex.getMessage().contains("Invalid configuration! Can not find valid RM_HA_ID."));}}
public void testHAWithRMHostName() {innerTestHAWithRMHostName(false);configuration.clear();setUp();innerTestHAWithRMHostName(true);}
public void innerTestHAWithRMHostName(boolean includeBindHost) {//this is run two times, with and without a bind host configuredif (includeBindHost) {configuration.set(YarnConfiguration.RM_BIND_HOST, "9.9.9.9");}//We should only read rpc addresses from RM_RPCADDRESS_{rm_id} configurationconfiguration.set(HAUtil.addSuffix(YarnConfiguration.RM_HOSTNAME, RM1_NODE_ID), "1.1.1.1");configuration.set(HAUtil.addSuffix(YarnConfiguration.RM_HOSTNAME, RM2_NODE_ID), "0.0.0.0");configuration.set(HAUtil.addSuffix(YarnConfiguration.RM_HOSTNAME, RM3_NODE_ID), "2.2.2.2");try {Configuration conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);for (String confKey : YarnConfiguration.getServiceAddressConfKeys(conf)) {assertEquals("RPC address not set for " + confKey, RM1_ADDRESS, conf.get(HAUtil.addSuffix(confKey, RM1_NODE_ID)));assertEquals("RPC address not set for " + confKey, RM2_ADDRESS, conf.get(HAUtil.addSuffix(confKey, RM2_NODE_ID)));assertEquals("RPC address not set for " + confKey, RM3_ADDRESS, conf.get(HAUtil.addSuffix(confKey, RM3_NODE_ID)));if (includeBindHost) {assertEquals("Web address misconfigured WITH bind-host", rm.webAppAddress.substring(0, 7), "9.9.9.9");} else {//which doesn't happen for any of these fake addresses, so we end up with 0.0.0.0assertEquals("Web address misconfigured WITHOUT bind-host", rm.webAppAddress.substring(0, 7), "0.0.0.0");}}} catch (YarnRuntimeException e) {fail("Should not throw any exceptions.");}//test if only RM_HOSTBANE_{rm_id} is setconfiguration.clear();configuration.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);configuration.set(YarnConfiguration.RM_HA_IDS, RM1_NODE_ID + "," + RM2_NODE_ID);configuration.set(HAUtil.addSuffix(YarnConfiguration.RM_HOSTNAME, RM1_NODE_ID), "1.1.1.1");configuration.set(HAUtil.addSuffix(YarnConfiguration.RM_HOSTNAME, RM2_NODE_ID), "0.0.0.0");try {Configuration conf = new YarnConfiguration(configuration);rm = new MockRM(conf);rm.init(conf);assertEquals("RPC address not set for " + YarnConfiguration.RM_ADDRESS, "1.1.1.1:8032", conf.get(HAUtil.addSuffix(YarnConfiguration.RM_ADDRESS, RM1_NODE_ID)));assertEquals("RPC address not set for " + YarnConfiguration.RM_ADDRESS, "0.0.0.0:8032", conf.get(HAUtil.addSuffix(YarnConfiguration.RM_ADDRESS, RM2_NODE_ID)));} catch (YarnRuntimeException e) {fail("Should not throw any exceptions.");}}
private void verifyClusterMetrics(int activeNodes, int appsSubmitted, int appsPending, int containersPending, int availableMB, int activeApplications) {int timeoutSecs = 0;QueueMetrics metrics = rm.getResourceScheduler().getRootQueueMetrics();ClusterMetrics clusterMetrics = ClusterMetrics.getMetrics();boolean isAllMetricAssertionDone = false;String message = null;while (timeoutSecs++ < 5) {try {// verify queue metricsassertMetric("appsSubmitted", appsSubmitted, metrics.getAppsSubmitted());assertMetric("appsPending", appsPending, metrics.getAppsPending());assertMetric("containersPending", containersPending, metrics.getPendingContainers());assertMetric("availableMB", availableMB, metrics.getAvailableMB());assertMetric("activeApplications", activeApplications, metrics.getActiveApps());// verify node metricassertMetric("activeNodes", activeNodes, clusterMetrics.getNumActiveNMs());isAllMetricAssertionDone = true;break;} catch (AssertionError e) {message = e.getMessage();System.out.println("Waiting for metrics assertion to complete");Thread.sleep(1000);}}assertTrue(message, isAllMetricAssertionDone);}
private void assertMetric(String metricName, int expected, int actual) {assertEquals("Incorrect value for metric " + metricName, expected, actual);}
public EventHandler getEventHandler() {return null;}
public void register(Class<? extends Enum> eventType, EventHandler handler) {this.eventHandlerCount++;}
public int getEventHandlerCount() {return this.eventHandlerCount;}
protected void serviceStop() {this.stopped = true;super.serviceStop();}
public boolean isStopped() {return this.stopped;}public static SchedulingPolicy getInstance(Class<? extends SchedulingPolicy> clazz) {SchedulingPolicy policy = instances.get(clazz);if (policy == null) {policy = ReflectionUtils.newInstance(clazz, null);instances.put(clazz, policy);}return policy;}
public static SchedulingPolicy parse(String policy) {@SuppressWarnings("rawtypes") Class clazz;String text = policy.toLowerCase();if (text.equalsIgnoreCase(FairSharePolicy.NAME)) {clazz = FairSharePolicy.class;} else if (text.equalsIgnoreCase(FifoPolicy.NAME)) {clazz = FifoPolicy.class;} else if (text.equalsIgnoreCase(DominantResourceFairnessPolicy.NAME)) {clazz = DominantResourceFairnessPolicy.class;} else {try {clazz = Class.forName(policy);} catch (ClassNotFoundException cnfe) {throw new AllocationConfigurationException(policy + " SchedulingPolicy class not found!");}}if (!SchedulingPolicy.class.isAssignableFrom(clazz)) {throw new AllocationConfigurationException(policy + " does not extend SchedulingPolicy");}return getInstance(clazz);}
public void initialize(Resource clusterCapacity) {}
public static boolean isApplicableTo(SchedulingPolicy policy, byte depth) {return ((policy.getApplicableDepth() & depth) == depth) ? true : false;}public void testSetFile() {FileSystem fs = FileSystem.getLocal(conf);try {RandomDatum[] data = generate(10000);writeTest(fs, data, FILE, CompressionType.NONE);readTest(fs, data, FILE);writeTest(fs, data, FILE, CompressionType.BLOCK);readTest(fs, data, FILE);} finally {fs.close();}}
public void testSetFileAccessMethods() {try {FileSystem fs = FileSystem.getLocal(conf);int size = 10;writeData(fs, size);SetFile.Reader reader = createReader(fs);assertTrue("testSetFileWithConstruction1 error !!!", reader.next(new IntWritable(0)));// don't know why reader.get(i) return i+1assertEquals("testSetFileWithConstruction2 error !!!", new IntWritable(size / 2 + 1), reader.get(new IntWritable(size / 2)));assertNull("testSetFileWithConstruction3 error !!!", reader.get(new IntWritable(size * 2)));} catch (Exception ex) {fail("testSetFileWithConstruction error !!!");}}
private SetFile.Reader createReader(FileSystem fs) {return new SetFile.Reader(fs, FILE, WritableComparator.get(IntWritable.class), conf);}
private void writeData(FileSystem fs, int elementSize) {MapFile.delete(fs, FILE);SetFile.Writer writer = new SetFile.Writer(fs, FILE, IntWritable.class);for (int i = 0; i < elementSize; i++) writer.append(new IntWritable(i));writer.close();}
private static RandomDatum[] generate(int count) {LOG.info("generating " + count + " records in memory");RandomDatum[] data = new RandomDatum[count];RandomDatum.Generator generator = new RandomDatum.Generator();for (int i = 0; i < count; i++) {generator.next();data[i] = generator.getValue();}LOG.info("sorting " + count + " records");Arrays.sort(data);return data;}
private static void writeTest(FileSystem fs, RandomDatum[] data, String file, CompressionType compress) {MapFile.delete(fs, file);LOG.info("creating with " + data.length + " records");SetFile.Writer writer = new SetFile.Writer(conf, fs, file, WritableComparator.get(RandomDatum.class), compress);for (int i = 0; i < data.length; i++) writer.append(data[i]);writer.close();}
private static void readTest(FileSystem fs, RandomDatum[] data, String file) {RandomDatum v = new RandomDatum();int sample = (int) Math.sqrt(data.length);Random random = new Random();LOG.info("reading " + sample + " records");SetFile.Reader reader = new SetFile.Reader(fs, file, conf);for (int i = 0; i < sample; i++) {if (!reader.seek(data[random.nextInt(data.length)]))throw new RuntimeException("wrong value at " + i);}reader.close();LOG.info("done reading " + data.length);}
public static void main(String[] args) {int count = 1024 * 1024;boolean create = true;boolean check = true;String file = FILE;String compress = "NONE";String usage = "Usage: TestSetFile [-count N] [-nocreate] [-nocheck] [-compress type] file";if (args.length == 0) {System.err.println(usage);System.exit(-1);}int i = 0;Path fpath = null;FileSystem fs = null;try {for (; i < args.length; i++) {if (args[i] == null) {continue;} else if (args[i].equals("-count")) {count = Integer.parseInt(args[++i]);} else if (args[i].equals("-nocreate")) {create = false;} else if (args[i].equals("-nocheck")) {check = false;} else if (args[i].equals("-compress")) {compress = args[++i];} else {file = args[i];fpath = new Path(file);}}fs = fpath.getFileSystem(conf);LOG.info("count = " + count);LOG.info("create = " + create);LOG.info("check = " + check);LOG.info("compress = " + compress);LOG.info("file = " + file);RandomDatum[] data = generate(count);if (create) {writeTest(fs, data, file, CompressionType.valueOf(compress));}if (check) {readTest(fs, data, file);}} finally {fs.close();}}public synchronized int decompress(byte[] b, int off, int len) {try {return super.inflate(b, off, len);} catch (DataFormatException dfe) {throw new IOException(dfe.getMessage());}}public void setupHM() {Configuration conf = new Configuration();conf.setInt(CommonConfigurationKeys.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);conf.setInt(CommonConfigurationKeys.HA_HM_CHECK_INTERVAL_KEY, 50);conf.setInt(CommonConfigurationKeys.HA_HM_CONNECT_RETRY_INTERVAL_KEY, 50);conf.setInt(CommonConfigurationKeys.HA_HM_SLEEP_AFTER_DISCONNECT_KEY, 50);svc = new DummyHAService(HAServiceState.ACTIVE, null);hm = new HealthMonitor(conf, svc) {
@Overrideprotected HAServiceProtocol createProxy() throws IOException {createProxyCount.incrementAndGet();if (throwOOMEOnCreate) {throw new OutOfMemoryError("oome");}return super.createProxy();}};LOG.info("Starting health monitor");hm.start();LOG.info("Waiting for HEALTHY signal");waitForState(hm, HealthMonitor.State.SERVICE_HEALTHY);}
protected HAServiceProtocol createProxy() {createProxyCount.incrementAndGet();if (throwOOMEOnCreate) {throw new OutOfMemoryError("oome");}return super.createProxy();}
public void testMonitor() {LOG.info("Mocking bad health check, waiting for UNHEALTHY");svc.isHealthy = false;waitForState(hm, HealthMonitor.State.SERVICE_UNHEALTHY);LOG.info("Returning to healthy state, waiting for HEALTHY");svc.isHealthy = true;waitForState(hm, HealthMonitor.State.SERVICE_HEALTHY);LOG.info("Returning an IOException, as if node went down");// should expect many rapid retriesint countBefore = createProxyCount.get();svc.actUnreachable = true;waitForState(hm, HealthMonitor.State.SERVICE_NOT_RESPONDING);// Should retry several timeswhile (createProxyCount.get() < countBefore + 3) {Thread.sleep(10);}LOG.info("Returning to healthy state, waiting for HEALTHY");svc.actUnreachable = false;waitForState(hm, HealthMonitor.State.SERVICE_HEALTHY);hm.shutdown();hm.join();assertFalse(hm.isAlive());}
public void testHealthMonitorDies() {LOG.info("Mocking RTE in health monitor, waiting for FAILED");throwOOMEOnCreate = true;svc.actUnreachable = true;waitForState(hm, HealthMonitor.State.HEALTH_MONITOR_FAILED);hm.shutdown();hm.join();assertFalse(hm.isAlive());}
public void testCallbackThrowsRTE() {hm.addCallback(new Callback() {
@Overridepublic void enteredState(State newState) {throw new RuntimeException("Injected RTE");}});LOG.info("Mocking bad health check, waiting for UNHEALTHY");svc.isHealthy = false;waitForState(hm, HealthMonitor.State.HEALTH_MONITOR_FAILED);}
public void enteredState(State newState) {throw new RuntimeException("Injected RTE");}
private void waitForState(HealthMonitor hm, State state) {long st = Time.now();while (Time.now() - st < 2000) {if (hm.getHealthState() == state) {return;}Thread.sleep(50);}assertEquals(state, hm.getHealthState());}public void setup() {// create keytabFile keytabFile = new File(KerberosTestUtils.getKeytabFile());String clientPrincipal = KerberosTestUtils.getClientPrincipal();String serverPrincipal = KerberosTestUtils.getServerPrincipal();clientPrincipal = clientPrincipal.substring(0, clientPrincipal.lastIndexOf("@"));serverPrincipal = serverPrincipal.substring(0, serverPrincipal.lastIndexOf("@"));getKdc().createPrincipal(keytabFile, clientPrincipal, serverPrincipal);}
private Properties getAuthenticationHandlerConfiguration() {Properties props = new Properties();props.setProperty(AuthenticationFilter.AUTH_TYPE, "kerberos");props.setProperty(KerberosAuthenticationHandler.PRINCIPAL, KerberosTestUtils.getServerPrincipal());props.setProperty(KerberosAuthenticationHandler.KEYTAB, KerberosTestUtils.getKeytabFile());props.setProperty(KerberosAuthenticationHandler.NAME_RULES, "RULE:[1:$1@$0](.*@" + KerberosTestUtils.getRealm() + ")s/@.*//\n");return props;}
public void testFallbacktoPseudoAuthenticator() {AuthenticatorTestCase auth = new AuthenticatorTestCase();Properties props = new Properties();props.setProperty(AuthenticationFilter.AUTH_TYPE, "simple");props.setProperty(PseudoAuthenticationHandler.ANONYMOUS_ALLOWED, "false");AuthenticatorTestCase.setAuthenticationHandlerConfig(props);auth._testAuthentication(new KerberosAuthenticator(), false);}
public void testFallbacktoPseudoAuthenticatorAnonymous() {AuthenticatorTestCase auth = new AuthenticatorTestCase();Properties props = new Properties();props.setProperty(AuthenticationFilter.AUTH_TYPE, "simple");props.setProperty(PseudoAuthenticationHandler.ANONYMOUS_ALLOWED, "true");AuthenticatorTestCase.setAuthenticationHandlerConfig(props);auth._testAuthentication(new KerberosAuthenticator(), false);}
public void testNotAuthenticated() {AuthenticatorTestCase auth = new AuthenticatorTestCase();AuthenticatorTestCase.setAuthenticationHandlerConfig(getAuthenticationHandlerConfiguration());auth.start();try {URL url = new URL(auth.getBaseURL());HttpURLConnection conn = (HttpURLConnection) url.openConnection();conn.connect();Assert.assertEquals(HttpURLConnection.HTTP_UNAUTHORIZED, conn.getResponseCode());Assert.assertTrue(conn.getHeaderField(KerberosAuthenticator.WWW_AUTHENTICATE) != null);} finally {auth.stop();}}
public void testAuthentication() {final AuthenticatorTestCase auth = new AuthenticatorTestCase();AuthenticatorTestCase.setAuthenticationHandlerConfig(getAuthenticationHandlerConfiguration());KerberosTestUtils.doAsClient(new Callable<Void>() {
@Overridepublic Void call() throws Exception {auth._testAuthentication(new KerberosAuthenticator(), false);return null;}});}
public Void call() {auth._testAuthentication(new KerberosAuthenticator(), false);return null;}
public void testAuthenticationPost() {final AuthenticatorTestCase auth = new AuthenticatorTestCase();AuthenticatorTestCase.setAuthenticationHandlerConfig(getAuthenticationHandlerConfiguration());KerberosTestUtils.doAsClient(new Callable<Void>() {
@Overridepublic Void call() throws Exception {auth._testAuthentication(new KerberosAuthenticator(), true);return null;}});}
public Void call() {auth._testAuthentication(new KerberosAuthenticator(), true);return null;}public String getDescription() {return description;}
public String getName() {return name;}public void testDataNodeMetrics() {Configuration conf = new HdfsConfiguration();SimulatedFSDataset.setFactory(conf);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();try {FileSystem fs = cluster.getFileSystem();final long LONG_FILE_LEN = Integer.MAX_VALUE + 1L;DFSTestUtil.createFile(fs, new Path("/tmp.txt"), LONG_FILE_LEN, (short) 1, 1L);List<DataNode> datanodes = cluster.getDataNodes();assertEquals(datanodes.size(), 1);DataNode datanode = datanodes.get(0);MetricsRecordBuilder rb = getMetrics(datanode.getMetrics().name());assertCounter("BytesWritten", LONG_FILE_LEN, rb);} finally {if (cluster != null) {cluster.shutdown();}}}
public void testSendDataPacketMetrics() {Configuration conf = new HdfsConfiguration();final int interval = 1;conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, "" + interval);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();try {FileSystem fs = cluster.getFileSystem();// Create and read a 1 byte filePath tmpfile = new Path("/tmp.txt");DFSTestUtil.createFile(fs, tmpfile, (long) 1, (short) 1, 1L);DFSTestUtil.readFile(fs, tmpfile);List<DataNode> datanodes = cluster.getDataNodes();assertEquals(datanodes.size(), 1);DataNode datanode = datanodes.get(0);MetricsRecordBuilder rb = getMetrics(datanode.getMetrics().name());// signaling the end of the blockassertCounter("SendDataPacketTransferNanosNumOps", (long) 2, rb);assertCounter("SendDataPacketBlockedOnNetworkNanosNumOps", (long) 2, rb);// Wait for at least 1 rolloverThread.sleep((interval + 1) * 1000);// Check that the sendPacket percentiles rolled to non-zero valuesString sec = interval + "s";assertQuantileGauges("SendDataPacketBlockedOnNetworkNanos" + sec, rb);assertQuantileGauges("SendDataPacketTransferNanos" + sec, rb);} finally {if (cluster != null) {cluster.shutdown();}}}
public void testReceivePacketMetrics() {Configuration conf = new HdfsConfiguration();final int interval = 1;conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, "" + interval);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();try {cluster.waitActive();DistributedFileSystem fs = cluster.getFileSystem();Path testFile = new Path("/testFlushNanosMetric.txt");FSDataOutputStream fout = fs.create(testFile);fout.write(new byte[1]);fout.hsync();fout.close();List<DataNode> datanodes = cluster.getDataNodes();DataNode datanode = datanodes.get(0);MetricsRecordBuilder dnMetrics = getMetrics(datanode.getMetrics().name());// 1 that occurs on closing the data and metadata files.assertCounter("FlushNanosNumOps", 2L, dnMetrics);// Expect two syncs, one from the hsync, one on close.assertCounter("FsyncNanosNumOps", 2L, dnMetrics);// Wait for at least 1 rolloverThread.sleep((interval + 1) * 1000);// Check the receivePacket percentiles that should be non-zeroString sec = interval + "s";assertQuantileGauges("FlushNanos" + sec, dnMetrics);assertQuantileGauges("FsyncNanos" + sec, dnMetrics);} finally {if (cluster != null) {cluster.shutdown();}}}
public void testRoundTripAckMetric() {final int datanodeCount = 2;final int interval = 1;Configuration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, "" + interval);MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(datanodeCount).build();try {cluster.waitActive();FileSystem fs = cluster.getFileSystem();Path testFile = new Path("/testRoundTripAckMetric.txt");FSDataOutputStream fsout = fs.create(testFile, (short) datanodeCount);DFSOutputStream dout = (DFSOutputStream) fsout.getWrappedStream();dout.setChunksPerPacket(5);dout.setArtificialSlowdown(3000);fsout.write(new byte[10000]);DatanodeInfo[] pipeline = null;int count = 0;while (pipeline == null && count < 5) {pipeline = dout.getPipeline();System.out.println("Waiting for pipeline to be created.");Thread.sleep(1000);count++;}// Get the head node that should be receiving downstream acksDatanodeInfo headInfo = pipeline[0];DataNode headNode = null;for (DataNode datanode : cluster.getDataNodes()) {if (datanode.getDatanodeId().equals(headInfo)) {headNode = datanode;break;}}assertNotNull("Could not find the head of the datanode write pipeline", headNode);// Close the file and wait for the metrics to rolloverThread.sleep((interval + 1) * 1000);// Check the ack was receivedMetricsRecordBuilder dnMetrics = getMetrics(headNode.getMetrics().name());assertTrue("Expected non-zero number of acks", getLongCounter("PacketAckRoundTripTimeNanosNumOps", dnMetrics) > 0);assertQuantileGauges("PacketAckRoundTripTimeNanos" + interval + "s", dnMetrics);} finally {if (cluster != null) {cluster.shutdown();}}}public void setUp() {super.setUp();conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();fs = cluster.getFileSystem();namenode = conf.get(DFSConfigKeys.FS_DEFAULT_NAME_KEY, "file:///");username = System.getProperty("user.name");}
public void tearDown() {super.tearDown();if (fs != null) {fs.close();}if (cluster != null) {cluster.shutdown();}}
protected String getTestFile() {return "testAclCLI.xml";}
protected String expandCommand(final String cmd) {String expCmd = cmd;expCmd = expCmd.replaceAll("NAMENODE", namenode);expCmd = expCmd.replaceAll("USERNAME", username);expCmd = expCmd.replaceAll("#LF#", System.getProperty("line.separator"));expCmd = super.expandCommand(expCmd);return expCmd;}
protected Result execute(CLICommand cmd) {return cmd.getExecutor(namenode).executeCommand(cmd.getCmd());}
public void testAll() {super.testAll();}public void testJobId() {long ts1 = 1315890136000l;long ts2 = 1315890136001l;JobId j1 = createJobId(ts1, 2);JobId j2 = createJobId(ts1, 1);JobId j3 = createJobId(ts2, 1);JobId j4 = createJobId(ts1, 2);assertTrue(j1.equals(j4));assertFalse(j1.equals(j2));assertFalse(j1.equals(j3));assertTrue(j1.compareTo(j4) == 0);assertTrue(j1.compareTo(j2) > 0);assertTrue(j1.compareTo(j3) < 0);assertTrue(j1.hashCode() == j4.hashCode());assertFalse(j1.hashCode() == j2.hashCode());assertFalse(j1.hashCode() == j3.hashCode());JobId j5 = createJobId(ts1, 231415);assertEquals("job_" + ts1 + "_0002", j1.toString());assertEquals("job_" + ts1 + "_231415", j5.toString());}
public void testTaskId() {long ts1 = 1315890136000l;long ts2 = 1315890136001l;TaskId t1 = createTaskId(ts1, 1, 2, TaskType.MAP);TaskId t2 = createTaskId(ts1, 1, 2, TaskType.REDUCE);TaskId t3 = createTaskId(ts1, 1, 1, TaskType.MAP);TaskId t4 = createTaskId(ts1, 1, 2, TaskType.MAP);TaskId t5 = createTaskId(ts2, 1, 1, TaskType.MAP);assertTrue(t1.equals(t4));assertFalse(t1.equals(t2));assertFalse(t1.equals(t3));assertFalse(t1.equals(t5));assertTrue(t1.compareTo(t4) == 0);assertTrue(t1.compareTo(t2) < 0);assertTrue(t1.compareTo(t3) > 0);assertTrue(t1.compareTo(t5) < 0);assertTrue(t1.hashCode() == t4.hashCode());assertFalse(t1.hashCode() == t2.hashCode());assertFalse(t1.hashCode() == t3.hashCode());assertFalse(t1.hashCode() == t5.hashCode());TaskId t6 = createTaskId(ts1, 324151, 54643747, TaskType.REDUCE);assertEquals("task_" + ts1 + "_0001_m_000002", t1.toString());assertEquals("task_" + ts1 + "_324151_r_54643747", t6.toString());}
public void testTaskAttemptId() {long ts1 = 1315890136000l;long ts2 = 1315890136001l;TaskAttemptId t1 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 2);TaskAttemptId t2 = createTaskAttemptId(ts1, 2, 2, TaskType.REDUCE, 2);TaskAttemptId t3 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 3);TaskAttemptId t4 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 1);TaskAttemptId t5 = createTaskAttemptId(ts1, 2, 1, TaskType.MAP, 3);TaskAttemptId t6 = createTaskAttemptId(ts1, 2, 2, TaskType.MAP, 2);assertTrue(t1.equals(t6));assertFalse(t1.equals(t2));assertFalse(t1.equals(t3));assertFalse(t1.equals(t5));assertTrue(t1.compareTo(t6) == 0);assertTrue(t1.compareTo(t2) < 0);assertTrue(t1.compareTo(t3) < 0);assertTrue(t1.compareTo(t4) > 0);assertTrue(t1.compareTo(t5) > 0);assertTrue(t1.hashCode() == t6.hashCode());assertFalse(t1.hashCode() == t2.hashCode());assertFalse(t1.hashCode() == t3.hashCode());assertFalse(t1.hashCode() == t5.hashCode());TaskAttemptId t7 = createTaskAttemptId(ts2, 5463346, 4326575, TaskType.REDUCE, 54375);assertEquals("attempt_" + ts1 + "_0002_m_000002_2", t1.toString());assertEquals("attempt_" + ts2 + "_5463346_r_4326575_54375", t7.toString());}
private JobId createJobId(long clusterTimestamp, int idInt) {return MRBuilderUtils.newJobId(ApplicationId.newInstance(clusterTimestamp, idInt), idInt);}
private TaskId createTaskId(long clusterTimestamp, int jobIdInt, int taskIdInt, TaskType taskType) {return MRBuilderUtils.newTaskId(createJobId(clusterTimestamp, jobIdInt), taskIdInt, taskType);}
private TaskAttemptId createTaskAttemptId(long clusterTimestamp, int jobIdInt, int taskIdInt, TaskType taskType, int taskAttemptIdInt) {return MRBuilderUtils.newTaskAttemptId(createTaskId(clusterTimestamp, jobIdInt, taskIdInt, taskType), taskAttemptIdInt);}static void testRequest(XDR request, int serverPort) {// Reset resultSize so as to avoid interference from other tests in this class.resultSize = 0;SimpleTcpClient tcpClient = new SimpleTcpClient("localhost", serverPort, request, true);tcpClient.run();}
protected void handleInternal(ChannelHandlerContext ctx, RpcInfo info) {// RpcProgramNfs3#handleInternal.RpcCall rpcCall = (RpcCall) info.header();final int procedure = rpcCall.getProcedure();if (procedure != 0) {boolean portMonitorSuccess = doPortMonitoring(info.remoteAddress());if (!portMonitorSuccess) {sendRejectedReply(rpcCall, info.remoteAddress(), ctx);return;}}resultSize = info.data().readableBytes();RpcAcceptedReply reply = RpcAcceptedReply.getAcceptInstance(1234, new VerifierNone());XDR out = new XDR();reply.write(out);ChannelBuffer b = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap().buffer());RpcResponse rsp = new RpcResponse(b, info.remoteAddress());RpcUtil.sendRpcResponse(ctx, rsp);}
protected boolean isIdempotent(RpcCall call) {return false;}
public void testSingleFrame() {RpcFrameDecoder decoder = new RpcFrameDecoder();ByteBuffer buffer = ByteBuffer.allocate(1);ChannelBuffer buf = new ByteBufferBackedChannelBuffer(buffer);ChannelBuffer channelBuffer = (ChannelBuffer) decoder.decode(Mockito.mock(ChannelHandlerContext.class), Mockito.mock(Channel.class), buf);assertTrue(channelBuffer == null);// Test all bytes are not received yetbyte[] fragment = new byte[4 + 9];// final fragmentfragment[0] = (byte) (1 << 7);fragment[1] = 0;fragment[2] = 0;// fragment size = 10 bytesfragment[3] = (byte) 10;assertTrue(XDR.isLastFragment(fragment));assertTrue(XDR.fragmentSize(fragment) == 10);buffer = ByteBuffer.allocate(4 + 9);buffer.put(fragment);buffer.flip();buf = new ByteBufferBackedChannelBuffer(buffer);channelBuffer = (ChannelBuffer) decoder.decode(Mockito.mock(ChannelHandlerContext.class), Mockito.mock(Channel.class), buf);assertTrue(channelBuffer == null);}
public void testMultipleFrames() {RpcFrameDecoder decoder = new RpcFrameDecoder();// Test multiple framesbyte[] fragment1 = new byte[4 + 10];fragment1[0] = 0;fragment1[1] = 0;fragment1[2] = 0;fragment1[3] = (byte) 10;assertFalse(XDR.isLastFragment(fragment1));assertTrue(XDR.fragmentSize(fragment1) == 10);ByteBuffer buffer = ByteBuffer.allocate(4 + 10);buffer.put(fragment1);buffer.flip();ChannelBuffer buf = new ByteBufferBackedChannelBuffer(buffer);ChannelBuffer channelBuffer = (ChannelBuffer) decoder.decode(Mockito.mock(ChannelHandlerContext.class), Mockito.mock(Channel.class), buf);assertTrue(channelBuffer == null);byte[] fragment2 = new byte[4 + 10];// final fragmentfragment2[0] = (byte) (1 << 7);fragment2[1] = 0;fragment2[2] = 0;// fragment size = 10 bytesfragment2[3] = (byte) 10;assertTrue(XDR.isLastFragment(fragment2));assertTrue(XDR.fragmentSize(fragment2) == 10);buffer = ByteBuffer.allocate(4 + 10);buffer.put(fragment2);buffer.flip();buf = new ByteBufferBackedChannelBuffer(buffer);channelBuffer = (ChannelBuffer) decoder.decode(Mockito.mock(ChannelHandlerContext.class), Mockito.mock(Channel.class), buf);assertTrue(channelBuffer != null);// Complete frame should have to total size 10+10=20assertEquals(20, channelBuffer.readableBytes());}
public void testFrames() {int serverPort = startRpcServer(true);XDR xdrOut = createGetportMount();int headerSize = xdrOut.size();int bufsize = 2 * 1024 * 1024;byte[] buffer = new byte[bufsize];xdrOut.writeFixedOpaque(buffer);int requestSize = xdrOut.size() - headerSize;// Send the request to the servertestRequest(xdrOut, serverPort);// Verify the server got the request with right sizeassertEquals(requestSize, resultSize);}
public void testUnprivilegedPort() {int serverPort = startRpcServer(false);XDR xdrOut = createGetportMount();int bufsize = 2 * 1024 * 1024;byte[] buffer = new byte[bufsize];xdrOut.writeFixedOpaque(buffer);// Send the request to the servertestRequest(xdrOut, serverPort);// Verify the server rejected the request.assertEquals(0, resultSize);// Ensure that the NULL procedure does in fact succeed.xdrOut = new XDR();createPortmapXDRheader(xdrOut, 0);int headerSize = xdrOut.size();buffer = new byte[bufsize];xdrOut.writeFixedOpaque(buffer);int requestSize = xdrOut.size() - headerSize;// Send the request to the servertestRequest(xdrOut, serverPort);// Verify the server did not reject the request.assertEquals(requestSize, resultSize);}
private static int startRpcServer(boolean allowInsecurePorts) {Random rand = new Random();int serverPort = 30000 + rand.nextInt(10000);// A few retries in case initial choice is in use.int retries = 10;while (true) {try {RpcProgram program = new TestFrameDecoder.TestRpcProgram("TestRpcProgram", "localhost", serverPort, 100000, 1, 2, allowInsecurePorts);SimpleTcpServer tcpServer = new SimpleTcpServer(serverPort, program, 1);tcpServer.run();// Successfully bound a port, break out.break;} catch (ChannelException ce) {if (retries-- > 0) {serverPort += rand.nextInt(20);} else {throw ce;}}}return serverPort;}
static void createPortmapXDRheader(XDR xdr_out, int procedure) {// Make this a methodRpcCall.getInstance(0, 100000, 2, procedure, new CredentialsNone(), new VerifierNone()).write(xdr_out);}
static XDR createGetportMount() {XDR xdr_out = new XDR();createPortmapXDRheader(xdr_out, 3);return xdr_out;}public short getVersion() {return version;}
public DataChecksum getChecksum() {return checksum;}
public static BlockMetadataHeader preadHeader(FileChannel fc) {byte arr[] = new byte[2 + DataChecksum.HEADER_LEN];ByteBuffer buf = ByteBuffer.wrap(arr);while (buf.hasRemaining()) {if (fc.read(buf, 0) <= 0) {throw new EOFException("unexpected EOF while reading " + "metadata file header");}}short version = (short) ((arr[0] << 8) | (arr[1] & 0xff));DataChecksum dataChecksum = DataChecksum.newDataChecksum(arr, 2);return new BlockMetadataHeader(version, dataChecksum);}
public static BlockMetadataHeader readHeader(DataInputStream in) {return readHeader(in.readShort(), in);}
public static BlockMetadataHeader readHeader(File file) {DataInputStream in = null;try {in = new DataInputStream(new BufferedInputStream(new FileInputStream(file)));return readHeader(in);} finally {IOUtils.closeStream(in);}}
static BlockMetadataHeader readHeader(RandomAccessFile raf) {byte[] buf = new byte[getHeaderSize()];raf.seek(0);raf.readFully(buf, 0, buf.length);return readHeader(new DataInputStream(new ByteArrayInputStream(buf)));}
private static BlockMetadataHeader readHeader(short version, DataInputStream in) {DataChecksum checksum = DataChecksum.newDataChecksum(in);return new BlockMetadataHeader(version, checksum);}
public static void writeHeader(DataOutputStream out, BlockMetadataHeader header) {out.writeShort(header.getVersion());header.getChecksum().writeHeader(out);}
static void writeHeader(DataOutputStream out, DataChecksum checksum) {writeHeader(out, new BlockMetadataHeader(VERSION, checksum));}
public static int getHeaderSize() {return Short.SIZE / Byte.SIZE + DataChecksum.getChecksumHeaderSize();}private void reopenQueue(Block html) {html.script().$type("text/javascript")._("function reopenQueryNodes() {", "  var currentParam = window.location.href.split('?');", "  var tmpCurrentParam = currentParam;", "  var queryQueuesString = '';", "  if (tmpCurrentParam.length > 1) {", "// openQueues=q1#q2&param1=value1&param2=value2", "tmpCurrentParam = tmpCurrentParam[1];", "if (tmpCurrentParam.indexOf('openQueues=') != -1 ) {", "  tmpCurrentParam = tmpCurrentParam.split('openQueues=')[1].split('&')[0];", "  queryQueuesString = tmpCurrentParam;", "}", "  }", "  if (queryQueuesString != '') {", "queueArray = queryQueuesString.split('#');", "$('#cs .q').each(function() {", "  var name = $(this).html();", "  if (name != 'root' && $.inArray(name, queueArray) != -1) {", "$(this).closest('li').removeClass('jstree-closed').addClass('jstree-open'); ", "  }", "});", "  }", "  $('#cs').bind( {", "  'open_node.jstree' :function(e, data) { storeExpandedQueue(e, data); },", "  'close_node.jstree':function(e, data) { storeExpandedQueue(e, data); }", "  });", "}")._();}
private void storeExpandedQueue(Block html) {html.script().$type("text/javascript")._("function storeExpandedQueue(e, data) {", "  var OPEN_QUEUES = 'openQueues';", "  var ACTION_OPEN = 'open';", "  var ACTION_CLOSED = 'closed';", "  var $li = $(data.args[0]);", "  var action = ACTION_CLOSED;  //closed or open", "  var queueName = ''", "  if ($li.hasClass('jstree-open')) {", "  action=ACTION_OPEN;", "  }", "  queueName = $li.find('.q').html();", "  // http://localhost:8088/cluster/scheduler?openQueues=q1#q2&param1=value1&param2=value2 ", "  //   ==> [http://localhost:8088/cluster/scheduler , openQueues=q1#q2&param1=value1&param2=value2]", "  var currentParam = window.location.href.split('?');", "  var tmpCurrentParam = currentParam;", "  var queryString = '';", "  if (tmpCurrentParam.length > 1) {", "// openQueues=q1#q2&param1=value1&param2=value2", "tmpCurrentParam = tmpCurrentParam[1];", "currentParam = tmpCurrentParam;", "tmpCurrentParam = tmpCurrentParam.split('&');", "var len = tmpCurrentParam.length;", "var paramExist = false;", "if (len > 1) {// Currently no query param are present but in future if any are added for that handling it now", "  queryString = '';", "  for (var i = 0 ; i < len ; i++) {  // searching for param openQueues", "if (tmpCurrentParam[i].substr(0,11) == OPEN_QUEUES + '=') {", "  if (action == ACTION_OPEN) {", "tmpCurrentParam[i] = addQueueName(tmpCurrentParam[i],queueName);", "  }", "  else if (action == ACTION_CLOSED) {", "tmpCurrentParam[i] = removeQueueName(tmpCurrentParam[i] , queueName);", "  }", "  paramExist = true;", "}", "if (i > 0) {", "  queryString += '&';", "}", "queryString += tmpCurrentParam[i];", "  }", "  // If in existing query string OPEN_QUEUES param is not present", "  if (action == ACTION_OPEN && !paramExist) {", "queryString = currentParam + '&' + OPEN_QUEUES + '=' + queueName;", "  }", "} ", "// Only one param is present in current query string", "else {", "  tmpCurrentParam=tmpCurrentParam[0];", "  // checking if the only param present in query string is OPEN_QUEUES or not and making queryString accordingly", "  if (tmpCurrentParam.substr(0,11) == OPEN_QUEUES + '=') {", "if (action == ACTION_OPEN) {", "  queryString = addQueueName(tmpCurrentParam,queueName);", "}", "else if (action == ACTION_CLOSED) {", "  queryString = removeQueueName(tmpCurrentParam , queueName);", "}", "  }", "  else {", "if (action == ACTION_OPEN) {", "  queryString = tmpCurrentParam + '&' + OPEN_QUEUES + '=' + queueName;", "}", "  }", "}", "  } else {", "if (action == ACTION_OPEN) {", "  tmpCurrentParam = '';", "  currentParam = tmpCurrentParam;", "  queryString = OPEN_QUEUES+'='+queueName;", "}", "  }", "  if (queryString != '') {", "queryString = '?' + queryString;", "  }", "  var url = window.location.protocol + '//' + window.location.host + window.location.pathname + queryString;", "  window.history.pushState( { path : url }, '', url);", "};", "", "function removeQueueName(queryString, queueName) {", "  var index = queryString.indexOf(queueName);", "  // Finding if queue is present in query param then only remove it", "  if (index != -1) {", "// removing openQueues=", "var tmp = queryString.substr(11, queryString.length);", "tmp = tmp.split('#');", "var len = tmp.length;", "var newQueryString = '';", "for (var i = 0 ; i < len ; i++) {", "  if (tmp[i] != queueName) {", "if (newQueryString != '') {", "  newQueryString += '#';", "}", "newQueryString += tmp[i];", "  }", "}", "queryString = newQueryString;", "if (newQueryString != '') {", "  queryString = 'openQueues=' + newQueryString;", "}", "  }", "  return queryString;", "}", "", "function addQueueName(queryString, queueName) {", "  queueArray = queryString.split('#');", "  if ($.inArray(queueArray, queueName) == -1) {", "queryString = queryString + '#' + queueName;", "  }", "  return queryString;", "}")._();}
protected void render(Block html) {reopenQueue(html);storeExpandedQueue(html);}public void run() {int failures = 0;LOG.info(reduce + " Thread started: " + getName());try {while (!stopped && !Thread.currentThread().isInterrupted()) {try {int numNewMaps = getMapCompletionEvents();failures = 0;if (numNewMaps > 0) {LOG.info(reduce + ": " + "Got " + numNewMaps + " new map-outputs");}LOG.debug("GetMapEventsThread about to sleep for " + SLEEP_TIME);if (!Thread.currentThread().isInterrupted()) {Thread.sleep(SLEEP_TIME);}} catch (InterruptedException e) {LOG.info("EventFetcher is interrupted.. Returning");return;} catch (IOException ie) {LOG.info("Exception in getting events", ie);if (++failures >= MAX_RETRIES) {throw new IOException("too many failures downloading events", ie);}if (!Thread.currentThread().isInterrupted()) {Thread.sleep(RETRY_PERIOD);}}}} catch (InterruptedException e) {return;} catch (Throwable t) {exceptionReporter.reportException(t);return;}}
public void shutDown() {this.stopped = true;interrupt();try {join(5000);} catch (InterruptedException ie) {LOG.warn("Got interrupted while joining " + getName(), ie);}}
protected int getMapCompletionEvents() {int numNewMaps = 0;TaskCompletionEvent events[] = null;do {MapTaskCompletionEventsUpdate update = umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID) reduce.getJobID(), fromEventIdx, maxEventsToFetch, (org.apache.hadoop.mapred.TaskAttemptID) reduce);events = update.getMapTaskCompletionEvents();LOG.debug("Got " + events.length + " map completion events from " + fromEventIdx);assert !update.shouldReset() : "Unexpected legacy state";// Update the last seen event IDfromEventIdx += events.length;//outputs at all.for (TaskCompletionEvent event : events) {scheduler.resolve(event);if (TaskCompletionEvent.Status.SUCCEEDED == event.getTaskStatus()) {++numNewMaps;}}} while (events.length == maxEventsToFetch);return numNewMaps;}public BlockCollection getBlockCollection() {return bc;}
public void setBlockCollection(BlockCollection bc) {this.bc = bc;}
public DatanodeDescriptor getDatanode(int index) {DatanodeStorageInfo storage = getStorageInfo(index);return storage == null ? null : storage.getDatanodeDescriptor();}
DatanodeStorageInfo getStorageInfo(int index) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 < triplets.length : "Index is out of bound";return (DatanodeStorageInfo) triplets[index * 3];}
private BlockInfo getPrevious(int index) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 + 1 < triplets.length : "Index is out of bound";BlockInfo info = (BlockInfo) triplets[index * 3 + 1];assert info == null || info.getClass().getName().startsWith(BlockInfo.class.getName()) : "BlockInfo is expected at " + index * 3;return info;}
BlockInfo getNext(int index) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 + 2 < triplets.length : "Index is out of bound";BlockInfo info = (BlockInfo) triplets[index * 3 + 2];assert info == null || info.getClass().getName().startsWith(BlockInfo.class.getName()) : "BlockInfo is expected at " + index * 3;return info;}
private void setStorageInfo(int index, DatanodeStorageInfo storage) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 < triplets.length : "Index is out of bound";triplets[index * 3] = storage;}
private BlockInfo setPrevious(int index, BlockInfo to) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 + 1 < triplets.length : "Index is out of bound";BlockInfo info = (BlockInfo) triplets[index * 3 + 1];triplets[index * 3 + 1] = to;return info;}
private BlockInfo setNext(int index, BlockInfo to) {assert this.triplets != null : "BlockInfo is not initialized";assert index >= 0 && index * 3 + 2 < triplets.length : "Index is out of bound";BlockInfo info = (BlockInfo) triplets[index * 3 + 2];triplets[index * 3 + 2] = to;return info;}
public int getCapacity() {assert this.triplets != null : "BlockInfo is not initialized";assert triplets.length % 3 == 0 : "Malformed BlockInfo";return triplets.length / 3;}
private int ensureCapacity(int num) {assert this.triplets != null : "BlockInfo is not initialized";int last = numNodes();if (triplets.length >= (last + num) * 3)return last;/* Not enough space left. Create a new array. Should normally  * happen only when replication is manually increased by the user. */Object[] old = triplets;triplets = new Object[(last + num) * 3];System.arraycopy(old, 0, triplets, 0, last * 3);return last;}
public int numNodes() {assert this.triplets != null : "BlockInfo is not initialized";assert triplets.length % 3 == 0 : "Malformed BlockInfo";for (int idx = getCapacity() - 1; idx >= 0; idx--) {if (getDatanode(idx) != null)return idx + 1;}return 0;}
boolean addStorage(DatanodeStorageInfo storage) {// find the last null nodeint lastNode = ensureCapacity(1);setStorageInfo(lastNode, storage);setNext(lastNode, null);setPrevious(lastNode, null);return true;}
boolean removeStorage(DatanodeStorageInfo storage) {int dnIndex = findStorageInfo(storage);if (// the node is not founddnIndex < 0)return false;assert getPrevious(dnIndex) == null && getNext(dnIndex) == null : "Block is still in the list and must be removed first.";// find the last not null nodeint lastNode = numNodes() - 1;// replace current node triplet by the lastNode one setStorageInfo(dnIndex, getStorageInfo(lastNode));setNext(dnIndex, getNext(lastNode));setPrevious(dnIndex, getPrevious(lastNode));// set the last triplet to nullsetStorageInfo(lastNode, null);setNext(lastNode, null);setPrevious(lastNode, null);return true;}
boolean findDatanode(DatanodeDescriptor dn) {int len = getCapacity();for (int idx = 0; idx < len; idx++) {DatanodeDescriptor cur = getDatanode(idx);if (cur == dn) {return true;}if (cur == null) {break;}}return false;}
DatanodeStorageInfo findStorageInfo(DatanodeDescriptor dn) {int len = getCapacity();for (int idx = 0; idx < len; idx++) {DatanodeStorageInfo cur = getStorageInfo(idx);if (cur == null)break;if (cur.getDatanodeDescriptor() == dn)return cur;}return null;}
int findStorageInfo(DatanodeStorageInfo storageInfo) {int len = getCapacity();for (int idx = 0; idx < len; idx++) {DatanodeStorageInfo cur = getStorageInfo(idx);if (cur == storageInfo)return idx;if (cur == null)break;}return -1;}
BlockInfo listInsert(BlockInfo head, DatanodeStorageInfo storage) {int dnIndex = this.findStorageInfo(storage);assert dnIndex >= 0 : "Data node is not found: current";assert getPrevious(dnIndex) == null && getNext(dnIndex) == null : "Block is already in the list and cannot be inserted.";this.setPrevious(dnIndex, null);this.setNext(dnIndex, head);if (head != null)head.setPrevious(head.findStorageInfo(storage), this);return this;}
BlockInfo listRemove(BlockInfo head, DatanodeStorageInfo storage) {if (head == null)return null;int dnIndex = this.findStorageInfo(storage);if (// this block is not on the data-node listdnIndex < 0)return head;BlockInfo next = this.getNext(dnIndex);BlockInfo prev = this.getPrevious(dnIndex);this.setNext(dnIndex, null);this.setPrevious(dnIndex, null);if (prev != null)prev.setNext(prev.findStorageInfo(storage), next);if (next != null)next.setPrevious(next.findStorageInfo(storage), prev);if (// removing the headthis == head)head = next;return head;}
public BlockInfo moveBlockToHead(BlockInfo head, DatanodeStorageInfo storage, int curIndex, int headIndex) {if (head == this) {return this;}BlockInfo next = this.setNext(curIndex, head);BlockInfo prev = this.setPrevious(curIndex, null);head.setPrevious(headIndex, this);prev.setNext(prev.findStorageInfo(storage), next);if (next != null)next.setPrevious(next.findStorageInfo(storage), prev);return this;}
public BlockUCState getBlockUCState() {return BlockUCState.COMPLETE;}
public boolean isComplete() {return getBlockUCState().equals(BlockUCState.COMPLETE);}
public BlockInfoUnderConstruction convertToBlockUnderConstruction(BlockUCState s, DatanodeStorageInfo[] targets) {if (isComplete()) {return new BlockInfoUnderConstruction(this, getBlockCollection().getBlockReplication(), s, targets);}// the block is already under constructionBlockInfoUnderConstruction ucBlock = (BlockInfoUnderConstruction) this;ucBlock.setBlockUCState(s);ucBlock.setExpectedLocations(targets);return ucBlock;}
public int hashCode() {// Super implementation is sufficientreturn super.hashCode();}
public boolean equals(Object obj) {// Sufficient to rely on super's implementationreturn (this == obj) || super.equals(obj);}
public LightWeightGSet.LinkedElement getNext() {return nextLinkedElement;}
public void setNext(LightWeightGSet.LinkedElement next) {this.nextLinkedElement = next;}private HdfsConfiguration getHAConf() {HdfsConfiguration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_NAMESERVICES, NSID);conf.set(DFSConfigKeys.DFS_NAMESERVICE_ID, NSID);conf.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX, NSID), "nn1,nn2");conf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY, "nn1");conf.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY, NSID, "nn1"), HOST_A + ":12345");conf.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY, NSID, "nn2"), HOST_B + ":12345");return conf;}
public static String getFencerTrueCommand() {return Shell.WINDOWS ? FENCER_TRUE_COMMAND_WINDOWS : FENCER_TRUE_COMMAND_UNIX;}
public static String getFencerFalseCommand() {return Shell.WINDOWS ? FENCER_FALSE_COMMAND_WINDOWS : FENCER_FALSE_COMMAND_UNIX;}
public void setup() {mockProtocol = MockitoUtil.mockProtocol(HAServiceProtocol.class);mockZkfcProtocol = MockitoUtil.mockProtocol(ZKFCProtocol.class);tool = new DFSHAAdmin() {
@Overrideprotected HAServiceTarget resolveTarget(String nnId) {HAServiceTarget target = super.resolveTarget(nnId);HAServiceTarget spy = Mockito.spy(target);// OVerride the target to return our mock protocoltry {Mockito.doReturn(mockProtocol).when(spy).getProxy(Mockito.<Configuration>any(), Mockito.anyInt());Mockito.doReturn(mockZkfcProtocol).when(spy).getZKFCProxy(Mockito.<Configuration>any(), Mockito.anyInt());} catch (IOException e) {throw new AssertionError(e);}return spy;}};tool.setConf(getHAConf());tool.setErrOut(new PrintStream(errOutBytes));tool.setOut(new PrintStream(outBytes));}
protected HAServiceTarget resolveTarget(String nnId) {HAServiceTarget target = super.resolveTarget(nnId);HAServiceTarget spy = Mockito.spy(target);// OVerride the target to return our mock protocoltry {Mockito.doReturn(mockProtocol).when(spy).getProxy(Mockito.<Configuration>any(), Mockito.anyInt());Mockito.doReturn(mockZkfcProtocol).when(spy).getZKFCProxy(Mockito.<Configuration>any(), Mockito.anyInt());} catch (IOException e) {throw new AssertionError(e);}return spy;}
private void assertOutputContains(String string) {if (!errOutput.contains(string) && !output.contains(string)) {fail("Expected output to contain '" + string + "' but err_output was:\n" + errOutput + "\n and output was: \n" + output);}}
public void testNameserviceOption() {assertEquals(-1, runTool("-ns"));assertOutputContains("Missing nameservice ID");assertEquals(-1, runTool("-ns", "ns1"));assertOutputContains("Missing command");// "ns1" isn't defined but we check this lazily and help doesn't use the nsassertEquals(0, runTool("-ns", "ns1", "-help", "transitionToActive"));assertOutputContains("Transitions the service into Active");}
public void testNamenodeResolution() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();assertEquals(0, runTool("-getServiceState", "nn1"));Mockito.verify(mockProtocol).getServiceStatus();assertEquals(-1, runTool("-getServiceState", "undefined"));assertOutputContains("Unable to determine service address for namenode 'undefined'");}
public void testHelp() {assertEquals(0, runTool("-help"));assertEquals(0, runTool("-help", "transitionToActive"));assertOutputContains("Transitions the service into Active");}
public void testTransitionToActive() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();assertEquals(0, runTool("-transitionToActive", "nn1"));Mockito.verify(mockProtocol).transitionToActive(reqInfoCaptor.capture());assertEquals(RequestSource.REQUEST_BY_USER, reqInfoCaptor.getValue().getSource());}
public void testMutativeOperationsWithAutoHaEnabled() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();// Turn on auto-HA in the configHdfsConfiguration conf = getHAConf();conf.setBoolean(DFSConfigKeys.DFS_HA_AUTO_FAILOVER_ENABLED_KEY, true);conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);// Should fail without the forcemanual flagassertEquals(-1, runTool("-transitionToActive", "nn1"));assertTrue(errOutput.contains("Refusing to manually manage"));assertEquals(-1, runTool("-transitionToStandby", "nn1"));assertTrue(errOutput.contains("Refusing to manually manage"));Mockito.verify(mockProtocol, Mockito.never()).transitionToActive(anyReqInfo());Mockito.verify(mockProtocol, Mockito.never()).transitionToStandby(anyReqInfo());// for the RPCsetupConfirmationOnSystemIn();assertEquals(0, runTool("-transitionToActive", "-forcemanual", "nn1"));setupConfirmationOnSystemIn();assertEquals(0, runTool("-transitionToStandby", "-forcemanual", "nn1"));Mockito.verify(mockProtocol, Mockito.times(1)).transitionToActive(reqInfoCaptor.capture());Mockito.verify(mockProtocol, Mockito.times(1)).transitionToStandby(reqInfoCaptor.capture());// All of the RPCs should have had the "force" sourcefor (StateChangeRequestInfo ri : reqInfoCaptor.getAllValues()) {assertEquals(RequestSource.REQUEST_BY_USER_FORCED, ri.getSource());}}
private static void setupConfirmationOnSystemIn() {// Answer "yes" to the prompt about transition to activeSystem.setIn(new ByteArrayInputStream("yes\n".getBytes()));}
public void testMonitoringOperationsWithAutoHaEnabled() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();// Turn on auto-HAHdfsConfiguration conf = getHAConf();conf.setBoolean(DFSConfigKeys.DFS_HA_AUTO_FAILOVER_ENABLED_KEY, true);tool.setConf(conf);assertEquals(0, runTool("-checkHealth", "nn1"));Mockito.verify(mockProtocol).monitorHealth();assertEquals(0, runTool("-getServiceState", "nn1"));Mockito.verify(mockProtocol).getServiceStatus();}
public void testTransitionToStandby() {assertEquals(0, runTool("-transitionToStandby", "nn1"));Mockito.verify(mockProtocol).transitionToStandby(anyReqInfo());}
public void testFailoverWithNoFencerConfigured() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();assertEquals(-1, runTool("-failover", "nn1", "nn2"));}
public void testFailoverWithFencerConfigured() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2"));}
public void testFailoverWithFencerAndNameservice() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-ns", "ns1", "-failover", "nn1", "nn2"));}
public void testFailoverWithFencerConfiguredAndForce() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2", "--forcefence"));}
public void testFailoverWithForceActive() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2", "--forceactive"));}
public void testFailoverWithInvalidFenceArg() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(-1, runTool("-failover", "nn1", "nn2", "notforcefence"));}
public void testFailoverWithFenceButNoFencer() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();assertEquals(-1, runTool("-failover", "nn1", "nn2", "--forcefence"));}
public void testFailoverWithFenceAndBadFencer() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, "foobar!");tool.setConf(conf);assertEquals(-1, runTool("-failover", "nn1", "nn2", "--forcefence"));}
public void testFailoverWithAutoHa() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();// Turn on auto-HA in the configHdfsConfiguration conf = getHAConf();conf.setBoolean(DFSConfigKeys.DFS_HA_AUTO_FAILOVER_ENABLED_KEY, true);conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2"));Mockito.verify(mockZkfcProtocol).gracefulFailover();}
public void testForceFenceOptionListedBeforeArgs() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();HdfsConfiguration conf = getHAConf();conf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "--forcefence", "nn1", "nn2"));}
public void testGetServiceStatus() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();assertEquals(0, runTool("-getServiceState", "nn1"));Mockito.verify(mockProtocol).getServiceStatus();}
public void testCheckHealth() {assertEquals(0, runTool("-checkHealth", "nn1"));Mockito.verify(mockProtocol).monitorHealth();Mockito.doThrow(new HealthCheckFailedException("fake health check failure")).when(mockProtocol).monitorHealth();assertEquals(-1, runTool("-checkHealth", "nn1"));assertOutputContains("Health check failed: fake health check failure");}
public void testFencingConfigPerNameNode() {Mockito.doReturn(STANDBY_READY_RESULT).when(mockProtocol).getServiceStatus();final String nsSpecificKey = DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY + "." + NSID;final String nnSpecificKey = nsSpecificKey + ".nn1";HdfsConfiguration conf = getHAConf();// Set the default fencer to succeedconf.set(DFSConfigKeys.DFS_HA_FENCE_METHODS_KEY, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2", "--forcefence"));// Set the NN-specific fencer to fail. Should fail to fence.conf.set(nnSpecificKey, getFencerFalseCommand());tool.setConf(conf);assertEquals(-1, runTool("-failover", "nn1", "nn2", "--forcefence"));conf.unset(nnSpecificKey);// Set an NS-specific fencer to fail. Should fail.conf.set(nsSpecificKey, getFencerFalseCommand());tool.setConf(conf);assertEquals(-1, runTool("-failover", "nn1", "nn2", "--forcefence"));// Set the NS-specific fencer to succeed. Should succeedconf.set(nsSpecificKey, getFencerTrueCommand());tool.setConf(conf);assertEquals(0, runTool("-failover", "nn1", "nn2", "--forcefence"));}
private Object runTool(String... args) {errOutBytes.reset();outBytes.reset();LOG.info("Running: DFSHAAdmin " + Joiner.on(" ").join(args));int ret = tool.run(args);errOutput = new String(errOutBytes.toByteArray(), Charsets.UTF_8);output = new String(outBytes.toByteArray(), Charsets.UTF_8);LOG.info("Err_output:\n" + errOutput + "\nOutput:\n" + output);return ret;}
private StateChangeRequestInfo anyReqInfo() {return Mockito.any();}public long getValue() {return (~crc) & 0xffffffffL;}
public void reset() {crc = 0xffffffff;}
public void update(final byte[] b, final int offset, final int len) {int localCrc = crc;final int remainder = len & 0x7;int i = offset;for (final int end = offset + len - remainder; i < end; i += 8) {final int x = localCrc ^ ((((b[i] << 24) >>> 24) + ((b[i + 1] << 24) >>> 16)) + (((b[i + 2] << 24) >>> 8) + (b[i + 3] << 24)));localCrc = ((T[((x << 24) >>> 24) + 0x700] ^ T[((x << 16) >>> 24) + 0x600]) ^ (T[((x << 8) >>> 24) + 0x500] ^ T[(x >>> 24) + 0x400])) ^ ((T[((b[i + 4] << 24) >>> 24) + 0x300] ^ T[((b[i + 5] << 24) >>> 24) + 0x200]) ^ (T[((b[i + 6] << 24) >>> 24) + 0x100] ^ T[((b[i + 7] << 24) >>> 24)]));}/* loop unroll - duff's device style */switch(remainder) {case 7:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 6:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 5:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 4:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 3:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 2:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];case 1:localCrc = (localCrc >>> 8) ^ T[((localCrc ^ b[i++]) << 24) >>> 24];default:}// Publish crc out to objectcrc = localCrc;}
public final void update(int b) {crc = (crc >>> 8) ^ T[(((crc ^ b) << 24) >>> 24)];}public void constructors() {Configuration conf = new Configuration(false);assertEquals(conf.size(), 0);byte[] bytes = "<configuration><property><name>a</name><value>A</value></property></configuration>".getBytes();InputStream is = new ByteArrayInputStream(bytes);conf = new Configuration(false);ConfigurationUtils.load(conf, is);assertEquals(conf.size(), 1);assertEquals(conf.get("a"), "A");}
public void constructorsFail3() {InputStream is = new ByteArrayInputStream("<xonfiguration></xonfiguration>".getBytes());Configuration conf = new Configuration(false);ConfigurationUtils.load(conf, is);}
public void copy() {Configuration srcConf = new Configuration(false);Configuration targetConf = new Configuration(false);srcConf.set("testParameter1", "valueFromSource");srcConf.set("testParameter2", "valueFromSource");targetConf.set("testParameter2", "valueFromTarget");targetConf.set("testParameter3", "valueFromTarget");ConfigurationUtils.copy(srcConf, targetConf);assertEquals("valueFromSource", targetConf.get("testParameter1"));assertEquals("valueFromSource", targetConf.get("testParameter2"));assertEquals("valueFromTarget", targetConf.get("testParameter3"));}
public void injectDefaults() {Configuration srcConf = new Configuration(false);Configuration targetConf = new Configuration(false);srcConf.set("testParameter1", "valueFromSource");srcConf.set("testParameter2", "valueFromSource");targetConf.set("testParameter2", "originalValueFromTarget");targetConf.set("testParameter3", "originalValueFromTarget");ConfigurationUtils.injectDefaults(srcConf, targetConf);assertEquals("valueFromSource", targetConf.get("testParameter1"));assertEquals("originalValueFromTarget", targetConf.get("testParameter2"));assertEquals("originalValueFromTarget", targetConf.get("testParameter3"));assertEquals("valueFromSource", srcConf.get("testParameter1"));assertEquals("valueFromSource", srcConf.get("testParameter2"));assertNull(srcConf.get("testParameter3"));}
public void resolve() {Configuration conf = new Configuration(false);conf.set("a", "A");conf.set("b", "${a}");assertEquals(conf.getRaw("a"), "A");assertEquals(conf.getRaw("b"), "${a}");conf = ConfigurationUtils.resolve(conf);assertEquals(conf.getRaw("a"), "A");assertEquals(conf.getRaw("b"), "A");}
public void testVarResolutionAndSysProps() {String userName = System.getProperty("user.name");Configuration conf = new Configuration(false);conf.set("a", "A");conf.set("b", "${a}");conf.set("c", "${user.name}");conf.set("d", "${aaa}");assertEquals(conf.getRaw("a"), "A");assertEquals(conf.getRaw("b"), "${a}");assertEquals(conf.getRaw("c"), "${user.name}");assertEquals(conf.get("a"), "A");assertEquals(conf.get("b"), "A");assertEquals(conf.get("c"), userName);assertEquals(conf.get("d"), "${aaa}");conf.set("user.name", "foo");assertEquals(conf.get("user.name"), "foo");}private static String getOSLoginModuleName() {if (IBM_JAVA) {if (windows) {return is64Bit ? "com.ibm.security.auth.module.Win64LoginModule" : "com.ibm.security.auth.module.NTLoginModule";} else if (aix) {return is64Bit ? "com.ibm.security.auth.module.AIX64LoginModule" : "com.ibm.security.auth.module.AIXLoginModule";} else {return "com.ibm.security.auth.module.LinuxLoginModule";}} else {return windows ? "com.sun.security.auth.module.NTLoginModule" : "com.sun.security.auth.module.UnixLoginModule";}}
public AppConfigurationEntry[] getAppConfigurationEntry(String appName) {return USER_KERBEROS_CONF;}
public void setConnectionConfigurator(ConnectionConfigurator configurator) {connConfigurator = configurator;}
public void authenticate(URL url, AuthenticatedURL.Token token) {if (!token.isSet()) {this.url = url;base64 = new Base64(0);conn = (HttpURLConnection) url.openConnection();if (connConfigurator != null) {conn = connConfigurator.configure(conn);}conn.setRequestMethod(AUTH_HTTP_METHOD);conn.connect();if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) {LOG.debug("JDK performed authentication on our behalf.");// us, just pull out the token.AuthenticatedURL.extractToken(conn, token);return;} else if (isNegotiate()) {LOG.debug("Performing our own SPNEGO sequence.");doSpnegoSequence(token);} else {LOG.debug("Using fallback authenticator sequence.");Authenticator auth = getFallBackAuthenticator();// to make the connection (e.g., SSL certificates)auth.setConnectionConfigurator(connConfigurator);auth.authenticate(url, token);}}}
protected Authenticator getFallBackAuthenticator() {Authenticator auth = new PseudoAuthenticator();if (connConfigurator != null) {auth.setConnectionConfigurator(connConfigurator);}return auth;}
private boolean isNegotiate() {boolean negotiate = false;if (conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {String authHeader = conn.getHeaderField(WWW_AUTHENTICATE);negotiate = authHeader != null && authHeader.trim().startsWith(NEGOTIATE);}return negotiate;}
private void doSpnegoSequence(AuthenticatedURL.Token token) {try {AccessControlContext context = AccessController.getContext();Subject subject = Subject.getSubject(context);if (subject == null) {LOG.debug("No subject in context, logging in");subject = new Subject();LoginContext login = new LoginContext("", subject, null, new KerberosConfiguration());login.login();}if (LOG.isDebugEnabled()) {LOG.debug("Using subject: " + subject);}Subject.doAs(subject, new PrivilegedExceptionAction<Void>() {
@Overridepublic Void run() throws Exception {GSSContext gssContext = null;try {GSSManager gssManager = GSSManager.getInstance();String servicePrincipal = KerberosUtil.getServicePrincipal("HTTP", KerberosAuthenticator.this.url.getHost());Oid oid = KerberosUtil.getOidInstance("NT_GSS_KRB5_PRINCIPAL");GSSName serviceName = gssManager.createName(servicePrincipal, oid);oid = KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");gssContext = gssManager.createContext(serviceName, oid, null, GSSContext.DEFAULT_LIFETIME);gssContext.requestCredDeleg(true);gssContext.requestMutualAuth(true);byte[] inToken = new byte[0];byte[] outToken;boolean established = false;// Loop while the context is still not establishedwhile (!established) {outToken = gssContext.initSecContext(inToken, 0, inToken.length);if (outToken != null) {sendToken(outToken);}if (!gssContext.isEstablished()) {inToken = readToken();} else {established = true;}}} finally {if (gssContext != null) {gssContext.dispose();gssContext = null;}}return null;}});} catch (PrivilegedActionException ex) {throw new AuthenticationException(ex.getException());} catch (LoginException ex) {throw new AuthenticationException(ex);}AuthenticatedURL.extractToken(conn, token);}
public Void run() {GSSContext gssContext = null;try {GSSManager gssManager = GSSManager.getInstance();String servicePrincipal = KerberosUtil.getServicePrincipal("HTTP", KerberosAuthenticator.this.url.getHost());Oid oid = KerberosUtil.getOidInstance("NT_GSS_KRB5_PRINCIPAL");GSSName serviceName = gssManager.createName(servicePrincipal, oid);oid = KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");gssContext = gssManager.createContext(serviceName, oid, null, GSSContext.DEFAULT_LIFETIME);gssContext.requestCredDeleg(true);gssContext.requestMutualAuth(true);byte[] inToken = new byte[0];byte[] outToken;boolean established = false;// Loop while the context is still not establishedwhile (!established) {outToken = gssContext.initSecContext(inToken, 0, inToken.length);if (outToken != null) {sendToken(outToken);}if (!gssContext.isEstablished()) {inToken = readToken();} else {established = true;}}} finally {if (gssContext != null) {gssContext.dispose();gssContext = null;}}return null;}
private void sendToken(byte[] outToken) {String token = base64.encodeToString(outToken);conn = (HttpURLConnection) url.openConnection();if (connConfigurator != null) {conn = connConfigurator.configure(conn);}conn.setRequestMethod(AUTH_HTTP_METHOD);conn.setRequestProperty(AUTHORIZATION, NEGOTIATE + " " + token);conn.connect();}
private byte[] readToken() {int status = conn.getResponseCode();if (status == HttpURLConnection.HTTP_OK || status == HttpURLConnection.HTTP_UNAUTHORIZED) {String authHeader = conn.getHeaderField(WWW_AUTHENTICATE);if (authHeader == null || !authHeader.trim().startsWith(NEGOTIATE)) {throw new AuthenticationException("Invalid SPNEGO sequence, '" + WWW_AUTHENTICATE + "' header incorrect: " + authHeader);}String negotiation = authHeader.trim().substring((NEGOTIATE + " ").length()).trim();return base64.decode(negotiation);}throw new AuthenticationException("Invalid SPNEGO sequence, status code: " + status);}private static String getTestString(int len) {StringBuilder buffer = new StringBuilder();int length = (len == RAND_LEN) ? RANDOM.nextInt(1000) : len;while (buffer.length() < length) {int codePoint = RANDOM.nextInt(Character.MAX_CODE_POINT);char tmpStr[] = new char[2];if (Character.isDefined(codePoint)) {//unpaired surrogateif (codePoint < Character.MIN_SUPPLEMENTARY_CODE_POINT && !Character.isHighSurrogate((char) codePoint) && !Character.isLowSurrogate((char) codePoint)) {Character.toChars(codePoint, tmpStr, 0);buffer.append(tmpStr);}}}return buffer.toString();}
public static String getTestString() {return getTestString(RAND_LEN);}
public static String getLongString() {String str = getTestString();int length = Short.MAX_VALUE + str.length();StringBuilder buffer = new StringBuilder();while (buffer.length() < length) buffer.append(str);return buffer.toString();}
public void testWritable() {for (int i = 0; i < NUM_ITERATIONS; i++) {String str;if (i == 0)str = getLongString();elsestr = getTestString();TestWritable.testWritable(new Text(str));}}
public void testCoding() {String before = "Bad \t encoding \t testcase";Text text = new Text(before);String after = text.toString();assertTrue(before.equals(after));for (int i = 0; i < NUM_ITERATIONS; i++) {if (i == 0)before = getLongString();elsebefore = getTestString();ByteBuffer bb = Text.encode(before);byte[] utf8Text = bb.array();byte[] utf8Java = before.getBytes("UTF-8");assertEquals(0, WritableComparator.compareBytes(utf8Text, 0, bb.limit(), utf8Java, 0, utf8Java.length));// test utf8 to stringafter = Text.decode(utf8Java);assertTrue(before.equals(after));}}
public void testIO() {DataOutputBuffer out = new DataOutputBuffer();DataInputBuffer in = new DataInputBuffer();for (int i = 0; i < NUM_ITERATIONS; i++) {// generate a random stringString before;if (i == 0)before = getLongString();elsebefore = getTestString();// write itout.reset();Text.writeString(out, before);// test that it reads correctlyin.reset(out.getData(), out.getLength());String after = Text.readString(in);assertTrue(before.equals(after));// Test compatibility with Java's other decoder int strLenSize = WritableUtils.getVIntSize(Text.utf8Length(before));String after2 = new String(out.getData(), strLenSize, out.getLength() - strLenSize, "UTF-8");assertTrue(before.equals(after2));}}
public void doTestLimitedIO(String str, int len) {DataOutputBuffer out = new DataOutputBuffer();DataInputBuffer in = new DataInputBuffer();out.reset();try {Text.writeString(out, str, len);fail("expected writeString to fail when told to write a string " + "that was too long!  The string was '" + str + "'");} catch (IOException e) {}Text.writeString(out, str, len + 1);// test that it reads correctlyin.reset(out.getData(), out.getLength());in.mark(len);String after;try {after = Text.readString(in, len);fail("expected readString to fail when told to read a string " + "that was too long!  The string was '" + str + "'");} catch (IOException e) {}in.reset();after = Text.readString(in, len + 1);assertTrue(str.equals(after));}
public void testLimitedIO() {doTestLimitedIO("abcd", 3);doTestLimitedIO("foo bar baz", 10);doTestLimitedIO("1", 0);}
public void testCompare() {DataOutputBuffer out1 = new DataOutputBuffer();DataOutputBuffer out2 = new DataOutputBuffer();DataOutputBuffer out3 = new DataOutputBuffer();Text.Comparator comparator = new Text.Comparator();for (int i = 0; i < NUM_ITERATIONS; i++) {// reset output bufferout1.reset();out2.reset();out3.reset();// generate two random stringsString str1 = getTestString();String str2 = getTestString();if (i == 0) {str1 = getLongString();str2 = getLongString();} else {str1 = getTestString();str2 = getTestString();}// convert to textsText txt1 = new Text(str1);Text txt2 = new Text(str2);Text txt3 = new Text(str1);// serialize themtxt1.write(out1);txt2.write(out2);txt3.write(out3);// compare two strings by looking at their binary formatsint ret1 = comparator.compare(out1.getData(), 0, out1.getLength(), out2.getData(), 0, out2.getLength());// compare two stringsint ret2 = txt1.compareTo(txt2);assertEquals(ret1, ret2);assertEquals("Equivalence of different txt objects, same content", 0, txt1.compareTo(txt3));assertEquals("Equvalence of data output buffers", 0, comparator.compare(out1.getData(), 0, out3.getLength(), out3.getData(), 0, out3.getLength()));}}
public void testFind() {Text text = new Text("abcd€bdcd€");assertTrue(text.find("abd") == -1);assertTrue(text.find("ac") == -1);assertTrue(text.find("€") == 4);assertTrue(text.find("€", 5) == 11);}
public void testFindAfterUpdatingContents() {Text text = new Text("abcd");text.set("a".getBytes());assertEquals(text.getLength(), 1);assertEquals(text.find("a"), 0);assertEquals(text.find("b"), -1);}
public void testValidate() {Text text = new Text("abcd€bdcd€");byte[] utf8 = text.getBytes();int length = text.getLength();Text.validateUTF8(utf8, 0, length);}
public void testClear() {// Test lengths on an empty text objectText text = new Text();assertEquals("Actual string on an empty text object must be an empty string", "", text.toString());assertEquals("Underlying byte array length must be zero", 0, text.getBytes().length);assertEquals("String's length must be zero", 0, text.getLength());// Test if clear works as intendedtext = new Text("abcd€bdcd€");int len = text.getLength();text.clear();assertEquals("String must be empty after clear()", "", text.toString());assertTrue("Length of the byte array must not decrease after clear()", text.getBytes().length >= len);assertEquals("Length of the string must be reset to 0 after clear()", 0, text.getLength());}
public void testTextText() {Text a = new Text("abc");Text b = new Text("a");b.set(a);assertEquals("abc", b.toString());a.append("xdefgxxx".getBytes(), 1, 4);assertEquals("modified aliased string", "abc", b.toString());assertEquals("appended string incorrectly", "abcdefg", a.toString());// add an extra byte so that capacity = 14 and length = 8a.append(new byte[] { 'd' }, 0, 1);assertEquals(14, a.getBytes().length);assertEquals(8, a.copyBytes().length);}
public void run() {final String name = this.getName();DataOutputBuffer out = new DataOutputBuffer();DataInputBuffer in = new DataInputBuffer();for (int i = 0; i < 1000; ++i) {try {out.reset();WritableUtils.writeString(out, name);in.reset(out.getData(), out.getLength());String s = WritableUtils.readString(in);assertEquals("input buffer reset contents = " + name, name, s);} catch (Exception ioe) {throw new RuntimeException(ioe);}}}
public void testConcurrentEncodeDecode() {Thread thread1 = new ConcurrentEncodeDecodeThread("apache");Thread thread2 = new ConcurrentEncodeDecodeThread("hadoop");thread1.start();thread2.start();thread2.join();thread2.join();}
public void testAvroReflect() {AvroTestUtil.testReflect(new Text("foo"), "{\"type\":\"string\",\"java-class\":\"org.apache.hadoop.io.Text\"}");}
public void testCharAt() {String line = "adsawseeeeegqewgasddga";Text text = new Text(line);for (int i = 0; i < line.length(); i++) {assertTrue("testCharAt error1 !!!", text.charAt(i) == line.charAt(i));}assertEquals("testCharAt error2 !!!", -1, text.charAt(-1));assertEquals("testCharAt error3 !!!", -1, text.charAt(100));}
public void testReadWriteOperations() {String line = "adsawseeeeegqewgasddga";byte[] inputBytes = line.getBytes();inputBytes = Bytes.concat(new byte[] { (byte) 22 }, inputBytes);DataInputBuffer in = new DataInputBuffer();DataOutputBuffer out = new DataOutputBuffer();Text text = new Text(line);try {in.reset(inputBytes, inputBytes.length);text.readFields(in);} catch (Exception ex) {fail("testReadFields error !!!");}try {text.write(out);} catch (IOException ex) {} catch (Exception ex) {fail("testReadWriteOperations error !!!");}}
public void testReadWithKnownLength() {String line = "hello world";byte[] inputBytes = line.getBytes(Charsets.UTF_8);DataInputBuffer in = new DataInputBuffer();Text text = new Text();in.reset(inputBytes, inputBytes.length);text.readWithKnownLength(in, 5);assertEquals("hello", text.toString());// Read longer length, make sure it lengthensin.reset(inputBytes, inputBytes.length);text.readWithKnownLength(in, 7);assertEquals("hello w", text.toString());// Read shorter length, make sure it shortensin.reset(inputBytes, inputBytes.length);text.readWithKnownLength(in, 2);assertEquals("he", text.toString());}
public void testBytesToCodePoint() {try {ByteBuffer bytes = ByteBuffer.wrap(new byte[] { -2, 45, 23, 12, 76, 89 });Text.bytesToCodePoint(bytes);assertTrue("testBytesToCodePoint error !!!", bytes.position() == 6);} catch (BufferUnderflowException ex) {fail("testBytesToCodePoint unexp exception");} catch (Exception e) {fail("testBytesToCodePoint unexp exception");}}
public void testbytesToCodePointWithInvalidUTF() {try {Text.bytesToCodePoint(ByteBuffer.wrap(new byte[] { -2 }));fail("testbytesToCodePointWithInvalidUTF error unexp exception !!!");} catch (BufferUnderflowException ex) {} catch (Exception e) {fail("testbytesToCodePointWithInvalidUTF error unexp exception !!!");}}
public void testUtf8Length() {assertEquals("testUtf8Length1 error   !!!", 1, Text.utf8Length(new String(new char[] { (char) 1 })));assertEquals("testUtf8Length127 error !!!", 1, Text.utf8Length(new String(new char[] { (char) 127 })));assertEquals("testUtf8Length128 error !!!", 2, Text.utf8Length(new String(new char[] { (char) 128 })));assertEquals("testUtf8Length193 error !!!", 2, Text.utf8Length(new String(new char[] { (char) 193 })));assertEquals("testUtf8Length225 error !!!", 2, Text.utf8Length(new String(new char[] { (char) 225 })));assertEquals("testUtf8Length254 error !!!", 2, Text.utf8Length(new String(new char[] { (char) 254 })));}
public static void main(String[] args) {TestText test = new TestText("main");test.testIO();test.testCompare();test.testCoding();test.testWritable();test.testFind();test.testValidate();}public static Collection params() {return Arrays.asList(new Object[][] { { 1, simpleConf }, { 2, kerberosConf } });}
public static void setUp() {try {testMiniKDC = new MiniKdc(MiniKdc.createConf(), testRootDir);setupKDC();} catch (Exception e) {assertTrue("Couldn't create MiniKDC", false);}}
public static void tearDown() {if (testMiniKDC != null) {testMiniKDC.stop();}}
private static void setupKDC() {if (!miniKDCStarted) {testMiniKDC.start();getKdc().createPrincipal(httpSpnegoKeytabFile, "HTTP/localhost", "client", UserGroupInformation.getLoginUser().getShortUserName());miniKDCStarted = true;}}
private static MiniKdc getKdc() {return testMiniKDC;}
private static void setupAndStartRM(Configuration conf) {UserGroupInformation.setConfiguration(conf);rm = new MockRM(conf);}
public void testSimpleAuth() {rm.start();// this should work for secure and non-secure clustersURL url = new URL("http://localhost:8088/cluster");HttpURLConnection conn = (HttpURLConnection) url.openConnection();try {conn.getInputStream();assertEquals(Status.OK.getStatusCode(), conn.getResponseCode());} catch (Exception e) {fail("Fetching url failed");}if (UserGroupInformation.isSecurityEnabled()) {testAnonymousKerberosUser();} else {testAnonymousSimpleUser();}rm.stop();}
private void testAnonymousKerberosUser() {ApplicationSubmissionContextInfo app = new ApplicationSubmissionContextInfo();String appid = "application_123_0";app.setApplicationId(appid);String requestBody = TestRMWebServicesDelegationTokenAuthentication.getMarshalledAppInfo(app);URL url = new URL("http://localhost:8088/ws/v1/cluster/apps/new-application");HttpURLConnection conn = (HttpURLConnection) url.openConnection();TestRMWebServicesDelegationTokenAuthentication.setupConn(conn, "POST", "application/xml", requestBody);try {conn.getInputStream();fail("Anonymous users should not be allowed to get new application ids in secure mode.");} catch (IOException ie) {assertEquals(Status.FORBIDDEN.getStatusCode(), conn.getResponseCode());}url = new URL("http://localhost:8088/ws/v1/cluster/apps");conn = (HttpURLConnection) url.openConnection();TestRMWebServicesDelegationTokenAuthentication.setupConn(conn, "POST", "application/xml", requestBody);try {conn.getInputStream();fail("Anonymous users should not be allowed to submit apps in secure mode.");} catch (IOException ie) {assertEquals(Status.FORBIDDEN.getStatusCode(), conn.getResponseCode());}requestBody = "{ \"state\": \"KILLED\"}";url = new URL("http://localhost:8088/ws/v1/cluster/apps/application_123_0/state");conn = (HttpURLConnection) url.openConnection();TestRMWebServicesDelegationTokenAuthentication.setupConn(conn, "PUT", "application/json", requestBody);try {conn.getInputStream();fail("Anonymous users should not be allowed to kill apps in secure mode.");} catch (IOException ie) {assertEquals(Status.FORBIDDEN.getStatusCode(), conn.getResponseCode());}}
private void testAnonymousSimpleUser() {ApplicationSubmissionContextInfo app = new ApplicationSubmissionContextInfo();String appid = "application_123_0";app.setApplicationId(appid);String requestBody = TestRMWebServicesDelegationTokenAuthentication.getMarshalledAppInfo(app);URL url = new URL("http://localhost:8088/ws/v1/cluster/apps");HttpURLConnection conn = (HttpURLConnection) url.openConnection();TestRMWebServicesDelegationTokenAuthentication.setupConn(conn, "POST", "application/xml", requestBody);conn.getInputStream();assertEquals(Status.ACCEPTED.getStatusCode(), conn.getResponseCode());boolean appExists = rm.getRMContext().getRMApps().containsKey(ConverterUtils.toApplicationId(appid));assertTrue(appExists);RMApp actualApp = rm.getRMContext().getRMApps().get(ConverterUtils.toApplicationId(appid));String owner = actualApp.getUser();assertEquals(rm.getConfig().get(CommonConfigurationKeys.HADOOP_HTTP_STATIC_USER, CommonConfigurationKeys.DEFAULT_HADOOP_HTTP_STATIC_USER), owner);appid = "application_123_1";app.setApplicationId(appid);requestBody = TestRMWebServicesDelegationTokenAuthentication.getMarshalledAppInfo(app);url = new URL("http://localhost:8088/ws/v1/cluster/apps?user.name=client");conn = (HttpURLConnection) url.openConnection();TestRMWebServicesDelegationTokenAuthentication.setupConn(conn, "POST", MediaType.APPLICATION_XML, requestBody);conn.getInputStream();appExists = rm.getRMContext().getRMApps().containsKey(ConverterUtils.toApplicationId(appid));assertTrue(appExists);actualApp = rm.getRMContext().getRMApps().get(ConverterUtils.toApplicationId(appid));owner = actualApp.getUser();assertEquals("client", owner);}public void setUp() {// create the test root on local_fsfcTarget = FileContext.getLocalFSFileContext();chrootedTo = fileContextTestHelper.getAbsoluteTestRootPath(fcTarget);// In case previous test was killed before cleanupfcTarget.delete(chrootedTo, true);fcTarget.mkdir(chrootedTo, FileContext.DEFAULT_PERM, true);Configuration conf = new Configuration();// ChRoot to the root of the testDirectoryfc = FileContext.getFileContext(new ChRootedFs(fcTarget.getDefaultFileSystem(), chrootedTo), conf);}
public void tearDown() {fcTarget.delete(chrootedTo, true);}
public void testBasicPaths() {URI uri = fc.getDefaultFileSystem().getUri();Assert.assertEquals(chrootedTo.toUri(), uri);Assert.assertEquals(fc.makeQualified(new Path(System.getProperty("user.home"))), fc.getWorkingDirectory());Assert.assertEquals(fc.makeQualified(new Path(System.getProperty("user.home"))), fc.getHomeDirectory());Assert.assertEquals(new Path("/foo/bar").makeQualified(FsConstants.LOCAL_FS_URI, null), fc.makeQualified(new Path("/foo/bar")));}
public void testCreateDelete() {// Create file fileContextTestHelper.createFileNonRecursive(fc, "/foo");Assert.assertTrue(isFile(fc, new Path("/foo")));Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo, "foo")));// Create file with recursive dirfileContextTestHelper.createFile(fc, "/newDir/foo");Assert.assertTrue(isFile(fc, new Path("/newDir/foo")));Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo, "newDir/foo")));// Delete the created fileAssert.assertTrue(fc.delete(new Path("/newDir/foo"), false));Assert.assertFalse(exists(fc, new Path("/newDir/foo")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "newDir/foo")));// Create file with a 2 component dirs recursivelyfileContextTestHelper.createFile(fc, "/newDir/newDir2/foo");Assert.assertTrue(isFile(fc, new Path("/newDir/newDir2/foo")));Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo, "newDir/newDir2/foo")));// Delete the created fileAssert.assertTrue(fc.delete(new Path("/newDir/newDir2/foo"), false));Assert.assertFalse(exists(fc, new Path("/newDir/newDir2/foo")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "newDir/newDir2/foo")));}
public void testMkdirDelete() {fc.mkdir(fileContextTestHelper.getTestRootPath(fc, "/dirX"), FileContext.DEFAULT_PERM, false);Assert.assertTrue(isDir(fc, new Path("/dirX")));Assert.assertTrue(isDir(fcTarget, new Path(chrootedTo, "dirX")));fc.mkdir(fileContextTestHelper.getTestRootPath(fc, "/dirX/dirY"), FileContext.DEFAULT_PERM, false);Assert.assertTrue(isDir(fc, new Path("/dirX/dirY")));Assert.assertTrue(isDir(fcTarget, new Path(chrootedTo, "dirX/dirY")));// Delete the created dirAssert.assertTrue(fc.delete(new Path("/dirX/dirY"), false));Assert.assertFalse(exists(fc, new Path("/dirX/dirY")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "dirX/dirY")));Assert.assertTrue(fc.delete(new Path("/dirX"), false));Assert.assertFalse(exists(fc, new Path("/dirX")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "dirX")));}
public void testRename() {// Rename a filefileContextTestHelper.createFile(fc, "/newDir/foo");fc.rename(new Path("/newDir/foo"), new Path("/newDir/fooBar"));Assert.assertFalse(exists(fc, new Path("/newDir/foo")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "newDir/foo")));Assert.assertTrue(isFile(fc, fileContextTestHelper.getTestRootPath(fc, "/newDir/fooBar")));Assert.assertTrue(isFile(fcTarget, new Path(chrootedTo, "newDir/fooBar")));// Rename a dirfc.mkdir(new Path("/newDir/dirFoo"), FileContext.DEFAULT_PERM, false);fc.rename(new Path("/newDir/dirFoo"), new Path("/newDir/dirFooBar"));Assert.assertFalse(exists(fc, new Path("/newDir/dirFoo")));Assert.assertFalse(exists(fcTarget, new Path(chrootedTo, "newDir/dirFoo")));Assert.assertTrue(isDir(fc, fileContextTestHelper.getTestRootPath(fc, "/newDir/dirFooBar")));Assert.assertTrue(isDir(fcTarget, new Path(chrootedTo, "newDir/dirFooBar")));}
public void testRenameAcrossFs() {fc.mkdir(new Path("/newDir/dirFoo"), FileContext.DEFAULT_PERM, true);// the root will get interpreted to the root of the chrooted fs.fc.rename(new Path("/newDir/dirFoo"), new Path("file:///dirFooBar"));FileContextTestHelper.isDir(fc, new Path("/dirFooBar"));}
public void testList() {FileStatus fs = fc.getFileStatus(new Path("/"));Assert.assertTrue(fs.isDirectory());Assert.assertEquals(fs.getPath(), chrootedTo);FileStatus[] dirPaths = fc.util().listStatus(new Path("/"));Assert.assertEquals(0, dirPaths.length);fileContextTestHelper.createFileNonRecursive(fc, "/foo");fileContextTestHelper.createFileNonRecursive(fc, "/bar");fc.mkdir(new Path("/dirX"), FileContext.DEFAULT_PERM, false);fc.mkdir(fileContextTestHelper.getTestRootPath(fc, "/dirY"), FileContext.DEFAULT_PERM, false);fc.mkdir(new Path("/dirX/dirXX"), FileContext.DEFAULT_PERM, false);dirPaths = fc.util().listStatus(new Path("/"));Assert.assertEquals(4, dirPaths.length);// Note the the file status paths are the full paths on targetfs = fileContextTestHelper.containsPath(fcTarget, "foo", dirPaths);Assert.assertNotNull(fs);Assert.assertTrue(fs.isFile());fs = fileContextTestHelper.containsPath(fcTarget, "bar", dirPaths);Assert.assertNotNull(fs);Assert.assertTrue(fs.isFile());fs = fileContextTestHelper.containsPath(fcTarget, "dirX", dirPaths);Assert.assertNotNull(fs);Assert.assertTrue(fs.isDirectory());fs = fileContextTestHelper.containsPath(fcTarget, "dirY", dirPaths);Assert.assertNotNull(fs);Assert.assertTrue(fs.isDirectory());}
public void testWorkingDirectory() {// First we cd to our test rootfc.mkdir(new Path("/testWd"), FileContext.DEFAULT_PERM, false);Path workDir = new Path("/testWd");Path fqWd = fc.makeQualified(workDir);fc.setWorkingDirectory(workDir);Assert.assertEquals(fqWd, fc.getWorkingDirectory());fc.setWorkingDirectory(new Path("."));Assert.assertEquals(fqWd, fc.getWorkingDirectory());fc.setWorkingDirectory(new Path(".."));Assert.assertEquals(fqWd.getParent(), fc.getWorkingDirectory());// Go back to our test rootworkDir = new Path("/testWd");fqWd = fc.makeQualified(workDir);fc.setWorkingDirectory(workDir);Assert.assertEquals(fqWd, fc.getWorkingDirectory());Path relativeDir = new Path("existingDir1");Path absoluteDir = new Path(workDir, "existingDir1");fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);Path fqAbsoluteDir = fc.makeQualified(absoluteDir);fc.setWorkingDirectory(relativeDir);Assert.assertEquals(fqAbsoluteDir, fc.getWorkingDirectory());// cd using a absolute pathabsoluteDir = new Path("/test/existingDir2");fqAbsoluteDir = fc.makeQualified(absoluteDir);fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);fc.setWorkingDirectory(absoluteDir);Assert.assertEquals(fqAbsoluteDir, fc.getWorkingDirectory());// Now open a file relative to the wd we just set above.Path absolutePath = new Path(absoluteDir, "foo");fc.create(absolutePath, EnumSet.of(CreateFlag.CREATE)).close();fc.open(new Path("foo")).close();// Now mkdir relative to the dir we cd'ed tofc.mkdir(new Path("newDir"), FileContext.DEFAULT_PERM, true);Assert.assertTrue(isDir(fc, new Path(absoluteDir, "newDir")));absoluteDir = fileContextTestHelper.getTestRootPath(fc, "nonexistingPath");try {fc.setWorkingDirectory(absoluteDir);Assert.fail("cd to non existing dir should have failed");} catch (Exception e) {}// Try a URIfinal String LOCAL_FS_ROOT_URI = "file:///tmp/test";absoluteDir = new Path(LOCAL_FS_ROOT_URI + "/existingDir");fc.mkdir(absoluteDir, FileContext.DEFAULT_PERM, true);fc.setWorkingDirectory(absoluteDir);Assert.assertEquals(absoluteDir, fc.getWorkingDirectory());}
public void testResolvePath() {Assert.assertEquals(chrootedTo, fc.getDefaultFileSystem().resolvePath(new Path("/")));fileContextTestHelper.createFile(fc, "/foo");Assert.assertEquals(new Path(chrootedTo, "foo"), fc.getDefaultFileSystem().resolvePath(new Path("/foo")));}
public void testResolvePathNonExisting() {fc.getDefaultFileSystem().resolvePath(new Path("/nonExisting"));}
public void testIsValidNameValidInBaseFs() {AbstractFileSystem baseFs = Mockito.spy(fc.getDefaultFileSystem());ChRootedFs chRootedFs = new ChRootedFs(baseFs, new Path("/chroot"));Mockito.doReturn(true).when(baseFs).isValidName(Mockito.anyString());Assert.assertTrue(chRootedFs.isValidName("/test"));Mockito.verify(baseFs).isValidName("/chroot/test");}
public void testIsValidNameInvalidInBaseFs() {AbstractFileSystem baseFs = Mockito.spy(fc.getDefaultFileSystem());ChRootedFs chRootedFs = new ChRootedFs(baseFs, new Path("/chroot"));Mockito.doReturn(false).when(baseFs).isValidName(Mockito.anyString());Assert.assertFalse(chRootedFs.isValidName("/test"));Mockito.verify(baseFs).isValidName("/chroot/test");}public void startUp() {conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();cluster.waitActive();fs = cluster.getFileSystem();dfs = (DistributedFileSystem) fs;}
public void tearDown() {if (fs != null) {fs.close();}if (cluster != null) {cluster.shutdown();}}
public void testManualSafeMode() {fs = cluster.getFileSystem();Path file1 = new Path("/tmp/testManualSafeMode/file1");Path file2 = new Path("/tmp/testManualSafeMode/file2");// create two files with one block each.DFSTestUtil.createFile(fs, file1, 1000, (short) 1, 0);DFSTestUtil.createFile(fs, file2, 1000, (short) 1, 0);fs.close();cluster.shutdown();// now bring up just the NameNode.cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false).build();cluster.waitActive();dfs = cluster.getFileSystem();assertTrue("No datanode is started. Should be in SafeMode", dfs.setSafeMode(SafeModeAction.SAFEMODE_GET));// manually set safemode.dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);// now bring up the datanode and wait for it to be active.cluster.startDataNodes(conf, 1, true, null, null);cluster.waitActive();// wait longer than dfs.namenode.safemode.extensiontry {Thread.sleep(2000);} catch (InterruptedException ignored) {}assertTrue("should still be in SafeMode", dfs.setSafeMode(SafeModeAction.SAFEMODE_GET));assertFalse("should not be in SafeMode", dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE));}
public void testNoExtensionIfNoBlocks() {cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 60000);cluster.restartNameNode();// exit safemode on startup because there are no blocks in the namespace.String status = cluster.getNameNode().getNamesystem().getSafemode();assertEquals("", status);}
public void testInitializeReplQueuesEarly() {LOG.info("Starting testInitializeReplQueuesEarly");// concentrating all blocks on the first node.BlockManagerTestUtil.setWritingPrefersLocalNode(cluster.getNamesystem().getBlockManager(), false);cluster.startDataNodes(conf, 2, true, StartupOption.REGULAR, null);cluster.waitActive();LOG.info("Creating files");DFSTestUtil.createFile(fs, TEST_PATH, 15 * BLOCK_SIZE, (short) 1, 1L);LOG.info("Stopping all DataNodes");List<DataNodeProperties> dnprops = Lists.newLinkedList();dnprops.add(cluster.stopDataNode(0));dnprops.add(cluster.stopDataNode(0));dnprops.add(cluster.stopDataNode(0));cluster.getConfiguration(0).setFloat(DFSConfigKeys.DFS_NAMENODE_REPL_QUEUE_THRESHOLD_PCT_KEY, 1f / 15f);LOG.info("Restarting NameNode");cluster.restartNameNode();final NameNode nn = cluster.getNameNode();String status = nn.getNamesystem().getSafemode();assertEquals("Safe mode is ON. The reported blocks 0 needs additional " + "15 blocks to reach the threshold 0.9990 of total blocks 15.\n" + "The number of live datanodes 0 has reached the minimum number 0. " + "Safe mode will be turned off automatically once the thresholds " + "have been reached.", status);assertFalse("Mis-replicated block queues should not be initialized " + "until threshold is crossed", NameNodeAdapter.safeModeInitializedReplQueues(nn));LOG.info("Restarting one DataNode");cluster.restartDataNode(dnprops.remove(0));// the restarted DN to come in.GenericTestUtils.waitFor(new Supplier<Boolean>() {
@Overridepublic Boolean get() {return getLongCounter("StorageBlockReportOps", getMetrics(NN_METRICS)) == cluster.getStoragesPerDatanode();}}, 10, 10000);final int safe = NameNodeAdapter.getSafeModeSafeBlocks(nn);assertTrue("Expected first block report to make some blocks safe.", safe > 0);assertTrue("Did not expect first block report to make all blocks safe.", safe < 15);assertTrue(NameNodeAdapter.safeModeInitializedReplQueues(nn));// Failure here will manifest as a test timeout.BlockManagerTestUtil.updateState(nn.getNamesystem().getBlockManager());long underReplicatedBlocks = nn.getNamesystem().getUnderReplicatedBlocks();while (underReplicatedBlocks != (15 - safe)) {LOG.info("UnderReplicatedBlocks expected=" + (15 - safe) + ", actual=" + underReplicatedBlocks);Thread.sleep(100);BlockManagerTestUtil.updateState(nn.getNamesystem().getBlockManager());underReplicatedBlocks = nn.getNamesystem().getUnderReplicatedBlocks();}cluster.restartDataNodes();}
public Boolean get() {return getLongCounter("StorageBlockReportOps", getMetrics(NN_METRICS)) == cluster.getStoragesPerDatanode();}
public void testRbwBlocksNotConsideredUnderReplicated() {List<FSDataOutputStream> stms = Lists.newArrayList();try {// exit safemode on restart.DFSTestUtil.createFile(fs, new Path("/junk-blocks"), BLOCK_SIZE * 4, (short) 1, 1L);// hide this bug from the test!for (int i = 0; i < 10; i++) {FSDataOutputStream stm = fs.create(new Path("/append-" + i), true, BLOCK_SIZE, (short) 1, BLOCK_SIZE);stms.add(stm);stm.write(1);stm.hflush();}cluster.restartNameNode();FSNamesystem ns = cluster.getNameNode(0).getNamesystem();BlockManagerTestUtil.updateState(ns.getBlockManager());assertEquals(0, ns.getPendingReplicationBlocks());assertEquals(0, ns.getCorruptReplicaBlocks());assertEquals(0, ns.getMissingBlocksCount());} finally {for (FSDataOutputStream stm : stms) {IOUtils.closeStream(stm);}cluster.shutdown();}}
public void runFsFun(String msg, FSRun f) {try {f.run(fs);fail(msg);} catch (IOException ioe) {assertTrue(ioe.getMessage().contains("safe mode"));}}
public void testOperationsWhileInSafeMode() {final Path file1 = new Path("/file1");assertFalse(dfs.setSafeMode(SafeModeAction.SAFEMODE_GET));DFSTestUtil.createFile(fs, file1, 1024, (short) 1, 0);assertTrue("Could not enter SM", dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER));runFsFun("Set quota while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {((DistributedFileSystem) fs).setQuota(file1, 1, 1);}});runFsFun("Set perm while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setPermission(file1, FsPermission.getDefault());}});runFsFun("Set owner while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setOwner(file1, "user", "group");}});runFsFun("Set repl while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setReplication(file1, (short) 1);}});runFsFun("Append file while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {DFSTestUtil.appendFile(fs, file1, "new bytes");}});runFsFun("Delete file while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.delete(file1, false);}});runFsFun("Rename file while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.rename(file1, new Path("file2"));}});runFsFun("Set time while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setTimes(file1, 0, 0);}});runFsFun("modifyAclEntries while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.modifyAclEntries(file1, Lists.<AclEntry>newArrayList());}});runFsFun("removeAclEntries while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.removeAclEntries(file1, Lists.<AclEntry>newArrayList());}});runFsFun("removeDefaultAcl while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.removeDefaultAcl(file1);}});runFsFun("removeAcl while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.removeAcl(file1);}});runFsFun("setAcl while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setAcl(file1, Lists.<AclEntry>newArrayList());}});runFsFun("setXAttr while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.setXAttr(file1, "user.a1", null);}});runFsFun("removeXAttr while in SM", new FSRun() {
@Overridepublic void run(FileSystem fs) throws IOException {fs.removeXAttr(file1, "user.a1");}});try {DFSTestUtil.readFile(fs, file1);} catch (IOException ioe) {fail("Set times failed while in SM");}try {fs.getAclStatus(file1);} catch (IOException ioe) {fail("getAclStatus failed while in SM");}// Test accessUserGroupInformation ugiX = UserGroupInformation.createRemoteUser("userX");FileSystem myfs = ugiX.doAs(new PrivilegedExceptionAction<FileSystem>() {
@Overridepublic FileSystem run() throws IOException {return FileSystem.get(conf);}});myfs.access(file1, FsAction.READ);try {myfs.access(file1, FsAction.WRITE);fail("The access call should have failed.");} catch (AccessControlException e) {}assertFalse("Could not leave SM", dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE));}
public void run(FileSystem fs) {((DistributedFileSystem) fs).setQuota(file1, 1, 1);}
public void run(FileSystem fs) {fs.setPermission(file1, FsPermission.getDefault());}
public void run(FileSystem fs) {fs.setOwner(file1, "user", "group");}
public void run(FileSystem fs) {fs.setReplication(file1, (short) 1);}
public void run(FileSystem fs) {DFSTestUtil.appendFile(fs, file1, "new bytes");}
public void run(FileSystem fs) {fs.delete(file1, false);}
public void run(FileSystem fs) {fs.rename(file1, new Path("file2"));}
public void run(FileSystem fs) {fs.setTimes(file1, 0, 0);}
public void run(FileSystem fs) {fs.modifyAclEntries(file1, Lists.<AclEntry>newArrayList());}
public void run(FileSystem fs) {fs.removeAclEntries(file1, Lists.<AclEntry>newArrayList());}
public void run(FileSystem fs) {fs.removeDefaultAcl(file1);}
public void run(FileSystem fs) {fs.removeAcl(file1);}
public void run(FileSystem fs) {fs.setAcl(file1, Lists.<AclEntry>newArrayList());}
public void run(FileSystem fs) {fs.setXAttr(file1, "user.a1", null);}
public void run(FileSystem fs) {fs.removeXAttr(file1, "user.a1");}
public FileSystem run() {return FileSystem.get(conf);}
public void testDatanodeThreshold() {cluster.shutdown();Configuration conf = cluster.getConfiguration(0);conf.setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 0);conf.setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY, 1);cluster.restartNameNode();fs = cluster.getFileSystem();String tipMsg = cluster.getNamesystem().getSafemode();assertTrue("Safemode tip message doesn't look right: " + tipMsg, tipMsg.contains("The number of live datanodes 0 needs an additional " + "1 live datanodes to reach the minimum number 1.\n" + "Safe mode will be turned off automatically"));// Start a datanodecluster.startDataNodes(conf, 1, true, null, null);// Wait long enough for safemode check to refiretry {Thread.sleep(1000);} catch (InterruptedException ignored) {}// We now should be out of safe mode.assertEquals("", cluster.getNamesystem().getSafemode());}
public void testSafeModeUtils() {dfs = cluster.getFileSystem();// Enter safemode.dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);assertTrue("State was expected to be in safemode.", dfs.isInSafeMode());// Exit safemode.dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);assertFalse("State was expected to be out of safemode.", dfs.isInSafeMode());}
public void testSafeModeWhenZeroBlockLocations() {try {Path file1 = new Path("/tmp/testManualSafeMode/file1");Path file2 = new Path("/tmp/testManualSafeMode/file2");System.out.println("Created file1 and file2.");// create two files with one block each.DFSTestUtil.createFile(fs, file1, 1000, (short) 1, 0);DFSTestUtil.createFile(fs, file2, 2000, (short) 1, 0);checkGetBlockLocationsWorks(fs, file1);NameNode namenode = cluster.getNameNode();// manually set safemode.dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);assertTrue("should still be in SafeMode", namenode.isInSafeMode());// getBlock locations should still work since block locations existscheckGetBlockLocationsWorks(fs, file1);dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);assertFalse("should not be in SafeMode", namenode.isInSafeMode());// Now 2nd part of the tests where there aren't block locationscluster.shutdownDataNodes();cluster.shutdownNameNode(0);// now bring up just the NameNode.cluster.restartNameNode();cluster.waitActive();System.out.println("Restarted cluster with just the NameNode");namenode = cluster.getNameNode();assertTrue("No datanode is started. Should be in SafeMode", namenode.isInSafeMode());FileStatus stat = fs.getFileStatus(file1);try {fs.getFileBlockLocations(stat, 0, 1000);assertTrue("Should have got safemode exception", false);} catch (SafeModeException e) {} catch (RemoteException re) {if (!re.getClassName().equals(SafeModeException.class.getName()))assertTrue("Should have got safemode exception", false);}dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);assertFalse("Should not be in safemode", namenode.isInSafeMode());checkGetBlockLocationsWorks(fs, file1);} finally {if (fs != null)fs.close();if (cluster != null)cluster.shutdown();}}
void checkGetBlockLocationsWorks(FileSystem fs, Path fileName) {FileStatus stat = fs.getFileStatus(fileName);try {fs.getFileBlockLocations(stat, 0, 1000);} catch (SafeModeException e) {assertTrue("Should have not got safemode exception", false);} catch (RemoteException re) {assertTrue("Should have not got safemode exception", false);}}public void testGetNewStamp() {int numDataNodes = 1;Configuration conf = new HdfsConfiguration();MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();try {cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();NamenodeProtocols namenode = cluster.getNameNodeRpc();/* Test writing to finalized replicas */Path file = new Path("dataprotocol.dat");DFSTestUtil.createFile(fileSys, file, 1L, (short) numDataNodes, 0L);// get the first blockid for the fileExtendedBlock firstBlock = DFSTestUtil.getFirstBlock(fileSys, file);// test getNewStampAndToken on a finalized blocktry {namenode.updateBlockForPipeline(firstBlock, "");Assert.fail("Can not get a new GS from a finalized block");} catch (IOException e) {Assert.assertTrue(e.getMessage().contains("is not under Construction"));}// test getNewStampAndToken on a non-existent blocktry {long newBlockId = firstBlock.getBlockId() + 1;ExtendedBlock newBlock = new ExtendedBlock(firstBlock.getBlockPoolId(), newBlockId, 0, firstBlock.getGenerationStamp());namenode.updateBlockForPipeline(newBlock, "");Assert.fail("Cannot get a new GS from a non-existent block");} catch (IOException e) {Assert.assertTrue(e.getMessage().contains("does not exist"));}// change first block to a RBWDFSOutputStream out = null;try {out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();FSDataInputStream in = null;try {in = fileSys.open(file);firstBlock = DFSTestUtil.getAllBlocks(in).get(0).getBlock();} finally {IOUtils.closeStream(in);}// test non-lease holderDFSClient dfs = ((DistributedFileSystem) fileSys).dfs;try {namenode.updateBlockForPipeline(firstBlock, "test" + dfs.clientName);Assert.fail("Cannot get a new GS for a non lease holder");} catch (LeaseExpiredException e) {Assert.assertTrue(e.getMessage().startsWith("Lease mismatch"));}// test null lease holdertry {namenode.updateBlockForPipeline(firstBlock, null);Assert.fail("Cannot get a new GS for a null lease holder");} catch (LeaseExpiredException e) {Assert.assertTrue(e.getMessage().startsWith("Lease mismatch"));}// test getNewStampAndToken on a rbw blocknamenode.updateBlockForPipeline(firstBlock, dfs.clientName);} finally {IOUtils.closeStream(out);}} finally {cluster.shutdown();}}
public void testPipelineRecoveryForLastBlock() {DFSClientFaultInjector faultInjector = Mockito.mock(DFSClientFaultInjector.class);DFSClientFaultInjector oldInjector = DFSClientFaultInjector.instance;DFSClientFaultInjector.instance = faultInjector;Configuration conf = new HdfsConfiguration();conf.setInt(DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_RETRIES_KEY, 3);MiniDFSCluster cluster = null;try {int numDataNodes = 3;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol1.dat");Mockito.when(faultInjector.failPacket()).thenReturn(true);DFSTestUtil.createFile(fileSys, file, 68000000L, (short) numDataNodes, 0L);// Read should succeed.FSDataInputStream in = fileSys.open(file);try {int c = in.read();} catch (org.apache.hadoop.hdfs.BlockMissingException bme) {Assert.fail("Block is missing because the file was closed with" + " corrupt replicas.");}} finally {DFSClientFaultInjector.instance = oldInjector;if (cluster != null) {cluster.shutdown();}}}
public void testPipelineRecoveryOnOOB() {Configuration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_CLIENT_DATANODE_RESTART_TIMEOUT_KEY, "15");MiniDFSCluster cluster = null;try {int numDataNodes = 1;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol2.dat");DFSTestUtil.createFile(fileSys, file, 10240L, (short) 1, 0L);DFSOutputStream out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();DFSAdmin dfsadmin = new DFSAdmin(conf);DataNode dn = cluster.getDataNodes().get(0);final String dnAddr = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args1 = { "-shutdownDatanode", dnAddr, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args1));// Wait long enough to receive an OOB ack before closing the file.Thread.sleep(4000);// Retart the datanode cluster.restartDataNode(0, true);// The following forces a data packet and end of block packets to be sent. out.close();} finally {if (cluster != null) {cluster.shutdown();}}}
public void testPipelineRecoveryOnRestartFailure() {Configuration conf = new HdfsConfiguration();conf.set(DFSConfigKeys.DFS_CLIENT_DATANODE_RESTART_TIMEOUT_KEY, "5");MiniDFSCluster cluster = null;try {int numDataNodes = 2;cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();cluster.waitActive();FileSystem fileSys = cluster.getFileSystem();Path file = new Path("dataprotocol3.dat");DFSTestUtil.createFile(fileSys, file, 10240L, (short) 2, 0L);DFSOutputStream out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();DFSAdmin dfsadmin = new DFSAdmin(conf);DataNode dn = cluster.getDataNodes().get(0);final String dnAddr1 = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args1 = { "-shutdownDatanode", dnAddr1, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args1));Thread.sleep(4000);// expire and regular pipeline recovery will kick in. out.close();// At this point there is only one node in the cluster. out = (DFSOutputStream) (fileSys.append(file).getWrappedStream());out.write(1);out.hflush();dn = cluster.getDataNodes().get(1);final String dnAddr2 = dn.getDatanodeId().getIpcAddr(false);// issue shutdown to the datanode.final String[] args2 = { "-shutdownDatanode", dnAddr2, "upgrade" };Assert.assertEquals(0, dfsadmin.run(args2));Thread.sleep(4000);try {// close should failout.close();assert false;} catch (IOException ioe) {}} finally {if (cluster != null) {cluster.shutdown();}}}public void init(String compression, String comparator, int numRecords1stBlock, int numRecords2ndBlock) {init(compression, comparator);this.records1stBlock = numRecords1stBlock;this.records2ndBlock = numRecords2ndBlock;}
public void init(String compression, String comparator) {this.compression = compression;this.comparator = comparator;}
public void setUp() {path = new Path(ROOT, outputFile);fs = path.getFileSystem(conf);out = fs.create(path);writer = new Writer(out, BLOCK_SIZE, compression, comparator, conf);}
public void tearDown() {if (!skip)fs.delete(path, true);}
public void testNoDataEntry() {if (skip)return;closeOutput();Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.assertTrue(reader.isSorted());Scanner scanner = reader.createScanner();Assert.assertTrue(scanner.atEnd());scanner.close();reader.close();}
public void testOneDataEntry() {if (skip)return;writeRecords(1);readRecords(1);checkBlockIndex(0, 0);readValueBeforeKey(0);readKeyWithoutValue(0);readValueWithoutKey(0);readKeyManyTimes(0);}
public void testTwoDataEntries() {if (skip)return;writeRecords(2);readRecords(2);}
public void testOneBlock() {if (skip)return;// just under one blockwriteRecords(records1stBlock);readRecords(records1stBlock);// last key should be in the first block (block 0)checkBlockIndex(records1stBlock - 1, 0);}
public void testOneBlockPlusOneEntry() {if (skip)return;writeRecords(records1stBlock + 1);readRecords(records1stBlock + 1);checkBlockIndex(records1stBlock - 1, 0);checkBlockIndex(records1stBlock, 1);}
public void testTwoBlocks() {if (skip)return;writeRecords(records1stBlock + 5);readRecords(records1stBlock + 5);checkBlockIndex(records1stBlock + 4, 1);}
public void testThreeBlocks() {if (skip)return;writeRecords(2 * records1stBlock + 5);readRecords(2 * records1stBlock + 5);checkBlockIndex(2 * records1stBlock + 4, 2);// 1st key in filereadValueBeforeKey(0);readKeyWithoutValue(0);readValueWithoutKey(0);readKeyManyTimes(0);// last key in filereadValueBeforeKey(2 * records1stBlock + 4);readKeyWithoutValue(2 * records1stBlock + 4);readValueWithoutKey(2 * records1stBlock + 4);readKeyManyTimes(2 * records1stBlock + 4);// 1st key in mid block, verify block indexes then readcheckBlockIndex(records1stBlock - 1, 0);checkBlockIndex(records1stBlock, 1);readValueBeforeKey(records1stBlock);readKeyWithoutValue(records1stBlock);readValueWithoutKey(records1stBlock);readKeyManyTimes(records1stBlock);// last key in mid block, verify block indexes then readcheckBlockIndex(records1stBlock + records2ndBlock - 1, 1);checkBlockIndex(records1stBlock + records2ndBlock, 2);readValueBeforeKey(records1stBlock + records2ndBlock - 1);readKeyWithoutValue(records1stBlock + records2ndBlock - 1);readValueWithoutKey(records1stBlock + records2ndBlock - 1);readKeyManyTimes(records1stBlock + records2ndBlock - 1);// mid in mid blockreadValueBeforeKey(records1stBlock + 10);readKeyWithoutValue(records1stBlock + 10);readValueWithoutKey(records1stBlock + 10);readKeyManyTimes(records1stBlock + 10);}
Location locate(Scanner scanner, byte[] key) {if (scanner.seekTo(key) == true) {return scanner.currentLocation;}return scanner.endLocation;}
public void testLocate() {if (skip)return;writeRecords(3 * records1stBlock);Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();locate(scanner, composeSortedKey(KEY, 2).getBytes());locate(scanner, composeSortedKey(KEY, records1stBlock - 1).getBytes());locate(scanner, composeSortedKey(KEY, records1stBlock).getBytes());Location locX = locate(scanner, "keyX".getBytes());Assert.assertEquals(scanner.endLocation, locX);scanner.close();reader.close();}
public void testFailureWriterNotClosed() {if (skip)return;Reader reader = null;try {reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.fail("Cannot read before closing the writer.");} catch (IOException e) {} finally {if (reader != null) {reader.close();}}}
public void testFailureWriteMetaBlocksWithSameName() {if (skip)return;writer.append("keyX".getBytes(), "valueX".getBytes());// create a new metablockDataOutputStream outMeta = writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());outMeta.write(123);outMeta.write("foo".getBytes());outMeta.close();// add the same metablocktry {writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());Assert.fail("Cannot create metablocks with the same name.");} catch (Exception e) {}closeOutput();}
public void testFailureGetNonExistentMetaBlock() {if (skip)return;writer.append("keyX".getBytes(), "valueX".getBytes());// create a new metablockDataOutputStream outMeta = writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());outMeta.write(123);outMeta.write("foo".getBytes());outMeta.close();closeOutput();Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);DataInputStream mb = reader.getMetaBlock("testX");Assert.assertNotNull(mb);mb.close();try {DataInputStream mbBad = reader.getMetaBlock("testY");Assert.fail("Error on handling non-existent metablocks.");} catch (Exception e) {}reader.close();}
public void testFailureWriteRecordAfterMetaBlock() {if (skip)return;// write a key/value firstwriter.append("keyX".getBytes(), "valueX".getBytes());// create a new metablockDataOutputStream outMeta = writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());outMeta.write(123);outMeta.write("dummy".getBytes());outMeta.close();// add more key/valuetry {writer.append("keyY".getBytes(), "valueY".getBytes());Assert.fail("Cannot add key/value after start adding meta blocks.");} catch (Exception e) {}closeOutput();}
public void testFailureReadValueManyTimes() {if (skip)return;writeRecords(5);Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + 0);try {scanner.entry().getValue(vbuf);Assert.fail("Cannot get the value mlutiple times.");} catch (Exception e) {}scanner.close();reader.close();}
public void testFailureBadCompressionCodec() {if (skip)return;closeOutput();out = fs.create(path);try {writer = new Writer(out, BLOCK_SIZE, "BAD", comparator, conf);Assert.fail("Error on handling invalid compression codecs.");} catch (Exception e) {}}
public void testFailureOpenEmptyFile() {if (skip)return;closeOutput();// create an absolutely empty filepath = new Path(fs.getWorkingDirectory(), outputFile);out = fs.create(path);out.close();try {new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.fail("Error on handling empty files.");} catch (EOFException e) {}}
public void testFailureOpenRandomFile() {if (skip)return;closeOutput();path = new Path(fs.getWorkingDirectory(), outputFile);out = fs.create(path);Random rand = new Random();byte[] buf = new byte[K];// fill with > 1MB datafor (int nx = 0; nx < K + 2; nx++) {rand.nextBytes(buf);out.write(buf);}out.close();try {new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Assert.fail("Error on handling random files.");} catch (IOException e) {}}
public void testFailureKeyLongerThan64K() {if (skip)return;byte[] buf = new byte[64 * K + 1];Random rand = new Random();rand.nextBytes(buf);try {writer.append(buf, "valueX".getBytes());} catch (IndexOutOfBoundsException e) {}closeOutput();}
public void testFailureOutOfOrderKeys() {if (skip)return;try {writer.append("keyM".getBytes(), "valueM".getBytes());writer.append("keyA".getBytes(), "valueA".getBytes());Assert.fail("Error on handling out of order keys.");} catch (Exception e) {}closeOutput();}
public void testFailureNegativeOffset() {if (skip)return;try {writer.append("keyX".getBytes(), -1, 4, "valueX".getBytes(), 0, 6);Assert.fail("Error on handling negative offset.");} catch (Exception e) {}closeOutput();}
public void testFailureNegativeOffset_2() {if (skip)return;closeOutput();Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {scanner.lowerBound("keyX".getBytes(), -1, 4);Assert.fail("Error on handling negative offset.");} catch (Exception e) {} finally {reader.close();scanner.close();}closeOutput();}
public void testFailureNegativeLength() {if (skip)return;try {writer.append("keyX".getBytes(), 0, -1, "valueX".getBytes(), 0, 6);Assert.fail("Error on handling negative length.");} catch (Exception e) {}closeOutput();}
public void testFailureNegativeLength_2() {if (skip)return;closeOutput();Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {scanner.lowerBound("keyX".getBytes(), 0, -1);Assert.fail("Error on handling negative length.");} catch (Exception e) {} finally {scanner.close();reader.close();}closeOutput();}
public void testFailureNegativeLength_3() {if (skip)return;writeRecords(3);Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {// test negative array offsettry {scanner.seekTo("keyY".getBytes(), -1, 4);Assert.fail("Failed to handle negative offset.");} catch (Exception e) {}// test negative array lengthtry {scanner.seekTo("keyY".getBytes(), 0, -2);Assert.fail("Failed to handle negative key length.");} catch (Exception e) {}} finally {reader.close();scanner.close();}}
public void testFailureCompressionNotWorking() {if (skip)return;long rawDataSize = writeRecords(10 * records1stBlock, false);if (!compression.equalsIgnoreCase(Compression.Algorithm.NONE.getName())) {Assert.assertTrue(out.getPos() < rawDataSize);}closeOutput();}
public void testFailureFileWriteNotAt0Position() {if (skip)return;closeOutput();out = fs.create(path);out.write(123);try {writer = new Writer(out, BLOCK_SIZE, compression, comparator, conf);Assert.fail("Failed to catch file write not at position 0.");} catch (Exception e) {}closeOutput();}
private long writeRecords(int count) {return writeRecords(count, true);}
private long writeRecords(int count, boolean close) {long rawDataSize = writeRecords(writer, count);if (close) {closeOutput();}return rawDataSize;}
static long writeRecords(Writer writer, int count) {long rawDataSize = 0;int nx;for (nx = 0; nx < count; nx++) {byte[] key = composeSortedKey(KEY, nx).getBytes();byte[] value = (VALUE + nx).getBytes();writer.append(key, value);rawDataSize += WritableUtils.getVIntSize(key.length) + key.length + WritableUtils.getVIntSize(value.length) + value.length;}return rawDataSize;}
static String composeSortedKey(String prefix, int value) {return String.format("%s%010d", prefix, value);}
private void readRecords(int count) {readRecords(fs, path, count, conf);}
static void readRecords(FileSystem fs, Path path, int count, Configuration conf) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();try {for (int nx = 0; nx < count; nx++, scanner.advance()) {Assert.assertFalse(scanner.atEnd());byte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), composeSortedKey(KEY, nx));byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + nx);}Assert.assertTrue(scanner.atEnd());Assert.assertFalse(scanner.advance());} finally {scanner.close();reader.close();}}
private void checkBlockIndex(int recordIndex, int blockIndexExpected) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScanner();scanner.seekTo(composeSortedKey(KEY, recordIndex).getBytes());Assert.assertEquals(blockIndexExpected, scanner.currentLocation.getBlockIndex());scanner.close();reader.close();}
private void readValueBeforeKey(int recordIndex) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScannerByKey(composeSortedKey(KEY, recordIndex).getBytes(), null);try {byte[] vbuf = new byte[BUF_SIZE];int vlen = scanner.entry().getValueLength();scanner.entry().getValue(vbuf);Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + recordIndex);byte[] kbuf = new byte[BUF_SIZE];int klen = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf);Assert.assertEquals(new String(kbuf, 0, klen), composeSortedKey(KEY, recordIndex));} finally {scanner.close();reader.close();}}
private void readKeyWithoutValue(int recordIndex) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScannerByKey(composeSortedKey(KEY, recordIndex).getBytes(), null);try {// read the indexed keybyte[] kbuf1 = new byte[BUF_SIZE];int klen1 = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf1);Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY, recordIndex));if (scanner.advance() && !scanner.atEnd()) {// read the next key following the indexedbyte[] kbuf2 = new byte[BUF_SIZE];int klen2 = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf2);Assert.assertEquals(new String(kbuf2, 0, klen2), composeSortedKey(KEY, recordIndex + 1));}} finally {scanner.close();reader.close();}}
private void readValueWithoutKey(int recordIndex) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScannerByKey(composeSortedKey(KEY, recordIndex).getBytes(), null);byte[] vbuf1 = new byte[BUF_SIZE];int vlen1 = scanner.entry().getValueLength();scanner.entry().getValue(vbuf1);Assert.assertEquals(new String(vbuf1, 0, vlen1), VALUE + recordIndex);if (scanner.advance() && !scanner.atEnd()) {byte[] vbuf2 = new byte[BUF_SIZE];int vlen2 = scanner.entry().getValueLength();scanner.entry().getValue(vbuf2);Assert.assertEquals(new String(vbuf2, 0, vlen2), VALUE + (recordIndex + 1));}scanner.close();reader.close();}
private void readKeyManyTimes(int recordIndex) {Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);Scanner scanner = reader.createScannerByKey(composeSortedKey(KEY, recordIndex).getBytes(), null);// read the indexed keybyte[] kbuf1 = new byte[BUF_SIZE];int klen1 = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf1);Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY, recordIndex));klen1 = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf1);Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY, recordIndex));klen1 = scanner.entry().getKeyLength();scanner.entry().getKey(kbuf1);Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY, recordIndex));scanner.close();reader.close();}
private void closeOutput() {if (writer != null) {writer.close();writer = null;}if (out != null) {out.close();out = null;}}public void setUp() {if (options == null) {options = new MyOptions(new String[0]);}conf = new Configuration();conf.setInt("tfile.fs.input.buffer.size", options.fsInputBufferSize);conf.setInt("tfile.fs.output.buffer.size", options.fsOutputBufferSize);path = new Path(new Path(options.rootDir), options.file);fs = path.getFileSystem(conf);timer = new NanoTimer(false);rng = new Random(options.seed);keyLenGen = new RandomDistribution.Zipf(new Random(rng.nextLong()), options.minKeyLen, options.maxKeyLen, 1.2);DiscreteRNG valLenGen = new RandomDistribution.Flat(new Random(rng.nextLong()), options.minValLength, options.maxValLength);DiscreteRNG wordLenGen = new RandomDistribution.Flat(new Random(rng.nextLong()), options.minWordLen, options.maxWordLen);kvGen = new KVGenerator(rng, true, keyLenGen, valLenGen, wordLenGen, options.dictSize);}
public void tearDown() {fs.delete(path, true);}
private static FSDataOutputStream createFSOutput(Path name, FileSystem fs) {if (fs.exists(name)) {fs.delete(name, true);}FSDataOutputStream fout = fs.create(name);return fout;}
private void createTFile() {long totalBytes = 0;FSDataOutputStream fout = createFSOutput(path, fs);try {Writer writer = new Writer(fout, options.minBlockSize, options.compress, "memcmp", conf);try {BytesWritable key = new BytesWritable();BytesWritable val = new BytesWritable();timer.start();for (long i = 0; true; ++i) {if (i % 1000 == 0) {// test the size for every 1000 rows.if (fs.getFileStatus(path).getLen() >= options.fileSize) {break;}}kvGen.next(key, val, false);writer.append(key.get(), 0, key.getSize(), val.get(), 0, val.getSize());totalBytes += key.getSize();totalBytes += val.getSize();}timer.stop();} finally {writer.close();}} finally {fout.close();}// in us.double duration = (double) timer.read() / 1000;long fsize = fs.getFileStatus(path).getLen();System.out.printf("time: %s...uncompressed: %.2fMB...raw thrpt: %.2fMB/s\n", timer.toString(), (double) totalBytes / 1024 / 1024, totalBytes / duration);System.out.printf("time: %s...file size: %.2fMB...disk thrpt: %.2fMB/s\n", timer.toString(), (double) fsize / 1024 / 1024, fsize / duration);}
public void seekTFile() {int miss = 0;long totalBytes = 0;FSDataInputStream fsdis = fs.open(path);Reader reader = new Reader(fsdis, fs.getFileStatus(path).getLen(), conf);KeySampler kSampler = new KeySampler(rng, reader.getFirstKey(), reader.getLastKey(), keyLenGen);Scanner scanner = reader.createScanner();BytesWritable key = new BytesWritable();BytesWritable val = new BytesWritable();timer.reset();timer.start();for (int i = 0; i < options.seekCount; ++i) {kSampler.next(key);scanner.lowerBound(key.get(), 0, key.getSize());if (!scanner.atEnd()) {scanner.entry().get(key, val);totalBytes += key.getSize();totalBytes += val.getSize();} else {++miss;}}timer.stop();// in us.double duration = (double) timer.read() / 1000;System.out.printf("time: %s...avg seek: %s...%d hit...%d miss...avg I/O size: %.2fKB\n", timer.toString(), NanoTimer.nanoTimeToString(timer.read() / options.seekCount), options.seekCount - miss, miss, (double) totalBytes / 1024 / (options.seekCount - miss));}
public void testSeeks() {String[] supported = TFile.getSupportedCompressionAlgorithms();boolean proceed = false;for (String c : supported) {if (c.equals(options.compress)) {proceed = true;break;}}if (!proceed) {System.out.println("Skipped for " + options.compress);return;}if (options.doCreate()) {createTFile();}if (options.doRead()) {seekTFile();}}
public static IntegerRange parse(String s) {StringTokenizer st = new StringTokenizer(s, " \t,");if (st.countTokens() != 2) {throw new ParseException("Bad integer specification: " + s);}int from = Integer.parseInt(st.nextToken());int to = Integer.parseInt(st.nextToken());return new IntegerRange(from, to);}
public int from() {return from;}
public int to() {return to;}
public boolean proceed() {return proceed;}
private Options buildOptions() {Option compress = OptionBuilder.withLongOpt("compress").withArgName("[none|lzo|gz]").hasArg().withDescription("compression scheme").create('c');Option fileSize = OptionBuilder.withLongOpt("file-size").withArgName("size-in-MB").hasArg().withDescription("target size of the file (in MB).").create('s');Option fsInputBufferSz = OptionBuilder.withLongOpt("fs-input-buffer").withArgName("size").hasArg().withDescription("size of the file system input buffer (in bytes).").create('i');Option fsOutputBufferSize = OptionBuilder.withLongOpt("fs-output-buffer").withArgName("size").hasArg().withDescription("size of the file system output buffer (in bytes).").create('o');Option keyLen = OptionBuilder.withLongOpt("key-length").withArgName("min,max").hasArg().withDescription("the length range of the key (in bytes)").create('k');Option valueLen = OptionBuilder.withLongOpt("value-length").withArgName("min,max").hasArg().withDescription("the length range of the value (in bytes)").create('v');Option blockSz = OptionBuilder.withLongOpt("block").withArgName("size-in-KB").hasArg().withDescription("minimum block size (in KB)").create('b');Option seed = OptionBuilder.withLongOpt("seed").withArgName("long-int").hasArg().withDescription("specify the seed").create('S');Option operation = OptionBuilder.withLongOpt("operation").withArgName("r|w|rw").hasArg().withDescription("action: seek-only, create-only, seek-after-create").create('x');Option rootDir = OptionBuilder.withLongOpt("root-dir").withArgName("path").hasArg().withDescription("specify root directory where files will be created.").create('r');Option file = OptionBuilder.withLongOpt("file").withArgName("name").hasArg().withDescription("specify the file name to be created or read.").create('f');Option seekCount = OptionBuilder.withLongOpt("seek").withArgName("count").hasArg().withDescription("specify how many seek operations we perform (requires -x r or -x rw.").create('n');Option help = OptionBuilder.withLongOpt("help").hasArg(false).withDescription("show this screen").create("h");return new Options().addOption(compress).addOption(fileSize).addOption(fsInputBufferSz).addOption(fsOutputBufferSize).addOption(keyLen).addOption(blockSz).addOption(rootDir).addOption(valueLen).addOption(operation).addOption(seekCount).addOption(file).addOption(help);}
private void processOptions(CommandLine line, Options opts) {// --help -h and --version -V must be processed first.if (line.hasOption('h')) {HelpFormatter formatter = new HelpFormatter();System.out.println("TFile and SeqFile benchmark.");System.out.println();formatter.printHelp(100, "java ... TestTFileSeqFileComparison [options]", "\nSupported options:", opts, "");return;}if (line.hasOption('c')) {compress = line.getOptionValue('c');}if (line.hasOption('d')) {dictSize = Integer.parseInt(line.getOptionValue('d'));}if (line.hasOption('s')) {fileSize = Long.parseLong(line.getOptionValue('s')) * 1024 * 1024;}if (line.hasOption('i')) {fsInputBufferSize = Integer.parseInt(line.getOptionValue('i'));}if (line.hasOption('o')) {fsOutputBufferSize = Integer.parseInt(line.getOptionValue('o'));}if (line.hasOption('n')) {seekCount = Integer.parseInt(line.getOptionValue('n'));}if (line.hasOption('k')) {IntegerRange ir = IntegerRange.parse(line.getOptionValue('k'));minKeyLen = ir.from();maxKeyLen = ir.to();}if (line.hasOption('v')) {IntegerRange ir = IntegerRange.parse(line.getOptionValue('v'));minValLength = ir.from();maxValLength = ir.to();}if (line.hasOption('b')) {minBlockSize = Integer.parseInt(line.getOptionValue('b')) * 1024;}if (line.hasOption('r')) {rootDir = line.getOptionValue('r');}if (line.hasOption('f')) {file = line.getOptionValue('f');}if (line.hasOption('S')) {seed = Long.parseLong(line.getOptionValue('S'));}if (line.hasOption('x')) {String strOp = line.getOptionValue('x');if (strOp.equals("r")) {op = OP_READ;} else if (strOp.equals("w")) {op = OP_CREATE;} else if (strOp.equals("rw")) {op = OP_CREATE | OP_READ;} else {throw new ParseException("Unknown action specifier: " + strOp);}}proceed = true;}
private void validateOptions() {if (!compress.equals("none") && !compress.equals("lzo") && !compress.equals("gz")) {throw new ParseException("Unknown compression scheme: " + compress);}if (minKeyLen >= maxKeyLen) {throw new ParseException("Max key length must be greater than min key length.");}if (minValLength >= maxValLength) {throw new ParseException("Max value length must be greater than min value length.");}if (minWordLen >= maxWordLen) {throw new ParseException("Max word length must be greater than min word length.");}return;}
private void setStopProceed() {proceed = false;}
public boolean doCreate() {return (op & OP_CREATE) != 0;}
public boolean doRead() {return (op & OP_READ) != 0;}
public static void main(String[] argv) {TestTFileSeek testCase = new TestTFileSeek();MyOptions options = new MyOptions(argv);if (options.proceed == false) {return;}testCase.options = options;testCase.setUp();testCase.testSeeks();testCase.tearDown();}private static Unsafe safetyDance() {try {Field f = Unsafe.class.getDeclaredField("theUnsafe");f.setAccessible(true);return (Unsafe) f.get(null);} catch (Throwable e) {LOG.error("failed to load misc.Unsafe", e);}return null;}
private static int getUsableLength(FileInputStream stream) {int intSize = Ints.checkedCast(stream.getChannel().size());int slots = intSize / BYTES_PER_SLOT;if (slots == 0) {throw new IOException("size of shared memory segment was " + intSize + ", but that is not enough to hold even one slot.");}return slots * BYTES_PER_SLOT;}
public static ShmId createRandom() {return new ShmId(random.nextLong(), random.nextLong());}
public long getHi() {return hi;}
public long getLo() {return lo;}
public boolean equals(Object o) {if ((o == null) || (o.getClass() != this.getClass())) {return false;}ShmId other = (ShmId) o;return new EqualsBuilder().append(hi, other.hi).append(lo, other.lo).isEquals();}
public int hashCode() {return new HashCodeBuilder().append(this.hi).append(this.lo).toHashCode();}
public String toString() {return String.format("%016x%016x", hi, lo);}
public int compareTo(ShmId other) {return ComparisonChain.start().compare(hi, other.hi).compare(lo, other.lo).result();}
public ShmId getShmId() {return shmId;}
public int getSlotIdx() {return slotIdx;}
public boolean equals(Object o) {if ((o == null) || (o.getClass() != this.getClass())) {return false;}SlotId other = (SlotId) o;return new EqualsBuilder().append(shmId, other.shmId).append(slotIdx, other.slotIdx).isEquals();}
public int hashCode() {return new HashCodeBuilder().append(this.shmId).append(this.slotIdx).toHashCode();}
public String toString() {return String.format("SlotId(%s:%d)", shmId.toString(), slotIdx);}
public boolean hasNext() {synchronized (ShortCircuitShm.this) {return allocatedSlots.nextSetBit(slotIdx + 1) != -1;}}
public Slot next() {synchronized (ShortCircuitShm.this) {int nextSlotIdx = allocatedSlots.nextSetBit(slotIdx + 1);if (nextSlotIdx == -1) {throw new NoSuchElementException();}slotIdx = nextSlotIdx;return slots[nextSlotIdx];}}
public void remove() {throw new UnsupportedOperationException("SlotIterator " + "doesn't support removal");}
public ShortCircuitShm getShm() {return ShortCircuitShm.this;}
public ExtendedBlockId getBlockId() {return blockId;}
public SlotId getSlotId() {return new SlotId(getShmId(), getSlotIdx());}
public int getSlotIdx() {return Ints.checkedCast((slotAddress - baseAddress) / BYTES_PER_SLOT);}
void clear() {unsafe.putLongVolatile(null, this.slotAddress, 0);}
private boolean isSet(long flag) {long prev = unsafe.getLongVolatile(null, this.slotAddress);return (prev & flag) != 0;}
private void setFlag(long flag) {long prev;do {prev = unsafe.getLongVolatile(null, this.slotAddress);if ((prev & flag) != 0) {return;}} while (!unsafe.compareAndSwapLong(null, this.slotAddress, prev, prev | flag));}
private void clearFlag(long flag) {long prev;do {prev = unsafe.getLongVolatile(null, this.slotAddress);if ((prev & flag) == 0) {return;}} while (!unsafe.compareAndSwapLong(null, this.slotAddress, prev, prev & (~flag)));}
public boolean isValid() {return isSet(VALID_FLAG);}
public void makeValid() {setFlag(VALID_FLAG);}
public void makeInvalid() {clearFlag(VALID_FLAG);}
public boolean isAnchorable() {return isSet(ANCHORABLE_FLAG);}
public void makeAnchorable() {setFlag(ANCHORABLE_FLAG);}
public void makeUnanchorable() {clearFlag(ANCHORABLE_FLAG);}
public boolean isAnchored() {long prev = unsafe.getLongVolatile(null, this.slotAddress);if ((prev & VALID_FLAG) == 0) {// Slot is no longer valid.return false;}return ((prev & 0x7fffffff) != 0);}
public boolean addAnchor() {long prev;do {prev = unsafe.getLongVolatile(null, this.slotAddress);if ((prev & VALID_FLAG) == 0) {// Slot is no longer valid.return false;}if ((prev & ANCHORABLE_FLAG) == 0) {// Slot can't be anchored right now.return false;}if ((prev & 0x7fffffff) == 0x7fffffff) {// Too many other threads have anchored the slot (2 billion?)return false;}} while (!unsafe.compareAndSwapLong(null, this.slotAddress, prev, prev + 1));return true;}
public void removeAnchor() {long prev;do {prev = unsafe.getLongVolatile(null, this.slotAddress);Preconditions.checkState((prev & 0x7fffffff) != 0, "Tried to remove anchor for slot " + slotAddress + ", which was " + "not anchored.");} while (!unsafe.compareAndSwapLong(null, this.slotAddress, prev, prev - 1));}
public String toString() {return "Slot(slotIdx=" + getSlotIdx() + ", shm=" + getShm() + ")";}
public final ShmId getShmId() {return shmId;}
public final synchronized boolean isEmpty() {return allocatedSlots.nextSetBit(0) == -1;}
public final synchronized boolean isFull() {return allocatedSlots.nextClearBit(0) >= slots.length;}
private final long calculateSlotAddress(int slotIdx) {long offset = slotIdx;offset *= BYTES_PER_SLOT;return this.baseAddress + offset;}
public final synchronized Slot allocAndRegisterSlot(ExtendedBlockId blockId) {int idx = allocatedSlots.nextClearBit(0);if (idx >= slots.length) {throw new RuntimeException(this + ": no more slots are available.");}allocatedSlots.set(idx, true);Slot slot = new Slot(calculateSlotAddress(idx), blockId);slot.clear();slot.makeValid();slots[idx] = slot;if (LOG.isTraceEnabled()) {LOG.trace(this + ": allocAndRegisterSlot " + idx + ": allocatedSlots=" + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()));}return slot;}
public final synchronized Slot getSlot(int slotIdx) {if (!allocatedSlots.get(slotIdx)) {throw new InvalidRequestException(this + ": slot " + slotIdx + " does not exist.");}return slots[slotIdx];}
public final synchronized Slot registerSlot(int slotIdx, ExtendedBlockId blockId) {if (slotIdx < 0) {throw new InvalidRequestException(this + ": invalid negative slot " + "index " + slotIdx);}if (slotIdx >= slots.length) {throw new InvalidRequestException(this + ": invalid slot " + "index " + slotIdx);}if (allocatedSlots.get(slotIdx)) {throw new InvalidRequestException(this + ": slot " + slotIdx + " is already in use.");}Slot slot = new Slot(calculateSlotAddress(slotIdx), blockId);if (!slot.isValid()) {throw new InvalidRequestException(this + ": slot " + slotIdx + " is not marked as valid.");}slots[slotIdx] = slot;allocatedSlots.set(slotIdx, true);if (LOG.isTraceEnabled()) {LOG.trace(this + ": registerSlot " + slotIdx + ": allocatedSlots=" + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()));}return slot;}
public final synchronized void unregisterSlot(int slotIdx) {Preconditions.checkState(allocatedSlots.get(slotIdx), "tried to unregister slot " + slotIdx + ", which was not registered.");allocatedSlots.set(slotIdx, false);slots[slotIdx] = null;if (LOG.isTraceEnabled()) {LOG.trace(this + ": unregisterSlot " + slotIdx);}}
public SlotIterator slotIterator() {return new SlotIterator();}
public void free() {try {POSIX.munmap(baseAddress, mmappedLength);} catch (IOException e) {LOG.warn(this + ": failed to munmap", e);}LOG.trace(this + ": freed");}
public String toString() {return this.getClass().getSimpleName() + "(" + shmId + ")";}public void testPersist() {Configuration conf = new Configuration();testPersistHelper(conf);}
public void testCompression() {Configuration conf = new Configuration();conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY, true);conf.set(DFSConfigKeys.DFS_IMAGE_COMPRESSION_CODEC_KEY, "org.apache.hadoop.io.compress.GzipCodec");testPersistHelper(conf);}
private void testPersistHelper(Configuration conf) {MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).build();cluster.waitActive();FSNamesystem fsn = cluster.getNamesystem();DistributedFileSystem fs = cluster.getFileSystem();final Path dir = new Path("/abc/def");final Path file1 = new Path(dir, "f1");final Path file2 = new Path(dir, "f2");fs.create(file1).close();FSDataOutputStream out = fs.create(file2);out.writeBytes("hello");((DFSOutputStream) out.getWrappedStream()).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);fs.saveNamespace();fs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);cluster.restartNameNode();cluster.waitActive();fs = cluster.getFileSystem();assertTrue(fs.isDirectory(dir));assertTrue(fs.exists(file1));assertTrue(fs.exists(file2));INodeFile file2Node = fsn.dir.getINode4Write(file2.toString()).asFile();assertEquals("hello".length(), file2Node.computeFileSize());assertTrue(file2Node.isUnderConstruction());BlockInfo[] blks = file2Node.getBlocks();assertEquals(1, blks.length);assertEquals(BlockUCState.UNDER_CONSTRUCTION, blks[0].getBlockUCState());// check lease managerLease lease = fsn.leaseManager.getLeaseByPath(file2.toString());Assert.assertNotNull(lease);} finally {if (cluster != null) {cluster.shutdown();}}}
public void testDigest() {Configuration conf = new Configuration();MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();DistributedFileSystem fs = cluster.getFileSystem();fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);fs.saveNamespace();fs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);File currentDir = FSImageTestUtil.getNameNodeCurrentDirs(cluster, 0).get(0);File fsimage = FSImageTestUtil.findNewestImageFile(currentDir.getAbsolutePath());assertEquals(MD5FileUtils.readStoredMd5ForFile(fsimage), MD5FileUtils.computeMd5ForFile(fsimage));} finally {if (cluster != null) {cluster.shutdown();}}}
public void testLoadMtimeAtime() {Configuration conf = new Configuration();MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();cluster.waitActive();DistributedFileSystem hdfs = cluster.getFileSystem();String userDir = hdfs.getHomeDirectory().toUri().getPath().toString();Path file = new Path(userDir, "file");Path dir = new Path(userDir, "/dir");Path link = new Path(userDir, "/link");hdfs.createNewFile(file);hdfs.mkdirs(dir);hdfs.createSymlink(file, link, false);long mtimeFile = hdfs.getFileStatus(file).getModificationTime();long atimeFile = hdfs.getFileStatus(file).getAccessTime();long mtimeDir = hdfs.getFileStatus(dir).getModificationTime();long mtimeLink = hdfs.getFileLinkStatus(link).getModificationTime();long atimeLink = hdfs.getFileLinkStatus(link).getAccessTime();// save namespace and restart clusterhdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);hdfs.saveNamespace();hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);cluster.shutdown();cluster = new MiniDFSCluster.Builder(conf).format(false).numDataNodes(1).build();cluster.waitActive();hdfs = cluster.getFileSystem();assertEquals(mtimeFile, hdfs.getFileStatus(file).getModificationTime());assertEquals(atimeFile, hdfs.getFileStatus(file).getAccessTime());assertEquals(mtimeDir, hdfs.getFileStatus(dir).getModificationTime());assertEquals(mtimeLink, hdfs.getFileLinkStatus(link).getModificationTime());assertEquals(atimeLink, hdfs.getFileLinkStatus(link).getAccessTime());} finally {if (cluster != null) {cluster.shutdown();}}}protected void render(Block html) {//CSS in the correct spothtml.style(".metrics {margin-bottom:5px}");ClusterMetricsInfo clusterMetrics = new ClusterMetricsInfo(this.rm, this.rmContext);DIV<Hamlet> div = html.div().$class("metrics");div.h3("Cluster Metrics").table("#metricsoverview").thead().$class("ui-widget-header").tr().th().$class("ui-state-default")._("Apps Submitted")._().th().$class("ui-state-default")._("Apps Pending")._().th().$class("ui-state-default")._("Apps Running")._().th().$class("ui-state-default")._("Apps Completed")._().th().$class("ui-state-default")._("Containers Running")._().th().$class("ui-state-default")._("Memory Used")._().th().$class("ui-state-default")._("Memory Total")._().th().$class("ui-state-default")._("Memory Reserved")._().th().$class("ui-state-default")._("VCores Used")._().th().$class("ui-state-default")._("VCores Total")._().th().$class("ui-state-default")._("VCores Reserved")._().th().$class("ui-state-default")._("Active Nodes")._().th().$class("ui-state-default")._("Decommissioned Nodes")._().th().$class("ui-state-default")._("Lost Nodes")._().th().$class("ui-state-default")._("Unhealthy Nodes")._().th().$class("ui-state-default")._("Rebooted Nodes")._()._()._().tbody().$class("ui-widget-content").tr().td(String.valueOf(clusterMetrics.getAppsSubmitted())).td(String.valueOf(clusterMetrics.getAppsPending())).td(String.valueOf(clusterMetrics.getAppsRunning())).td(String.valueOf(clusterMetrics.getAppsCompleted() + clusterMetrics.getAppsFailed() + clusterMetrics.getAppsKilled())).td(String.valueOf(clusterMetrics.getContainersAllocated())).td(StringUtils.byteDesc(clusterMetrics.getAllocatedMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(clusterMetrics.getTotalMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(clusterMetrics.getReservedMB() * BYTES_IN_MB)).td(String.valueOf(clusterMetrics.getAllocatedVirtualCores())).td(String.valueOf(clusterMetrics.getTotalVirtualCores())).td(String.valueOf(clusterMetrics.getReservedVirtualCores())).td().a(url("nodes"), String.valueOf(clusterMetrics.getActiveNodes()))._().td().a(url("nodes/decommissioned"), String.valueOf(clusterMetrics.getDecommissionedNodes()))._().td().a(url("nodes/lost"), String.valueOf(clusterMetrics.getLostNodes()))._().td().a(url("nodes/unhealthy"), String.valueOf(clusterMetrics.getUnhealthyNodes()))._().td().a(url("nodes/rebooted"), String.valueOf(clusterMetrics.getRebootedNodes()))._()._()._()._();String user = request().getRemoteUser();if (user != null) {UserMetricsInfo userMetrics = new UserMetricsInfo(this.rm, this.rmContext, user);if (userMetrics.metricsAvailable()) {div.h3("User Metrics for " + user).table("#usermetricsoverview").thead().$class("ui-widget-header").tr().th().$class("ui-state-default")._("Apps Submitted")._().th().$class("ui-state-default")._("Apps Pending")._().th().$class("ui-state-default")._("Apps Running")._().th().$class("ui-state-default")._("Apps Completed")._().th().$class("ui-state-default")._("Containers Running")._().th().$class("ui-state-default")._("Containers Pending")._().th().$class("ui-state-default")._("Containers Reserved")._().th().$class("ui-state-default")._("Memory Used")._().th().$class("ui-state-default")._("Memory Pending")._().th().$class("ui-state-default")._("Memory Reserved")._().th().$class("ui-state-default")._("VCores Used")._().th().$class("ui-state-default")._("VCores Pending")._().th().$class("ui-state-default")._("VCores Reserved")._()._()._().tbody().$class("ui-widget-content").tr().td(String.valueOf(userMetrics.getAppsSubmitted())).td(String.valueOf(userMetrics.getAppsPending())).td(String.valueOf(userMetrics.getAppsRunning())).td(String.valueOf((userMetrics.getAppsCompleted() + userMetrics.getAppsFailed() + userMetrics.getAppsKilled()))).td(String.valueOf(userMetrics.getRunningContainers())).td(String.valueOf(userMetrics.getPendingContainers())).td(String.valueOf(userMetrics.getReservedContainers())).td(StringUtils.byteDesc(userMetrics.getAllocatedMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(userMetrics.getPendingMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(userMetrics.getReservedMB() * BYTES_IN_MB)).td(String.valueOf(userMetrics.getAllocatedVirtualCores())).td(String.valueOf(userMetrics.getPendingVirtualCores())).td(String.valueOf(userMetrics.getReservedVirtualCores()))._()._()._();}}div._();}public void testPendingReplication() {PendingReplicationBlocks pendingReplications;pendingReplications = new PendingReplicationBlocks(TIMEOUT * 1000);pendingReplications.start();//DatanodeStorageInfo[] storages = DFSTestUtil.createDatanodeStorageInfos(10);for (int i = 0; i < storages.length; i++) {Block block = new Block(i, i, 0);DatanodeStorageInfo[] targets = new DatanodeStorageInfo[i];System.arraycopy(storages, 0, targets, 0, i);pendingReplications.increment(block, DatanodeStorageInfo.toDatanodeDescriptors(targets));}assertEquals("Size of pendingReplications ", 10, pendingReplications.size());Block blk = new Block(8, 8, 0);pendingReplications.decrement(blk, storages[7].getDatanodeDescriptor());assertEquals("pendingReplications.getNumReplicas ", 7, pendingReplications.getNumReplicas(blk));for (int i = 0; i < 7; i++) {pendingReplications.decrement(blk, storages[i].getDatanodeDescriptor());}assertTrue(pendingReplications.size() == 9);pendingReplications.increment(blk, DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(8)));assertTrue(pendingReplications.size() == 10);for (int i = 0; i < 10; i++) {Block block = new Block(i, i, 0);int numReplicas = pendingReplications.getNumReplicas(block);assertTrue(numReplicas == i);}assertTrue(pendingReplications.getTimedOutBlocks() == null);try {Thread.sleep(1000);} catch (Exception e) {}for (int i = 10; i < 15; i++) {Block block = new Block(i, i, 0);pendingReplications.increment(block, DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(i)));}assertTrue(pendingReplications.size() == 15);int loop = 0;while (pendingReplications.size() > 0) {try {Thread.sleep(1000);} catch (Exception e) {}loop++;}System.out.println("Had to wait for " + loop + " seconds for the lot to timeout");assertEquals("Size of pendingReplications ", 0, pendingReplications.size());Block[] timedOut = pendingReplications.getTimedOutBlocks();assertTrue(timedOut != null && timedOut.length == 15);for (int i = 0; i < timedOut.length; i++) {assertTrue(timedOut[i].getBlockId() < 15);}pendingReplications.stop();}
public void testBlockReceived() {final Configuration conf = new HdfsConfiguration();conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 1024);MiniDFSCluster cluster = null;try {cluster = new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_COUNT).build();cluster.waitActive();DistributedFileSystem hdfs = cluster.getFileSystem();FSNamesystem fsn = cluster.getNamesystem();BlockManager blkManager = fsn.getBlockManager();final String file = "/tmp.txt";final Path filePath = new Path(file);short replFactor = 1;DFSTestUtil.createFile(hdfs, filePath, 1024L, replFactor, 0);ArrayList<DataNode> datanodes = cluster.getDataNodes();for (int i = 0; i < DATANODE_COUNT; i++) {DataNodeTestUtils.setHeartbeatsDisabledForTests(datanodes.get(i), true);}hdfs.setReplication(filePath, (short) DATANODE_COUNT);BlockManagerTestUtil.computeAllPendingWork(blkManager);assertEquals(1, blkManager.pendingReplications.size());INodeFile fileNode = fsn.getFSDirectory().getINode4Write(file).asFile();Block[] blocks = fileNode.getBlocks();assertEquals(DATANODE_COUNT - 1, blkManager.pendingReplications.getNumReplicas(blocks[0]));LocatedBlock locatedBlock = hdfs.getClient().getLocatedBlocks(file, 0).get(0);DatanodeInfo existingDn = (locatedBlock.getLocations())[0];int reportDnNum = 0;String poolId = cluster.getNamesystem().getBlockPoolId();// report to NNfor (int i = 0; i < DATANODE_COUNT && reportDnNum < 2; i++) {if (!datanodes.get(i).getDatanodeId().equals(existingDn)) {DatanodeRegistration dnR = datanodes.get(i).getDNRegistrationForBP(poolId);StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks("Fake-storage-ID-Ignored", new ReceivedDeletedBlockInfo[] { new ReceivedDeletedBlockInfo(blocks[0], BlockStatus.RECEIVED_BLOCK, "") }) };cluster.getNameNodeRpc().blockReceivedAndDeleted(dnR, poolId, report);reportDnNum++;}}assertEquals(DATANODE_COUNT - 3, blkManager.pendingReplications.getNumReplicas(blocks[0]));// let the same datanodes report againfor (int i = 0; i < DATANODE_COUNT && reportDnNum < 2; i++) {if (!datanodes.get(i).getDatanodeId().equals(existingDn)) {DatanodeRegistration dnR = datanodes.get(i).getDNRegistrationForBP(poolId);StorageReceivedDeletedBlocks[] report = { new StorageReceivedDeletedBlocks("Fake-storage-ID-Ignored", new ReceivedDeletedBlockInfo[] { new ReceivedDeletedBlockInfo(blocks[0], BlockStatus.RECEIVED_BLOCK, "") }) };cluster.getNameNodeRpc().blockReceivedAndDeleted(dnR, poolId, report);reportDnNum++;}}assertEquals(DATANODE_COUNT - 3, blkManager.pendingReplications.getNumReplicas(blocks[0]));// re-enable heartbeat for the datanode that has datafor (int i = 0; i < DATANODE_COUNT; i++) {DataNodeTestUtils.setHeartbeatsDisabledForTests(datanodes.get(i), false);DataNodeTestUtils.triggerHeartbeat(datanodes.get(i));}Thread.sleep(5000);assertEquals(0, blkManager.pendingReplications.size());} finally {if (cluster != null) {cluster.shutdown();}}}
public void testPendingAndInvalidate() {final Configuration CONF = new HdfsConfiguration();CONF.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 1024);CONF.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, DFS_REPLICATION_INTERVAL);CONF.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, DFS_REPLICATION_INTERVAL);MiniDFSCluster cluster = new MiniDFSCluster.Builder(CONF).numDataNodes(DATANODE_COUNT).build();cluster.waitActive();FSNamesystem namesystem = cluster.getNamesystem();BlockManager bm = namesystem.getBlockManager();DistributedFileSystem fs = cluster.getFileSystem();try {// 1. create a filePath filePath = new Path("/tmp.txt");DFSTestUtil.createFile(fs, filePath, 1024, (short) 3, 0L);// 2. disable the heartbeatsfor (DataNode dn : cluster.getDataNodes()) {DataNodeTestUtils.setHeartbeatsDisabledForTests(dn, true);}// 3. mark a couple of blocks as corruptLocatedBlock block = NameNodeAdapter.getBlockLocations(cluster.getNameNode(), filePath.toString(), 0, 1).get(0);cluster.getNamesystem().writeLock();try {bm.findAndMarkBlockAsCorrupt(block.getBlock(), block.getLocations()[0], "STORAGE_ID", "TEST");bm.findAndMarkBlockAsCorrupt(block.getBlock(), block.getLocations()[1], "STORAGE_ID", "TEST");} finally {cluster.getNamesystem().writeUnlock();}BlockManagerTestUtil.computeAllPendingWork(bm);BlockManagerTestUtil.updateState(bm);assertEquals(bm.getPendingReplicationBlocksCount(), 1L);assertEquals(bm.pendingReplications.getNumReplicas(block.getBlock().getLocalBlock()), 2);// 4. delete the filefs.delete(filePath, true);// less than the default pending record timeout (5~10min)int retries = 10;long pendingNum = bm.getPendingReplicationBlocksCount();while (pendingNum != 0 && retries-- > 0) {// let NN do the deletionThread.sleep(1000);BlockManagerTestUtil.updateState(bm);pendingNum = bm.getPendingReplicationBlocksCount();}assertEquals(pendingNum, 0L);} finally {cluster.shutdown();}}public TaskType getType() {return taskType;}
protected TaskAttemptImpl createAttempt() {MockTaskAttemptImpl attempt = new MockTaskAttemptImpl(getID(), ++taskAttemptCounter, eventHandler, taskAttemptListener, remoteJobConfFile, partition, conf, jobToken, credentials, clock, appContext, taskType);taskAttempts.add(attempt);return attempt;}
protected int getMaxAttempts() {return 100;}
protected void internalError(TaskEventType type) {super.internalError(type);fail("Internal error: " + type);}
public TaskAttemptId getAttemptId() {return getID();}
protected Task createRemoteTask() {return new MockTask(taskType);}
public float getProgress() {return progress;}
public void setProgress(float progress) {this.progress = progress;}
public void setState(TaskAttemptState state) {this.state = state;}
public TaskAttemptState getState() {return state;}
public Counters getCounters() {return attemptCounters;}
public void setCounters(Counters counters) {attemptCounters = counters;}
public void run(JobConf job, TaskUmbilicalProtocol umbilical) {return;}
public boolean isMapTask() {return (taskType == TaskType.MAP);}
public void setup() {dispatcher = new InlineDispatcher();++startCount;conf = new JobConf();taskAttemptListener = mock(TaskAttemptListener.class);jobToken = (Token<JobTokenIdentifier>) mock(Token.class);remoteJobConfFile = mock(Path.class);credentials = null;clock = new SystemClock();metrics = mock(MRAppMetrics.class);dataLocations = new String[1];appId = ApplicationId.newInstance(System.currentTimeMillis(), 1);jobId = Records.newRecord(JobId.class);jobId.setId(1);jobId.setAppId(appId);appContext = mock(AppContext.class);taskSplitMetaInfo = mock(TaskSplitMetaInfo.class);when(taskSplitMetaInfo.getLocations()).thenReturn(dataLocations);taskAttempts = new ArrayList<MockTaskAttemptImpl>();}
private MockTaskImpl createMockTask(TaskType taskType) {return new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, taskType);}
public void teardown() {taskAttempts.clear();}
private TaskId getNewTaskID() {TaskId taskId = Records.newRecord(TaskId.class);taskId.setId(++taskCounter);taskId.setJobId(jobId);taskId.setTaskType(mockTask.getType());return taskId;}
private void scheduleTaskAttempt(TaskId taskId) {mockTask.handle(new TaskEvent(taskId, TaskEventType.T_SCHEDULE));assertTaskScheduledState();assertTaskAttemptAvataar(Avataar.VIRGIN);}
private void killTask(TaskId taskId) {mockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));assertTaskKillWaitState();}
private void killScheduledTaskAttempt(TaskAttemptId attemptId) {mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_KILLED));assertTaskScheduledState();}
private void launchTaskAttempt(TaskAttemptId attemptId) {mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_LAUNCHED));assertTaskRunningState();}
private void commitTaskAttempt(TaskAttemptId attemptId) {mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_COMMIT_PENDING));assertTaskRunningState();}
private MockTaskAttemptImpl getLastAttempt() {return taskAttempts.get(taskAttempts.size() - 1);}
private void updateLastAttemptProgress(float p) {getLastAttempt().setProgress(p);}
private void updateLastAttemptState(TaskAttemptState s) {getLastAttempt().setState(s);}
private void killRunningTaskAttempt(TaskAttemptId attemptId) {mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_KILLED));assertTaskRunningState();}
private void failRunningTaskAttempt(TaskAttemptId attemptId) {mockTask.handle(new TaskTAttemptEvent(attemptId, TaskEventType.T_ATTEMPT_FAILED));assertTaskRunningState();}
private void assertTaskNewState() {assertEquals(TaskState.NEW, mockTask.getState());}
private void assertTaskScheduledState() {assertEquals(TaskState.SCHEDULED, mockTask.getState());}
private void assertTaskRunningState() {assertEquals(TaskState.RUNNING, mockTask.getState());}
private void assertTaskKillWaitState() {assertEquals(TaskStateInternal.KILL_WAIT, mockTask.getInternalState());}
private void assertTaskSucceededState() {assertEquals(TaskState.SUCCEEDED, mockTask.getState());}
private void assertTaskAttemptAvataar(Avataar avataar) {for (TaskAttempt taskAttempt : mockTask.getAttempts().values()) {if (((TaskAttemptImpl) taskAttempt).getAvataar() == avataar) {return;}}fail("There is no " + (avataar == Avataar.VIRGIN ? "virgin" : "speculative") + "task attempt");}
public void testInit() {LOG.info("--- START: testInit ---");mockTask = createMockTask(TaskType.MAP);assertTaskNewState();assert (taskAttempts.size() == 0);}
public /**   * {@link TaskState#NEW}->{@link TaskState#SCHEDULED}   */void testScheduleTask() {LOG.info("--- START: testScheduleTask ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);}
public /**   * {@link TaskState#SCHEDULED}->{@link TaskState#KILL_WAIT}   */void testKillScheduledTask() {LOG.info("--- START: testKillScheduledTask ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);killTask(taskId);}
public /**   * Kill attempt   * {@link TaskState#SCHEDULED}->{@link TaskState#SCHEDULED}   */void testKillScheduledTaskAttempt() {LOG.info("--- START: testKillScheduledTaskAttempt ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);killScheduledTaskAttempt(getLastAttempt().getAttemptId());}
public /**   * Launch attempt   * {@link TaskState#SCHEDULED}->{@link TaskState#RUNNING}   */void testLaunchTaskAttempt() {LOG.info("--- START: testLaunchTaskAttempt ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());}
public /**   * Kill running attempt   * {@link TaskState#RUNNING}->{@link TaskState#RUNNING}*/void testKillRunningTaskAttempt() {LOG.info("--- START: testKillRunningTaskAttempt ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());killRunningTaskAttempt(getLastAttempt().getAttemptId());}
public void testKillSuccessfulTask() {LOG.info("--- START: testKillSuccesfulTask ---");mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());commitTaskAttempt(getLastAttempt().getAttemptId());mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));assertTaskSucceededState();mockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));assertTaskSucceededState();}
public void testTaskProgress() {LOG.info("--- START: testTaskProgress ---");mockTask = createMockTask(TaskType.MAP);// launch taskTaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);float progress = 0f;assert (mockTask.getProgress() == progress);launchTaskAttempt(getLastAttempt().getAttemptId());// update attempt1 progress = 50f;updateLastAttemptProgress(progress);assert (mockTask.getProgress() == progress);progress = 100f;updateLastAttemptProgress(progress);assert (mockTask.getProgress() == progress);progress = 0f;// mark first attempt as killedupdateLastAttemptState(TaskAttemptState.KILLED);assert (mockTask.getProgress() == progress);// as no successful attempts killRunningTaskAttempt(getLastAttempt().getAttemptId());assert (taskAttempts.size() == 2);assert (mockTask.getProgress() == 0f);launchTaskAttempt(getLastAttempt().getAttemptId());progress = 50f;updateLastAttemptProgress(progress);assert (mockTask.getProgress() == progress);}
public void testKillDuringTaskAttemptCommit() {mockTask = createMockTask(TaskType.REDUCE);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());updateLastAttemptState(TaskAttemptState.COMMIT_PENDING);commitTaskAttempt(getLastAttempt().getAttemptId());TaskAttemptId commitAttempt = getLastAttempt().getAttemptId();updateLastAttemptState(TaskAttemptState.KILLED);killRunningTaskAttempt(commitAttempt);assertFalse(mockTask.canCommit(commitAttempt));}
public void testFailureDuringTaskAttemptCommit() {mockTask = createMockTask(TaskType.MAP);TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());updateLastAttemptState(TaskAttemptState.COMMIT_PENDING);commitTaskAttempt(getLastAttempt().getAttemptId());// the attempt to failupdateLastAttemptState(TaskAttemptState.FAILED);failRunningTaskAttempt(getLastAttempt().getAttemptId());assertEquals(2, taskAttempts.size());updateLastAttemptState(TaskAttemptState.SUCCEEDED);commitTaskAttempt(getLastAttempt().getAttemptId());mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));assertFalse("First attempt should not commit", mockTask.canCommit(taskAttempts.get(0).getAttemptId()));assertTrue("Second attempt should commit", mockTask.canCommit(getLastAttempt().getAttemptId()));assertTaskSucceededState();}
private void runSpeculativeTaskAttemptSucceeds(TaskEventType firstAttemptFinishEvent) {TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());updateLastAttemptState(TaskAttemptState.RUNNING);// Add a speculative task attempt that succeedsmockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));launchTaskAttempt(getLastAttempt().getAttemptId());commitTaskAttempt(getLastAttempt().getAttemptId());mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));// The task should now have succeededassertTaskSucceededState();// Now complete the first task attempt, after the second has succeededmockTask.handle(new TaskTAttemptEvent(taskAttempts.get(0).getAttemptId(), firstAttemptFinishEvent));// The task should still be in the succeeded stateassertTaskSucceededState();// The task should contain speculative a task attemptassertTaskAttemptAvataar(Avataar.SPECULATIVE);}
public void testMapSpeculativeTaskAttemptSucceedsEvenIfFirstFails() {mockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);}
public void testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstFails() {mockTask = createMockTask(TaskType.REDUCE);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);}
public void testMapSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled() {mockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);}
public void testReduceSpeculativeTaskAttemptSucceedsEvenIfFirstIsKilled() {mockTask = createMockTask(TaskType.REDUCE);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);}
public void testMultipleTaskAttemptsSucceed() {mockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_SUCCEEDED);}
public void testCommitAfterSucceeds() {mockTask = createMockTask(TaskType.REDUCE);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_COMMIT_PENDING);}
public void testSpeculativeMapFetchFailure() {// Setup a scenario where speculative task wins, first attempt killedmockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_KILLED);assertEquals(2, taskAttempts.size());// speculative attempt retroactively fails from fetch failuresmockTask.handle(new TaskTAttemptEvent(taskAttempts.get(1).getAttemptId(), TaskEventType.T_ATTEMPT_FAILED));assertTaskScheduledState();assertEquals(3, taskAttempts.size());}
public void testSpeculativeMapMultipleSucceedFetchFailure() {// Setup a scenario where speculative task wins, first attempt succeedsmockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_SUCCEEDED);assertEquals(2, taskAttempts.size());// speculative attempt retroactively fails from fetch failuresmockTask.handle(new TaskTAttemptEvent(taskAttempts.get(1).getAttemptId(), TaskEventType.T_ATTEMPT_FAILED));assertTaskScheduledState();assertEquals(3, taskAttempts.size());}
public void testSpeculativeMapFailedFetchFailure() {// Setup a scenario where speculative task wins, first attempt succeedsmockTask = createMockTask(TaskType.MAP);runSpeculativeTaskAttemptSucceeds(TaskEventType.T_ATTEMPT_FAILED);assertEquals(2, taskAttempts.size());// speculative attempt retroactively fails from fetch failuresmockTask.handle(new TaskTAttemptEvent(taskAttempts.get(1).getAttemptId(), TaskEventType.T_ATTEMPT_FAILED));assertTaskScheduledState();assertEquals(3, taskAttempts.size());}
public void testFailedTransitions() {mockTask = new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, TaskType.MAP) {
@Overrideprotected int getMaxAttempts() {return 1;}};TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());// add three more speculative attemptsmockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));launchTaskAttempt(getLastAttempt().getAttemptId());mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));launchTaskAttempt(getLastAttempt().getAttemptId());mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));launchTaskAttempt(getLastAttempt().getAttemptId());assertEquals(4, taskAttempts.size());// have the first attempt fail, verify task failed due to no retriesMockTaskAttemptImpl taskAttempt = taskAttempts.get(0);taskAttempt.setState(TaskAttemptState.FAILED);mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_FAILED));assertEquals(TaskState.FAILED, mockTask.getState());// verify task can no longer be killedmockTask.handle(new TaskEvent(taskId, TaskEventType.T_KILL));assertEquals(TaskState.FAILED, mockTask.getState());// verify speculative doesn't launch new tasksmockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));mockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ATTEMPT_LAUNCHED));assertEquals(TaskState.FAILED, mockTask.getState());assertEquals(4, taskAttempts.size());// verify attempt events from active tasks don't knock task out of FAILEDtaskAttempt = taskAttempts.get(1);taskAttempt.setState(TaskAttemptState.COMMIT_PENDING);mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_COMMIT_PENDING));assertEquals(TaskState.FAILED, mockTask.getState());taskAttempt.setState(TaskAttemptState.FAILED);mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_FAILED));assertEquals(TaskState.FAILED, mockTask.getState());taskAttempt = taskAttempts.get(2);taskAttempt.setState(TaskAttemptState.SUCCEEDED);mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));assertEquals(TaskState.FAILED, mockTask.getState());taskAttempt = taskAttempts.get(3);taskAttempt.setState(TaskAttemptState.KILLED);mockTask.handle(new TaskTAttemptEvent(taskAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_KILLED));assertEquals(TaskState.FAILED, mockTask.getState());}
protected int getMaxAttempts() {return 1;}
public void testCountersWithSpeculation() {mockTask = new MockTaskImpl(jobId, partition, dispatcher.getEventHandler(), remoteJobConfFile, conf, taskAttemptListener, jobToken, credentials, clock, startCount, metrics, appContext, TaskType.MAP) {
@Overrideprotected int getMaxAttempts() {return 1;}};TaskId taskId = getNewTaskID();scheduleTaskAttempt(taskId);launchTaskAttempt(getLastAttempt().getAttemptId());updateLastAttemptState(TaskAttemptState.RUNNING);MockTaskAttemptImpl baseAttempt = getLastAttempt();// add a speculative attemptmockTask.handle(new TaskTAttemptEvent(getLastAttempt().getAttemptId(), TaskEventType.T_ADD_SPEC_ATTEMPT));launchTaskAttempt(getLastAttempt().getAttemptId());updateLastAttemptState(TaskAttemptState.RUNNING);MockTaskAttemptImpl specAttempt = getLastAttempt();assertEquals(2, taskAttempts.size());Counters specAttemptCounters = new Counters();Counter cpuCounter = specAttemptCounters.findCounter(TaskCounter.CPU_MILLISECONDS);cpuCounter.setValue(1000);specAttempt.setCounters(specAttemptCounters);// have the spec attempt succeed but second attempt at 1.0 progress as wellcommitTaskAttempt(specAttempt.getAttemptId());specAttempt.setProgress(1.0f);specAttempt.setState(TaskAttemptState.SUCCEEDED);mockTask.handle(new TaskTAttemptEvent(specAttempt.getAttemptId(), TaskEventType.T_ATTEMPT_SUCCEEDED));assertEquals(TaskState.SUCCEEDED, mockTask.getState());baseAttempt.setProgress(1.0f);Counters taskCounters = mockTask.getCounters();assertEquals("wrong counters for task", specAttemptCounters, taskCounters);}
protected int getMaxAttempts() {return 1;}public void setUp() {// Generate datafinal int seed = new Random().nextInt();final DataOutputBuffer dataBuf = new DataOutputBuffer();final RandomDatum.Generator generator = new RandomDatum.Generator(seed);for (int i = 0; i < count; ++i) {generator.next();final RandomDatum key = generator.getKey();final RandomDatum value = generator.getValue();key.write(dataBuf);value.write(dataBuf);}LOG.info("Generated " + count + " records");data = dataBuf.getData();dataLen = dataBuf.getLength();}
protected void writeData(OutputStream out) {out.write(data, 0, dataLen);out.close();}
protected int getDataLen() {return dataLen;}
private int readAll(InputStream in, byte[] b, int off, int len) {int n = 0;int total = 0;while (n != -1) {total += n;if (total >= len) {break;}n = in.read(b, off + total, len - total);}return total;}
protected OutputStream getOutputStream(int bufferSize) {return getOutputStream(bufferSize, key, iv);}
protected InputStream getInputStream(int bufferSize) {return getInputStream(bufferSize, key, iv);}
public void testRead() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);// Default buffer sizeInputStream in = getInputStream(defaultBufferSize);readCheck(in);in.close();// Small buffer sizein = getInputStream(smallBufferSize);readCheck(in);in.close();}
private void readCheck(InputStream in) {byte[] result = new byte[dataLen];int n = readAll(in, result, 0, dataLen);Assert.assertEquals(dataLen, n);byte[] expectedData = new byte[n];System.arraycopy(data, 0, expectedData, 0, n);Assert.assertArrayEquals(result, expectedData);// EOFn = in.read(result, 0, dataLen);Assert.assertEquals(n, -1);in.close();}
public void testWrite() {// Default buffer sizewriteCheck(defaultBufferSize);// Small buffer sizewriteCheck(smallBufferSize);}
private void writeCheck(int bufferSize) {OutputStream out = getOutputStream(bufferSize);writeData(out);if (out instanceof FSDataOutputStream) {Assert.assertEquals(((FSDataOutputStream) out).getPos(), getDataLen());}}
public void testCryptoIV() {byte[] iv1 = iv.clone();// Counter base: Long.MAX_VALUEsetCounterBaseForIV(iv1, Long.MAX_VALUE);cryptoCheck(iv1);// Counter base: Long.MAX_VALUE - 1setCounterBaseForIV(iv1, Long.MAX_VALUE - 1);cryptoCheck(iv1);// Counter base: Integer.MAX_VALUEsetCounterBaseForIV(iv1, Integer.MAX_VALUE);cryptoCheck(iv1);// Counter base: 0setCounterBaseForIV(iv1, 0);cryptoCheck(iv1);// Counter base: -1setCounterBaseForIV(iv1, -1);cryptoCheck(iv1);}
private void cryptoCheck(byte[] iv) {OutputStream out = getOutputStream(defaultBufferSize, key, iv);writeData(out);InputStream in = getInputStream(defaultBufferSize, key, iv);readCheck(in);in.close();}
private void setCounterBaseForIV(byte[] iv, long counterBase) {ByteBuffer buf = ByteBuffer.wrap(iv);buf.order(ByteOrder.BIG_ENDIAN);buf.putLong(iv.length - 8, counterBase);}
public void testSyncable() {syncableCheck();}
private void syncableCheck() {OutputStream out = getOutputStream(smallBufferSize);try {int bytesWritten = dataLen / 3;out.write(data, 0, bytesWritten);((Syncable) out).hflush();InputStream in = getInputStream(defaultBufferSize);verify(in, bytesWritten, data);in.close();out.write(data, bytesWritten, dataLen - bytesWritten);((Syncable) out).hsync();in = getInputStream(defaultBufferSize);verify(in, dataLen, data);in.close();} finally {out.close();}}
private void verify(InputStream in, int bytesToVerify, byte[] expectedBytes) {final byte[] readBuf = new byte[bytesToVerify];readAll(in, readBuf, 0, bytesToVerify);for (int i = 0; i < bytesToVerify; i++) {Assert.assertEquals(expectedBytes[i], readBuf[i]);}}
private int readAll(InputStream in, long pos, byte[] b, int off, int len) {int n = 0;int total = 0;while (n != -1) {total += n;if (total >= len) {break;}n = ((PositionedReadable) in).read(pos + total, b, off + total, len - total);}return total;}
public void testPositionedRead() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);// Pos: 1/3 dataLenpositionedReadCheck(in, dataLen / 3);// Pos: 1/2 dataLenpositionedReadCheck(in, dataLen / 2);in.close();}
private void positionedReadCheck(InputStream in, int pos) {byte[] result = new byte[dataLen];int n = readAll(in, pos, result, 0, dataLen);Assert.assertEquals(dataLen, n + pos);byte[] readData = new byte[n];System.arraycopy(result, 0, readData, 0, n);byte[] expectedData = new byte[n];System.arraycopy(data, pos, expectedData, 0, n);Assert.assertArrayEquals(readData, expectedData);}
public void testReadFully() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);final int len1 = dataLen / 4;// Read len1 bytesbyte[] readData = new byte[len1];readAll(in, readData, 0, len1);byte[] expectedData = new byte[len1];System.arraycopy(data, 0, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);// Pos: 1/3 dataLenreadFullyCheck(in, dataLen / 3);// Read len1 bytesreadData = new byte[len1];readAll(in, readData, 0, len1);expectedData = new byte[len1];System.arraycopy(data, len1, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);// Pos: 1/2 dataLenreadFullyCheck(in, dataLen / 2);// Read len1 bytesreadData = new byte[len1];readAll(in, readData, 0, len1);expectedData = new byte[len1];System.arraycopy(data, 2 * len1, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);in.close();}
private void readFullyCheck(InputStream in, int pos) {byte[] result = new byte[dataLen - pos];((PositionedReadable) in).readFully(pos, result);byte[] expectedData = new byte[dataLen - pos];System.arraycopy(data, pos, expectedData, 0, dataLen - pos);Assert.assertArrayEquals(result, expectedData);// Exceeds maximum length result = new byte[dataLen];try {((PositionedReadable) in).readFully(pos, result);Assert.fail("Read fully exceeds maximum length should fail.");} catch (IOException e) {}}
public void testSeek() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);// Pos: 1/3 dataLenseekCheck(in, dataLen / 3);// Pos: 0seekCheck(in, 0);// Pos: 1/2 dataLenseekCheck(in, dataLen / 2);final long pos = ((Seekable) in).getPos();// Pos: -3try {seekCheck(in, -3);Assert.fail("Seek to negative offset should fail.");} catch (IllegalArgumentException e) {GenericTestUtils.assertExceptionContains("Cannot seek to negative " + "offset", e);}Assert.assertEquals(pos, ((Seekable) in).getPos());// Pos: dataLen + 3try {seekCheck(in, dataLen + 3);Assert.fail("Seek after EOF should fail.");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Cannot seek after EOF", e);}Assert.assertEquals(pos, ((Seekable) in).getPos());in.close();}
private void seekCheck(InputStream in, int pos) {byte[] result = new byte[dataLen];((Seekable) in).seek(pos);int n = readAll(in, result, 0, dataLen);Assert.assertEquals(dataLen, n + pos);byte[] readData = new byte[n];System.arraycopy(result, 0, readData, 0, n);byte[] expectedData = new byte[n];System.arraycopy(data, pos, expectedData, 0, n);Assert.assertArrayEquals(readData, expectedData);}
public void testGetPos() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);byte[] result = new byte[dataLen];int n1 = readAll(in, result, 0, dataLen / 3);Assert.assertEquals(n1, ((Seekable) in).getPos());int n2 = readAll(in, result, n1, dataLen - n1);Assert.assertEquals(n1 + n2, ((Seekable) in).getPos());in.close();}
public void testAvailable() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);byte[] result = new byte[dataLen];int n1 = readAll(in, result, 0, dataLen / 3);Assert.assertEquals(in.available(), dataLen - n1);int n2 = readAll(in, result, n1, dataLen - n1);Assert.assertEquals(in.available(), dataLen - n1 - n2);in.close();}
public void testSkip() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);byte[] result = new byte[dataLen];int n1 = readAll(in, result, 0, dataLen / 3);Assert.assertEquals(n1, ((Seekable) in).getPos());long skipped = in.skip(dataLen / 3);int n2 = readAll(in, result, 0, dataLen);Assert.assertEquals(dataLen, n1 + skipped + n2);byte[] readData = new byte[n2];System.arraycopy(result, 0, readData, 0, n2);byte[] expectedData = new byte[n2];System.arraycopy(data, dataLen - n2, expectedData, 0, n2);Assert.assertArrayEquals(readData, expectedData);try {skipped = in.skip(-3);Assert.fail("Skip Negative length should fail.");} catch (IllegalArgumentException e) {GenericTestUtils.assertExceptionContains("Negative skip length", e);}// Skip after EOFskipped = in.skip(3);Assert.assertEquals(skipped, 0);in.close();}
private void byteBufferReadCheck(InputStream in, ByteBuffer buf, int bufPos) {buf.position(bufPos);int n = ((ByteBufferReadable) in).read(buf);byte[] readData = new byte[n];buf.rewind();buf.position(bufPos);buf.get(readData);byte[] expectedData = new byte[n];System.arraycopy(data, 0, expectedData, 0, n);Assert.assertArrayEquals(readData, expectedData);}
public void testByteBufferRead() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);// Default buffer size, initial buffer position is 0InputStream in = getInputStream(defaultBufferSize);ByteBuffer buf = ByteBuffer.allocate(dataLen + 100);byteBufferReadCheck(in, buf, 0);in.close();// Default buffer size, initial buffer position is not 0in = getInputStream(defaultBufferSize);buf.clear();byteBufferReadCheck(in, buf, 11);in.close();// Small buffer size, initial buffer position is 0in = getInputStream(smallBufferSize);buf.clear();byteBufferReadCheck(in, buf, 0);in.close();// Small buffer size, initial buffer position is not 0in = getInputStream(smallBufferSize);buf.clear();byteBufferReadCheck(in, buf, 11);in.close();// Direct buffer, default buffer size, initial buffer position is 0in = getInputStream(defaultBufferSize);buf = ByteBuffer.allocateDirect(dataLen + 100);byteBufferReadCheck(in, buf, 0);in.close();// Direct buffer, default buffer size, initial buffer position is not 0in = getInputStream(defaultBufferSize);buf.clear();byteBufferReadCheck(in, buf, 11);in.close();// Direct buffer, small buffer size, initial buffer position is 0in = getInputStream(smallBufferSize);buf.clear();byteBufferReadCheck(in, buf, 0);in.close();// Direct buffer, small buffer size, initial buffer position is not 0in = getInputStream(smallBufferSize);buf.clear();byteBufferReadCheck(in, buf, 11);in.close();}
public void testCombinedOp() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);final int len1 = dataLen / 8;final int len2 = dataLen / 10;InputStream in = getInputStream(defaultBufferSize);// Read len1 data.byte[] readData = new byte[len1];readAll(in, readData, 0, len1);byte[] expectedData = new byte[len1];System.arraycopy(data, 0, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);long pos = ((Seekable) in).getPos();Assert.assertEquals(len1, pos);// Seek forward len2((Seekable) in).seek(pos + len2);// Skip forward len2long n = in.skip(len2);Assert.assertEquals(len2, n);// Pos: 1/4 dataLenpositionedReadCheck(in, dataLen / 4);// Pos should be len1 + len2 + len2pos = ((Seekable) in).getPos();Assert.assertEquals(len1 + len2 + len2, pos);// Read forward len1ByteBuffer buf = ByteBuffer.allocate(len1);int nRead = ((ByteBufferReadable) in).read(buf);readData = new byte[nRead];buf.rewind();buf.get(readData);expectedData = new byte[nRead];System.arraycopy(data, (int) pos, expectedData, 0, nRead);Assert.assertArrayEquals(readData, expectedData);// Pos should be len1 + 2 * len2 + nReadpos = ((Seekable) in).getPos();Assert.assertEquals(len1 + 2 * len2 + nRead, pos);// Pos: 1/3 dataLenpositionedReadCheck(in, dataLen / 3);// Read forward len1readData = new byte[len1];readAll(in, readData, 0, len1);expectedData = new byte[len1];System.arraycopy(data, (int) pos, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);// Pos should be 2 * len1 + 2 * len2 + nReadpos = ((Seekable) in).getPos();Assert.assertEquals(2 * len1 + 2 * len2 + nRead, pos);// Read forward len1buf = ByteBuffer.allocate(len1);nRead = ((ByteBufferReadable) in).read(buf);readData = new byte[nRead];buf.rewind();buf.get(readData);expectedData = new byte[nRead];System.arraycopy(data, (int) pos, expectedData, 0, nRead);Assert.assertArrayEquals(readData, expectedData);// ByteBuffer read after EOF((Seekable) in).seek(dataLen);buf.clear();n = ((ByteBufferReadable) in).read(buf);Assert.assertEquals(n, -1);in.close();}
public void testSeekToNewSource() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);final int len1 = dataLen / 8;byte[] readData = new byte[len1];readAll(in, readData, 0, len1);// Pos: 1/3 dataLenseekToNewSourceCheck(in, dataLen / 3);// Pos: 0seekToNewSourceCheck(in, 0);// Pos: 1/2 dataLenseekToNewSourceCheck(in, dataLen / 2);// Pos: -3try {seekToNewSourceCheck(in, -3);Assert.fail("Seek to negative offset should fail.");} catch (IllegalArgumentException e) {GenericTestUtils.assertExceptionContains("Cannot seek to negative " + "offset", e);}// Pos: dataLen + 3try {seekToNewSourceCheck(in, dataLen + 3);Assert.fail("Seek after EOF should fail.");} catch (IOException e) {GenericTestUtils.assertExceptionContains("Attempted to read past " + "end of file", e);}in.close();}
private void seekToNewSourceCheck(InputStream in, int targetPos) {byte[] result = new byte[dataLen];((Seekable) in).seekToNewSource(targetPos);int n = readAll(in, result, 0, dataLen);Assert.assertEquals(dataLen, n + targetPos);byte[] readData = new byte[n];System.arraycopy(result, 0, readData, 0, n);byte[] expectedData = new byte[n];System.arraycopy(data, targetPos, expectedData, 0, n);Assert.assertArrayEquals(readData, expectedData);}
private ByteBufferPool getBufferPool() {return new ByteBufferPool() {
@Overridepublic ByteBuffer getBuffer(boolean direct, int length) {return ByteBuffer.allocateDirect(length);}
@Overridepublic void putBuffer(ByteBuffer buffer) {}};}
public ByteBuffer getBuffer(boolean direct, int length) {return ByteBuffer.allocateDirect(length);}
public void putBuffer(ByteBuffer buffer) {}
public void testHasEnhancedByteBufferAccess() {OutputStream out = getOutputStream(defaultBufferSize);writeData(out);InputStream in = getInputStream(defaultBufferSize);final int len1 = dataLen / 8;ByteBuffer buffer = ((HasEnhancedByteBufferAccess) in).read(getBufferPool(), len1, EnumSet.of(ReadOption.SKIP_CHECKSUMS));int n1 = buffer.remaining();byte[] readData = new byte[n1];buffer.get(readData);byte[] expectedData = new byte[n1];System.arraycopy(data, 0, expectedData, 0, n1);Assert.assertArrayEquals(readData, expectedData);((HasEnhancedByteBufferAccess) in).releaseBuffer(buffer);// Read len1 bytesreadData = new byte[len1];readAll(in, readData, 0, len1);expectedData = new byte[len1];System.arraycopy(data, n1, expectedData, 0, len1);Assert.assertArrayEquals(readData, expectedData);// ByteBuffer size is len1buffer = ((HasEnhancedByteBufferAccess) in).read(getBufferPool(), len1, EnumSet.of(ReadOption.SKIP_CHECKSUMS));int n2 = buffer.remaining();readData = new byte[n2];buffer.get(readData);expectedData = new byte[n2];System.arraycopy(data, n1 + len1, expectedData, 0, n2);Assert.assertArrayEquals(readData, expectedData);((HasEnhancedByteBufferAccess) in).releaseBuffer(buffer);in.close();}public ZooKeeper getNewZooKeeper() {++count;return mockZK;}
protected void sleepFor(int ms) {// time sleptLOG.info("Would have slept for " + ms + "ms");sleptFor += ms;}
public void init() {count = 0;mockZK = Mockito.mock(ZooKeeper.class);mockApp = Mockito.mock(ActiveStandbyElectorCallback.class);elector = new ActiveStandbyElectorTester("hostPort", 1000, ZK_PARENT_NAME, Ids.OPEN_ACL_UNSAFE, mockApp);}
private void mockNoPriorActive() {Mockito.doThrow(new KeeperException.NoNodeException()).when(mockZK).getData(Mockito.eq(ZK_BREADCRUMB_NAME), Mockito.anyBoolean(), Mockito.<Stat>any());}
private void mockPriorActive(byte[] data) {Mockito.doReturn(data).when(mockZK).getData(Mockito.eq(ZK_BREADCRUMB_NAME), Mockito.anyBoolean(), Mockito.<Stat>any());}
public void testJoinElectionException() {elector.joinElection(null);}
public void testJoinElection() {elector.joinElection(data);Mockito.verify(mockZK, Mockito.times(1)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);}
public void testCreateNodeResultBecomeActive() {mockNoPriorActive();elector.joinElection(data);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeActive();verifyExistCall(1);// not call becomeActive since its already activeStat stat = new Stat();stat.setEphemeralOwner(1L);Mockito.when(mockZK.getSessionId()).thenReturn(1L);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, stat);// should not call neutral mode/standby/activeMockito.verify(mockApp, Mockito.times(0)).enterNeutralMode();Mockito.verify(mockApp, Mockito.times(0)).becomeStandby();Mockito.verify(mockApp, Mockito.times(1)).becomeActive();// another joinElection not called.Mockito.verify(mockZK, Mockito.times(1)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);// no new monitor calledverifyExistCall(1);}
public void testFailToBecomeActive() {mockNoPriorActive();elector.joinElection(data);Assert.assertEquals(0, elector.sleptFor);Mockito.doThrow(new ServiceFailedException("failed to become active")).when(mockApp).becomeActive();elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);// Should have tried to become activeMockito.verify(mockApp).becomeActive();// should re-joinMockito.verify(mockZK, Mockito.times(2)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);Assert.assertEquals(2, count);Assert.assertTrue(elector.sleptFor > 0);}
public void testFailToBecomeActiveAfterZKDisconnect() {mockNoPriorActive();elector.joinElection(data);Assert.assertEquals(0, elector.sleptFor);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockZK, Mockito.times(2)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);verifyExistCall(1);Stat stat = new Stat();stat.setEphemeralOwner(1L);Mockito.when(mockZK.getSessionId()).thenReturn(1L);// Fake failure to become active from within the stat callbackMockito.doThrow(new ServiceFailedException("fail to become active")).when(mockApp).becomeActive();elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, stat);Mockito.verify(mockApp, Mockito.times(1)).becomeActive();// should re-joinMockito.verify(mockZK, Mockito.times(3)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);Assert.assertEquals(2, count);Assert.assertTrue(elector.sleptFor > 0);}
public void testFencesOldActive() {byte[] fakeOldActiveData = new byte[0];mockPriorActive(fakeOldActiveData);elector.joinElection(data);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);// Application fences active.Mockito.verify(mockApp, Mockito.times(1)).fenceOldActive(fakeOldActiveData);// Updates breadcrumb node to new dataMockito.verify(mockZK, Mockito.times(1)).setData(Mockito.eq(ZK_BREADCRUMB_NAME), Mockito.eq(data), Mockito.eq(0));// Then it becomes active itselfMockito.verify(mockApp, Mockito.times(1)).becomeActive();}
public void testQuitElectionRemovesBreadcrumbNode() {mockNoPriorActive();elector.joinElection(data);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);// Writes its own active infoMockito.verify(mockZK, Mockito.times(1)).create(Mockito.eq(ZK_BREADCRUMB_NAME), Mockito.eq(data), Mockito.eq(Ids.OPEN_ACL_UNSAFE), Mockito.eq(CreateMode.PERSISTENT));mockPriorActive(data);elector.quitElection(false);// Deletes its own active dataMockito.verify(mockZK, Mockito.times(1)).delete(Mockito.eq(ZK_BREADCRUMB_NAME), Mockito.eq(0));}
public void testCreateNodeResultBecomeStandby() {elector.joinElection(data);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeStandby();verifyExistCall(1);}
public void testCreateNodeResultError() {elector.joinElection(data);elector.processResult(Code.APIERROR.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Received create error from Zookeeper. code:APIERROR " + "for path " + ZK_LOCK_NAME);}
public void testCreateNodeResultRetryBecomeActive() {mockNoPriorActive();elector.joinElection(data);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);// 4 errors results in fatalErrorMockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Received create error from Zookeeper. code:CONNECTIONLOSS " + "for path " + ZK_LOCK_NAME + ". " + "Not retrying further znode create connection errors.");elector.joinElection(data);// recreate connection via getNewZooKeeperAssert.assertEquals(2, count);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);verifyExistCall(1);Stat stat = new Stat();stat.setEphemeralOwner(1L);Mockito.when(mockZK.getSessionId()).thenReturn(1L);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, stat);Mockito.verify(mockApp, Mockito.times(1)).becomeActive();verifyExistCall(1);Mockito.verify(mockZK, Mockito.times(6)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);}
public void testCreateNodeResultRetryBecomeStandby() {elector.joinElection(data);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);verifyExistCall(1);Stat stat = new Stat();stat.setEphemeralOwner(0);Mockito.when(mockZK.getSessionId()).thenReturn(1L);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, stat);Mockito.verify(mockApp, Mockito.times(1)).becomeStandby();verifyExistCall(1);}
public void testCreateNodeResultRetryNoNode() {elector.joinElection(data);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);verifyExistCall(1);elector.processResult(Code.NONODE.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);Mockito.verify(mockApp, Mockito.times(1)).enterNeutralMode();Mockito.verify(mockZK, Mockito.times(4)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);}
public void testStatNodeRetry() {elector.joinElection(data);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);elector.processResult(Code.CONNECTIONLOSS.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);Mockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Received stat error from Zookeeper. code:CONNECTIONLOSS. " + "Not retrying further znode monitoring connection errors.");}
public void testStatNodeError() {elector.joinElection(data);elector.processResult(Code.RUNTIMEINCONSISTENCY.intValue(), ZK_LOCK_NAME, mockZK, (Stat) null);Mockito.verify(mockApp, Mockito.times(0)).enterNeutralMode();Mockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Received stat error from Zookeeper. code:RUNTIMEINCONSISTENCY");}
public void testProcessCallbackEventNone() {mockNoPriorActive();elector.joinElection(data);WatchedEvent mockEvent = Mockito.mock(WatchedEvent.class);Mockito.when(mockEvent.getType()).thenReturn(Event.EventType.None);// first SyncConnected should not do anythingMockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.SyncConnected);elector.processWatchEvent(mockZK, mockEvent);Mockito.verify(mockZK, Mockito.times(0)).exists(Mockito.anyString(), Mockito.anyBoolean(), Mockito.<AsyncCallback.StatCallback>anyObject(), Mockito.<Object>anyObject());// disconnection should enter safe modeMockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.Disconnected);elector.processWatchEvent(mockZK, mockEvent);Mockito.verify(mockApp, Mockito.times(1)).enterNeutralMode();// re-connection should monitor master statusMockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.SyncConnected);elector.processWatchEvent(mockZK, mockEvent);verifyExistCall(1);// call to create lock znodeMockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.Expired);elector.processWatchEvent(mockZK, mockEvent);// already in safe mode above. should not enter safe mode againMockito.verify(mockApp, Mockito.times(1)).enterNeutralMode();// constructorAssert.assertEquals(2, count);// once in initial joinElection and one nowMockito.verify(mockZK, Mockito.times(2)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);// create znode success. become master and monitorelector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeActive();verifyExistCall(2);// error event results in fatal errorMockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.AuthFailed);elector.processWatchEvent(mockZK, mockEvent);Mockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Unexpected Zookeeper watch event state: AuthFailed");// only 1 state change callback is called at a timeMockito.verify(mockApp, Mockito.times(1)).enterNeutralMode();}
public void testProcessCallbackEventNode() {mockNoPriorActive();elector.joinElection(data);// make the object go into the monitoring stateelector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeStandby();verifyExistCall(1);WatchedEvent mockEvent = Mockito.mock(WatchedEvent.class);Mockito.when(mockEvent.getPath()).thenReturn(ZK_LOCK_NAME);// monitoring should be setup again after event is receivedMockito.when(mockEvent.getType()).thenReturn(Event.EventType.NodeDataChanged);elector.processWatchEvent(mockZK, mockEvent);verifyExistCall(2);// monitoring should be setup again after event is receivedMockito.when(mockEvent.getType()).thenReturn(Event.EventType.NodeChildrenChanged);elector.processWatchEvent(mockZK, mockEvent);verifyExistCall(3);// successful znode creation enters active state and sets monitorMockito.when(mockEvent.getType()).thenReturn(Event.EventType.NodeDeleted);elector.processWatchEvent(mockZK, mockEvent);// enterNeutralMode not called when app is standby and leader is lostMockito.verify(mockApp, Mockito.times(0)).enterNeutralMode();// once in initial joinElection() and one nowMockito.verify(mockZK, Mockito.times(2)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeActive();verifyExistCall(4);// monitorMockito.when(mockEvent.getType()).thenReturn(Event.EventType.NodeDeleted);elector.processWatchEvent(mockZK, mockEvent);Mockito.verify(mockApp, Mockito.times(1)).enterNeutralMode();// another joinElection calledMockito.verify(mockZK, Mockito.times(3)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);elector.processResult(Code.OK.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(2)).becomeActive();verifyExistCall(5);// bad path name results in fatal errorMockito.when(mockEvent.getPath()).thenReturn(null);elector.processWatchEvent(mockZK, mockEvent);Mockito.verify(mockApp, Mockito.times(1)).notifyFatalError("Unexpected watch error from Zookeeper");// fatal error means no new connection other than one from constructorAssert.assertEquals(1, count);// no new watches after fatal errorverifyExistCall(5);}
private void verifyExistCall(int times) {Mockito.verify(mockZK, Mockito.times(times)).exists(Mockito.eq(ZK_LOCK_NAME), Mockito.<Watcher>any(), Mockito.same(elector), Mockito.same(mockZK));}
public void testSuccessiveStandbyCalls() {elector.joinElection(data);// make the object go into the monitoring standby stateelector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeStandby();verifyExistCall(1);WatchedEvent mockEvent = Mockito.mock(WatchedEvent.class);Mockito.when(mockEvent.getPath()).thenReturn(ZK_LOCK_NAME);// monitoring should be setup again after event is receivedMockito.when(mockEvent.getType()).thenReturn(Event.EventType.NodeDeleted);elector.processWatchEvent(mockZK, mockEvent);// is standby. no need to notify anything nowMockito.verify(mockApp, Mockito.times(0)).enterNeutralMode();// another joinElection called.Mockito.verify(mockZK, Mockito.times(2)).create(ZK_LOCK_NAME, data, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);// lost electionelector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);// still standby. so no need to notify againMockito.verify(mockApp, Mockito.times(1)).becomeStandby();// monitor is set againverifyExistCall(2);}
public void testQuitElection() {elector.joinElection(data);Mockito.verify(mockZK, Mockito.times(0)).close();elector.quitElection(true);Mockito.verify(mockZK, Mockito.times(1)).close();verifyExistCall(0);byte[] data = new byte[8];elector.joinElection(data);// getNewZooKeeper called 2 times. once in constructor and once nowAssert.assertEquals(2, count);elector.processResult(Code.NODEEXISTS.intValue(), ZK_LOCK_NAME, mockZK, ZK_LOCK_NAME);Mockito.verify(mockApp, Mockito.times(1)).becomeStandby();verifyExistCall(1);}
public void testGetActiveData() {// get valid active databyte[] data = new byte[8];Mockito.when(mockZK.getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject())).thenReturn(data);Assert.assertEquals(data, elector.getActiveData());Mockito.verify(mockZK, Mockito.times(1)).getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject());// active does not existMockito.when(mockZK.getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject())).thenThrow(new KeeperException.NoNodeException());try {elector.getActiveData();Assert.fail("ActiveNotFoundException expected");} catch (ActiveNotFoundException e) {Mockito.verify(mockZK, Mockito.times(2)).getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject());}// error getting active data rethrows keeperexceptiontry {Mockito.when(mockZK.getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject())).thenThrow(new KeeperException.AuthFailedException());elector.getActiveData();Assert.fail("KeeperException.AuthFailedException expected");} catch (KeeperException.AuthFailedException ke) {Mockito.verify(mockZK, Mockito.times(3)).getData(Mockito.eq(ZK_LOCK_NAME), Mockito.eq(false), Mockito.<Stat>anyObject());}}
public void testEnsureBaseNode() {elector.ensureParentZNode();StringBuilder prefix = new StringBuilder();for (String part : ZK_PARENT_NAME.split("/")) {if (part.isEmpty())continue;prefix.append("/").append(part);if (!"/".equals(prefix.toString())) {Mockito.verify(mockZK).create(Mockito.eq(prefix.toString()), Mockito.<byte[]>any(), Mockito.eq(Ids.OPEN_ACL_UNSAFE), Mockito.eq(CreateMode.PERSISTENT));}}}
public void testEnsureBaseNodeFails() {Mockito.doThrow(new KeeperException.ConnectionLossException()).when(mockZK).create(Mockito.eq(ZK_PARENT_NAME), Mockito.<byte[]>any(), Mockito.eq(Ids.OPEN_ACL_UNSAFE), Mockito.eq(CreateMode.PERSISTENT));try {elector.ensureParentZNode();Assert.fail("Did not throw!");} catch (IOException ioe) {if (!(ioe.getCause() instanceof KeeperException.ConnectionLossException)) {throw ioe;}}// Should have tried three timesMockito.verify(mockZK, Mockito.times(3)).create(Mockito.eq(ZK_PARENT_NAME), Mockito.<byte[]>any(), Mockito.eq(Ids.OPEN_ACL_UNSAFE), Mockito.eq(CreateMode.PERSISTENT));}
public void testWithoutZKServer() {try {new ActiveStandbyElector("127.0.0.1", 2000, ZK_PARENT_NAME, Ids.OPEN_ACL_UNSAFE, Collections.<ZKAuthInfo>emptyList(), mockApp, CommonConfigurationKeys.HA_FC_ELECTOR_ZK_OP_RETRIES_DEFAULT);Assert.fail("Did not throw zookeeper connection loss exceptions!");} catch (KeeperException ke) {GenericTestUtils.assertExceptionContains("ConnectionLoss", ke);}}
public void testBecomeActiveBeforeServiceHealthy() {mockNoPriorActive();WatchedEvent mockEvent = Mockito.mock(WatchedEvent.class);Mockito.when(mockEvent.getType()).thenReturn(Event.EventType.None);// should not enter the election.Mockito.when(mockEvent.getState()).thenReturn(Event.KeeperState.Expired);elector.processWatchEvent(mockZK, mockEvent);// joinElection should not be called.Mockito.verify(mockZK, Mockito.times(0)).create(ZK_LOCK_NAME, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL, elector, mockZK);}public void handle(SchedulerEvent event) {scheduler.handle(event);}
public void handle(NodesListManagerEvent event) {nodesListManagerEvent = event;}
public void setUp() {InlineDispatcher rmDispatcher = new InlineDispatcher();rmContext = new RMContextImpl(rmDispatcher, null, null, null, mock(DelegationTokenRenewer.class), null, null, null, null, null);NodesListManager nodesListManager = mock(NodesListManager.class);HostsFileReader reader = mock(HostsFileReader.class);when(nodesListManager.getHostsReader()).thenReturn(reader);((RMContextImpl) rmContext).setNodesListManager(nodesListManager);scheduler = mock(YarnScheduler.class);doAnswer(new Answer<Void>() {
@Overridepublic Void answer(InvocationOnMock invocation) throws Throwable {final SchedulerEvent event = (SchedulerEvent) (invocation.getArguments()[0]);eventType = event.getType();if (eventType == SchedulerEventType.NODE_UPDATE) {List<UpdatedContainerInfo> lastestContainersInfoList = ((NodeUpdateSchedulerEvent) event).getRMNode().pullContainerUpdates();for (UpdatedContainerInfo lastestContainersInfo : lastestContainersInfoList) {completedContainers.addAll(lastestContainersInfo.getCompletedContainers());}}return null;}}).when(scheduler).handle(any(SchedulerEvent.class));rmDispatcher.register(SchedulerEventType.class, new TestSchedulerEventDispatcher());rmDispatcher.register(NodesListManagerEventType.class, new TestNodeListManagerEventDispatcher());NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);nodesListManagerEvent = null;}
public Void answer(InvocationOnMock invocation) {final SchedulerEvent event = (SchedulerEvent) (invocation.getArguments()[0]);eventType = event.getType();if (eventType == SchedulerEventType.NODE_UPDATE) {List<UpdatedContainerInfo> lastestContainersInfoList = ((NodeUpdateSchedulerEvent) event).getRMNode().pullContainerUpdates();for (UpdatedContainerInfo lastestContainersInfo : lastestContainersInfoList) {completedContainers.addAll(lastestContainersInfo.getCompletedContainers());}}return null;}
public void tearDown() {}
private RMNodeStatusEvent getMockRMNodeStatusEvent() {NodeHeartbeatResponse response = mock(NodeHeartbeatResponse.class);NodeHealthStatus healthStatus = mock(NodeHealthStatus.class);Boolean yes = new Boolean(true);doReturn(yes).when(healthStatus).getIsNodeHealthy();RMNodeStatusEvent event = mock(RMNodeStatusEvent.class);doReturn(healthStatus).when(event).getNodeHealthStatus();doReturn(response).when(event).getLatestResponse();doReturn(RMNodeEventType.STATUS_UPDATE).when(event).getType();return event;}
public void testExpiredContainer() {// Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));verify(scheduler).handle(any(NodeAddedSchedulerEvent.class));// Expire a containerContainerId completedContainerId = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);node.handle(new RMNodeCleanContainerEvent(null, completedContainerId));Assert.assertEquals(1, node.getContainersToCleanUp().size());// by checking number of 'completedContainers' it got in the previous eventRMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();ContainerStatus containerStatus = mock(ContainerStatus.class);doReturn(completedContainerId).when(containerStatus).getContainerId();doReturn(Collections.singletonList(containerStatus)).when(statusEvent).getContainers();node.handle(statusEvent);/* Expect the scheduler call handle function 2 times * 1. RMNode status from new to Running, handle the add_node event * 2. handle the node update event */verify(scheduler, times(2)).handle(any(NodeUpdateSchedulerEvent.class));}
public void testContainerUpdate() {//Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));NodeId nodeId = BuilderUtils.newNodeId("localhost:1", 1);RMNodeImpl node2 = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);node2.handle(new RMNodeStartedEvent(null, null, null));ContainerId completedContainerIdFromNode1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);ContainerId completedContainerIdFromNode2_1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 1);ContainerId completedContainerIdFromNode2_2 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 2);RMNodeStatusEvent statusEventFromNode1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEventFromNode2_1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEventFromNode2_2 = getMockRMNodeStatusEvent();ContainerStatus containerStatusFromNode1 = mock(ContainerStatus.class);ContainerStatus containerStatusFromNode2_1 = mock(ContainerStatus.class);ContainerStatus containerStatusFromNode2_2 = mock(ContainerStatus.class);doReturn(completedContainerIdFromNode1).when(containerStatusFromNode1).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode1)).when(statusEventFromNode1).getContainers();node.handle(statusEventFromNode1);Assert.assertEquals(1, completedContainers.size());Assert.assertEquals(completedContainerIdFromNode1, completedContainers.get(0).getContainerId());completedContainers.clear();doReturn(completedContainerIdFromNode2_1).when(containerStatusFromNode2_1).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode2_1)).when(statusEventFromNode2_1).getContainers();doReturn(completedContainerIdFromNode2_2).when(containerStatusFromNode2_2).getContainerId();doReturn(Collections.singletonList(containerStatusFromNode2_2)).when(statusEventFromNode2_2).getContainers();node2.setNextHeartBeat(false);node2.handle(statusEventFromNode2_1);node2.setNextHeartBeat(true);node2.handle(statusEventFromNode2_2);Assert.assertEquals(2, completedContainers.size());Assert.assertEquals(completedContainerIdFromNode2_1, completedContainers.get(0).getContainerId());Assert.assertEquals(completedContainerIdFromNode2_2, completedContainers.get(1).getContainerId());}
public void testStatusChange() {//Start the nodenode.handle(new RMNodeStartedEvent(null, null, null));//Add info to the queue firstnode.setNextHeartBeat(false);ContainerId completedContainerId1 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);ContainerId completedContainerId2 = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(1, 1), 1), 1);RMNodeStatusEvent statusEvent1 = getMockRMNodeStatusEvent();RMNodeStatusEvent statusEvent2 = getMockRMNodeStatusEvent();ContainerStatus containerStatus1 = mock(ContainerStatus.class);ContainerStatus containerStatus2 = mock(ContainerStatus.class);doReturn(completedContainerId1).when(containerStatus1).getContainerId();doReturn(Collections.singletonList(containerStatus1)).when(statusEvent1).getContainers();doReturn(completedContainerId2).when(containerStatus2).getContainerId();doReturn(Collections.singletonList(containerStatus2)).when(statusEvent2).getContainers();verify(scheduler, times(1)).handle(any(NodeUpdateSchedulerEvent.class));node.handle(statusEvent1);node.handle(statusEvent2);verify(scheduler, times(1)).handle(any(NodeUpdateSchedulerEvent.class));Assert.assertEquals(2, node.getQueueSize());node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals(0, node.getQueueSize());}
public void testRunningExpire() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost + 1, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.LOST, node.getState());}
public void testUnhealthyExpire() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost + 1, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.LOST, node.getState());}
public void testUnhealthyExpireForSchedulerRemove() {RMNodeImpl node = getUnhealthyNode();verify(scheduler, times(2)).handle(any(NodeRemovedSchedulerEvent.class));node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.EXPIRE));verify(scheduler, times(2)).handle(any(NodeRemovedSchedulerEvent.class));Assert.assertEquals(NodeState.LOST, node.getState());}
public void testRunningDecommission() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.DECOMMISSION));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned + 1, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.DECOMMISSIONED, node.getState());}
public void testUnhealthyDecommission() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.DECOMMISSION));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned + 1, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.DECOMMISSIONED, node.getState());}
public void testRunningRebooting() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.REBOOTING));Assert.assertEquals("Active Nodes", initialActive - 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted + 1, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.REBOOTED, node.getState());}
public void testUnhealthyRebooting() {RMNodeImpl node = getUnhealthyNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.REBOOTING));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy - 1, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted + 1, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.REBOOTED, node.getState());}
public void testUpdateHeartbeatResponseForCleanup() {RMNodeImpl node = getRunningNode();NodeId nodeId = node.getNodeID();// Expire a containerContainerId completedContainerId = BuilderUtils.newContainerId(BuilderUtils.newApplicationAttemptId(BuilderUtils.newApplicationId(0, 0), 0), 0);node.handle(new RMNodeCleanContainerEvent(nodeId, completedContainerId));Assert.assertEquals(1, node.getContainersToCleanUp().size());// Finish an applicationApplicationId finishedAppId = BuilderUtils.newApplicationId(0, 1);node.handle(new RMNodeCleanAppEvent(nodeId, finishedAppId));Assert.assertEquals(1, node.getAppsToCleanup().size());// but updating heartbeat response for cleanup doesRMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();node.handle(statusEvent);Assert.assertEquals(1, node.getContainersToCleanUp().size());Assert.assertEquals(1, node.getAppsToCleanup().size());NodeHeartbeatResponse hbrsp = Records.newRecord(NodeHeartbeatResponse.class);node.updateNodeHeartbeatResponseForCleanup(hbrsp);Assert.assertEquals(0, node.getContainersToCleanUp().size());Assert.assertEquals(0, node.getAppsToCleanup().size());Assert.assertEquals(1, hbrsp.getContainersToCleanup().size());Assert.assertEquals(completedContainerId, hbrsp.getContainersToCleanup().get(0));Assert.assertEquals(1, hbrsp.getApplicationsToCleanup().size());Assert.assertEquals(finishedAppId, hbrsp.getApplicationsToCleanup().get(0));}
private RMNodeImpl getRunningNode() {return getRunningNode(null);}
private RMNodeImpl getRunningNode(String nmVersion) {NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);Resource capability = Resource.newInstance(4096, 4);RMNodeImpl node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, ResourceOption.newInstance(capability, RMNode.OVER_COMMIT_TIMEOUT_MILLIS_DEFAULT), nmVersion);node.handle(new RMNodeStartedEvent(node.getNodeID(), null, null));Assert.assertEquals(NodeState.RUNNING, node.getState());return node;}
private RMNodeImpl getUnhealthyNode() {RMNodeImpl node = getRunningNode();NodeHealthStatus status = NodeHealthStatus.newInstance(false, "sick", System.currentTimeMillis());node.handle(new RMNodeStatusEvent(node.getNodeID(), status, new ArrayList<ContainerStatus>(), null, null));Assert.assertEquals(NodeState.UNHEALTHY, node.getState());return node;}
private RMNodeImpl getNewNode() {NodeId nodeId = BuilderUtils.newNodeId("localhost", 0);RMNodeImpl node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);return node;}
public void testAdd() {RMNodeImpl node = getNewNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeStartedEvent(node.getNodeID(), null, null));Assert.assertEquals("Active Nodes", initialActive + 1, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.RUNNING, node.getState());Assert.assertNotNull(nodesListManagerEvent);Assert.assertEquals(NodesListManagerEventType.NODE_USABLE, nodesListManagerEvent.getType());}
public void testReconnect() {RMNodeImpl node = getRunningNode();ClusterMetrics cm = ClusterMetrics.getMetrics();int initialActive = cm.getNumActiveNMs();int initialLost = cm.getNumLostNMs();int initialUnhealthy = cm.getUnhealthyNMs();int initialDecommissioned = cm.getNumDecommisionedNMs();int initialRebooted = cm.getNumRebootedNMs();node.handle(new RMNodeReconnectEvent(node.getNodeID(), node, null));Assert.assertEquals("Active Nodes", initialActive, cm.getNumActiveNMs());Assert.assertEquals("Lost Nodes", initialLost, cm.getNumLostNMs());Assert.assertEquals("Unhealthy Nodes", initialUnhealthy, cm.getUnhealthyNMs());Assert.assertEquals("Decommissioned Nodes", initialDecommissioned, cm.getNumDecommisionedNMs());Assert.assertEquals("Rebooted Nodes", initialRebooted, cm.getNumRebootedNMs());Assert.assertEquals(NodeState.RUNNING, node.getState());Assert.assertNotNull(nodesListManagerEvent);Assert.assertEquals(NodesListManagerEventType.NODE_USABLE, nodesListManagerEvent.getType());}
public void testReconnnectUpdate() {final String nmVersion1 = "nm version 1";final String nmVersion2 = "nm version 2";RMNodeImpl node = getRunningNode(nmVersion1);Assert.assertEquals(nmVersion1, node.getNodeManagerVersion());RMNodeImpl reconnectingNode = getRunningNode(nmVersion2);node.handle(new RMNodeReconnectEvent(node.getNodeID(), reconnectingNode, null));Assert.assertEquals(nmVersion2, node.getNodeManagerVersion());}protected void configureServlets() {bind(JAXBContextResolver.class);bind(RMWebServices.class);bind(GenericExceptionHandler.class);Configuration conf = new Configuration();conf.setClass(YarnConfiguration.RM_SCHEDULER, FifoScheduler.class, ResourceScheduler.class);rm = new MockRM(conf);bind(ResourceManager.class).toInstance(rm);bind(RMContext.class).toInstance(rm.getRMContext());bind(ApplicationACLsManager.class).toInstance(rm.getApplicationACLsManager());bind(QueueACLsManager.class).toInstance(rm.getQueueACLsManager());serve("/*").with(GuiceContainer.class);}
protected Injector getInjector() {return injector;}
public void setUp() {super.setUp();}
public static void initClusterMetrics() {ClusterMetrics clusterMetrics = ClusterMetrics.getMetrics();clusterMetrics.incrDecommisionedNMs();clusterMetrics.incrNumActiveNodes();clusterMetrics.incrNumLostNMs();clusterMetrics.incrNumRebootedNMs();clusterMetrics.incrNumUnhealthyNMs();}
public void testInfoXML() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("info").accept("application/xml").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_XML_TYPE, response.getType());String xml = response.getEntity(String.class);verifyClusterInfoXML(xml);}
public void testInvalidUri() {WebResource r = resource();String responseStr = "";try {responseStr = r.path("ws").path("v1").path("cluster").path("bogus").accept(MediaType.APPLICATION_JSON).get(String.class);fail("should have thrown exception on invalid uri");} catch (UniformInterfaceException ue) {ClientResponse response = ue.getResponse();assertEquals(Status.NOT_FOUND, response.getClientResponseStatus());WebServicesTestUtils.checkStringMatch("error string exists and shouldn't", "", responseStr);}}
public void testInvalidUri2() {WebResource r = resource();String responseStr = "";try {responseStr = r.accept(MediaType.APPLICATION_JSON).get(String.class);fail("should have thrown exception on invalid uri");} catch (UniformInterfaceException ue) {ClientResponse response = ue.getResponse();assertEquals(Status.NOT_FOUND, response.getClientResponseStatus());WebServicesTestUtils.checkStringMatch("error string exists and shouldn't", "", responseStr);}}
public void testInvalidAccept() {WebResource r = resource();String responseStr = "";try {responseStr = r.path("ws").path("v1").path("cluster").accept(MediaType.TEXT_PLAIN).get(String.class);fail("should have thrown exception on invalid uri");} catch (UniformInterfaceException ue) {ClientResponse response = ue.getResponse();assertEquals(Status.INTERNAL_SERVER_ERROR, response.getClientResponseStatus());WebServicesTestUtils.checkStringMatch("error string exists and shouldn't", "", responseStr);}}
public void testCluster() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void testClusterSlash() {WebResource r = resource();// test with trailing "/" to make sure acts same as without slashClientResponse response = r.path("ws").path("v1").path("cluster/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void testClusterDefault() {WebResource r = resource();// test with trailing "/" to make sure acts same as without slashClientResponse response = r.path("ws").path("v1").path("cluster").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void testInfo() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("info").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void testInfoSlash() {// test with trailing "/" to make sure acts same as without slashWebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("info/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void testInfoDefault() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("info").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterInfo(json);}
public void verifyClusterInfoXML(String xml) {DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();DocumentBuilder db = dbf.newDocumentBuilder();InputSource is = new InputSource();is.setCharacterStream(new StringReader(xml));Document dom = db.parse(is);NodeList nodes = dom.getElementsByTagName("clusterInfo");assertEquals("incorrect number of elements", 1, nodes.getLength());for (int i = 0; i < nodes.getLength(); i++) {Element element = (Element) nodes.item(i);verifyClusterGeneric(WebServicesTestUtils.getXmlLong(element, "id"), WebServicesTestUtils.getXmlLong(element, "startedOn"), WebServicesTestUtils.getXmlString(element, "state"), WebServicesTestUtils.getXmlString(element, "haState"), WebServicesTestUtils.getXmlString(element, "hadoopVersionBuiltOn"), WebServicesTestUtils.getXmlString(element, "hadoopBuildVersion"), WebServicesTestUtils.getXmlString(element, "hadoopVersion"), WebServicesTestUtils.getXmlString(element, "resourceManagerVersionBuiltOn"), WebServicesTestUtils.getXmlString(element, "resourceManagerBuildVersion"), WebServicesTestUtils.getXmlString(element, "resourceManagerVersion"));}}
public void verifyClusterInfo(JSONObject json) {assertEquals("incorrect number of elements", 1, json.length());JSONObject info = json.getJSONObject("clusterInfo");assertEquals("incorrect number of elements", 10, info.length());verifyClusterGeneric(info.getLong("id"), info.getLong("startedOn"), info.getString("state"), info.getString("haState"), info.getString("hadoopVersionBuiltOn"), info.getString("hadoopBuildVersion"), info.getString("hadoopVersion"), info.getString("resourceManagerVersionBuiltOn"), info.getString("resourceManagerBuildVersion"), info.getString("resourceManagerVersion"));}
public void verifyClusterGeneric(long clusterid, long startedon, String state, String haState, String hadoopVersionBuiltOn, String hadoopBuildVersion, String hadoopVersion, String resourceManagerVersionBuiltOn, String resourceManagerBuildVersion, String resourceManagerVersion) {assertEquals("clusterId doesn't match: ", ResourceManager.getClusterTimeStamp(), clusterid);assertEquals("startedOn doesn't match: ", ResourceManager.getClusterTimeStamp(), startedon);assertTrue("stated doesn't match: " + state, state.matches(STATE.INITED.toString()));assertTrue("HA state doesn't match: " + haState, haState.matches("INITIALIZING"));WebServicesTestUtils.checkStringMatch("hadoopVersionBuiltOn", VersionInfo.getDate(), hadoopVersionBuiltOn);WebServicesTestUtils.checkStringEqual("hadoopBuildVersion", VersionInfo.getBuildVersion(), hadoopBuildVersion);WebServicesTestUtils.checkStringMatch("hadoopVersion", VersionInfo.getVersion(), hadoopVersion);WebServicesTestUtils.checkStringMatch("resourceManagerVersionBuiltOn", YarnVersionInfo.getDate(), resourceManagerVersionBuiltOn);WebServicesTestUtils.checkStringEqual("resourceManagerBuildVersion", YarnVersionInfo.getBuildVersion(), resourceManagerBuildVersion);WebServicesTestUtils.checkStringMatch("resourceManagerVersion", YarnVersionInfo.getVersion(), resourceManagerVersion);}
public void testClusterMetrics() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("metrics").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterMetricsJSON(json);}
public void testClusterMetricsSlash() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("metrics/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterMetricsJSON(json);}
public void testClusterMetricsDefault() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("metrics").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterMetricsJSON(json);}
public void testClusterMetricsXML() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("metrics").accept("application/xml").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_XML_TYPE, response.getType());String xml = response.getEntity(String.class);verifyClusterMetricsXML(xml);}
public void verifyClusterMetricsXML(String xml) {DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();DocumentBuilder db = dbf.newDocumentBuilder();InputSource is = new InputSource();is.setCharacterStream(new StringReader(xml));Document dom = db.parse(is);NodeList nodes = dom.getElementsByTagName("clusterMetrics");assertEquals("incorrect number of elements", 1, nodes.getLength());for (int i = 0; i < nodes.getLength(); i++) {Element element = (Element) nodes.item(i);verifyClusterMetrics(WebServicesTestUtils.getXmlInt(element, "appsSubmitted"), WebServicesTestUtils.getXmlInt(element, "appsCompleted"), WebServicesTestUtils.getXmlInt(element, "reservedMB"), WebServicesTestUtils.getXmlInt(element, "availableMB"), WebServicesTestUtils.getXmlInt(element, "allocatedMB"), WebServicesTestUtils.getXmlInt(element, "reservedVirtualCores"), WebServicesTestUtils.getXmlInt(element, "availableVirtualCores"), WebServicesTestUtils.getXmlInt(element, "allocatedVirtualCores"), WebServicesTestUtils.getXmlInt(element, "totalVirtualCores"), WebServicesTestUtils.getXmlInt(element, "containersAllocated"), WebServicesTestUtils.getXmlInt(element, "totalMB"), WebServicesTestUtils.getXmlInt(element, "totalNodes"), WebServicesTestUtils.getXmlInt(element, "lostNodes"), WebServicesTestUtils.getXmlInt(element, "unhealthyNodes"), WebServicesTestUtils.getXmlInt(element, "decommissionedNodes"), WebServicesTestUtils.getXmlInt(element, "rebootedNodes"), WebServicesTestUtils.getXmlInt(element, "activeNodes"));}}
public void verifyClusterMetricsJSON(JSONObject json) {assertEquals("incorrect number of elements", 1, json.length());JSONObject clusterinfo = json.getJSONObject("clusterMetrics");assertEquals("incorrect number of elements", 23, clusterinfo.length());verifyClusterMetrics(clusterinfo.getInt("appsSubmitted"), clusterinfo.getInt("appsCompleted"), clusterinfo.getInt("reservedMB"), clusterinfo.getInt("availableMB"), clusterinfo.getInt("allocatedMB"), clusterinfo.getInt("reservedVirtualCores"), clusterinfo.getInt("availableVirtualCores"), clusterinfo.getInt("allocatedVirtualCores"), clusterinfo.getInt("totalVirtualCores"), clusterinfo.getInt("containersAllocated"), clusterinfo.getInt("totalMB"), clusterinfo.getInt("totalNodes"), clusterinfo.getInt("lostNodes"), clusterinfo.getInt("unhealthyNodes"), clusterinfo.getInt("decommissionedNodes"), clusterinfo.getInt("rebootedNodes"), clusterinfo.getInt("activeNodes"));}
public void verifyClusterMetrics(int submittedApps, int completedApps, int reservedMB, int availableMB, int allocMB, int reservedVirtualCores, int availableVirtualCores, int allocVirtualCores, int totalVirtualCores, int containersAlloc, int totalMB, int totalNodes, int lostNodes, int unhealthyNodes, int decommissionedNodes, int rebootedNodes, int activeNodes) {ResourceScheduler rs = rm.getResourceScheduler();QueueMetrics metrics = rs.getRootQueueMetrics();ClusterMetrics clusterMetrics = ClusterMetrics.getMetrics();long totalMBExpect = metrics.getAvailableMB() + metrics.getAllocatedMB();long totalVirtualCoresExpect = metrics.getAvailableVirtualCores() + metrics.getAllocatedVirtualCores();assertEquals("appsSubmitted doesn't match", metrics.getAppsSubmitted(), submittedApps);assertEquals("appsCompleted doesn't match", metrics.getAppsCompleted(), completedApps);assertEquals("reservedMB doesn't match", metrics.getReservedMB(), reservedMB);assertEquals("availableMB doesn't match", metrics.getAvailableMB(), availableMB);assertEquals("allocatedMB doesn't match", metrics.getAllocatedMB(), allocMB);assertEquals("reservedVirtualCores doesn't match", metrics.getReservedVirtualCores(), reservedVirtualCores);assertEquals("availableVirtualCores doesn't match", metrics.getAvailableVirtualCores(), availableVirtualCores);assertEquals("allocatedVirtualCores doesn't match", totalVirtualCoresExpect, allocVirtualCores);assertEquals("containersAllocated doesn't match", 0, containersAlloc);assertEquals("totalMB doesn't match", totalMBExpect, totalMB);assertEquals("totalNodes doesn't match", clusterMetrics.getNumActiveNMs() + clusterMetrics.getNumLostNMs() + clusterMetrics.getNumDecommisionedNMs() + clusterMetrics.getNumRebootedNMs() + clusterMetrics.getUnhealthyNMs(), totalNodes);assertEquals("lostNodes doesn't match", clusterMetrics.getNumLostNMs(), lostNodes);assertEquals("unhealthyNodes doesn't match", clusterMetrics.getUnhealthyNMs(), unhealthyNodes);assertEquals("decommissionedNodes doesn't match", clusterMetrics.getNumDecommisionedNMs(), decommissionedNodes);assertEquals("rebootedNodes doesn't match", clusterMetrics.getNumRebootedNMs(), rebootedNodes);assertEquals("activeNodes doesn't match", clusterMetrics.getNumActiveNMs(), activeNodes);}
public void testClusterSchedulerFifo() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterSchedulerFifo(json);}
public void testClusterSchedulerFifoSlash() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler/").accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterSchedulerFifo(json);}
public void testClusterSchedulerFifoDefault() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler").get(ClientResponse.class);assertEquals(MediaType.APPLICATION_JSON_TYPE, response.getType());JSONObject json = response.getEntity(JSONObject.class);verifyClusterSchedulerFifo(json);}
public void testClusterSchedulerFifoXML() {WebResource r = resource();ClientResponse response = r.path("ws").path("v1").path("cluster").path("scheduler").accept(MediaType.APPLICATION_XML).get(ClientResponse.class);assertEquals(MediaType.APPLICATION_XML_TYPE, response.getType());String xml = response.getEntity(String.class);verifySchedulerFifoXML(xml);}
public void verifySchedulerFifoXML(String xml) {DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();DocumentBuilder db = dbf.newDocumentBuilder();InputSource is = new InputSource();is.setCharacterStream(new StringReader(xml));Document dom = db.parse(is);NodeList nodesSched = dom.getElementsByTagName("scheduler");assertEquals("incorrect number of elements", 1, nodesSched.getLength());NodeList nodes = dom.getElementsByTagName("schedulerInfo");assertEquals("incorrect number of elements", 1, nodes.getLength());for (int i = 0; i < nodes.getLength(); i++) {Element element = (Element) nodes.item(i);verifyClusterSchedulerFifoGeneric(WebServicesTestUtils.getXmlAttrString(element, "xsi:type"), WebServicesTestUtils.getXmlString(element, "qstate"), WebServicesTestUtils.getXmlFloat(element, "capacity"), WebServicesTestUtils.getXmlFloat(element, "usedCapacity"), WebServicesTestUtils.getXmlInt(element, "minQueueMemoryCapacity"), WebServicesTestUtils.getXmlInt(element, "maxQueueMemoryCapacity"), WebServicesTestUtils.getXmlInt(element, "numNodes"), WebServicesTestUtils.getXmlInt(element, "usedNodeCapacity"), WebServicesTestUtils.getXmlInt(element, "availNodeCapacity"), WebServicesTestUtils.getXmlInt(element, "totalNodeCapacity"), WebServicesTestUtils.getXmlInt(element, "numContainers"));}}
public void verifyClusterSchedulerFifo(JSONObject json) {assertEquals("incorrect number of elements", 1, json.length());JSONObject info = json.getJSONObject("scheduler");assertEquals("incorrect number of elements", 1, info.length());info = info.getJSONObject("schedulerInfo");assertEquals("incorrect number of elements", 11, info.length());verifyClusterSchedulerFifoGeneric(info.getString("type"), info.getString("qstate"), (float) info.getDouble("capacity"), (float) info.getDouble("usedCapacity"), info.getInt("minQueueMemoryCapacity"), info.getInt("maxQueueMemoryCapacity"), info.getInt("numNodes"), info.getInt("usedNodeCapacity"), info.getInt("availNodeCapacity"), info.getInt("totalNodeCapacity"), info.getInt("numContainers"));}
public void verifyClusterSchedulerFifoGeneric(String type, String state, float capacity, float usedCapacity, int minQueueCapacity, int maxQueueCapacity, int numNodes, int usedNodeCapacity, int availNodeCapacity, int totalNodeCapacity, int numContainers) {assertEquals("type doesn't match", "fifoScheduler", type);assertEquals("qstate doesn't match", QueueState.RUNNING.toString(), state);assertEquals("capacity doesn't match", 1.0, capacity, 0.0);assertEquals("usedCapacity doesn't match", 0.0, usedCapacity, 0.0);assertEquals("minQueueMemoryCapacity doesn't match", YarnConfiguration.DEFAULT_RM_SCHEDULER_MINIMUM_ALLOCATION_MB, minQueueCapacity);assertEquals("maxQueueMemoryCapacity doesn't match", YarnConfiguration.DEFAULT_RM_SCHEDULER_MAXIMUM_ALLOCATION_MB, maxQueueCapacity);assertEquals("numNodes doesn't match", 0, numNodes);assertEquals("usedNodeCapacity doesn't match", 0, usedNodeCapacity);assertEquals("availNodeCapacity doesn't match", 0, availNodeCapacity);assertEquals("totalNodeCapacity doesn't match", 0, totalNodeCapacity);assertEquals("numContainers doesn't match", 0, numContainers);}public long getTime() {return time;}
public void tick(long ms) {time += ms;}
public void testGetAllocationFileFromClasspath() {Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, "test-fair-scheduler.xml");AllocationFileLoaderService allocLoader = new AllocationFileLoaderService();File allocationFile = allocLoader.getAllocationFile(conf);assertEquals("test-fair-scheduler.xml", allocationFile.getName());assertTrue(allocationFile.exists());}
public void testReload() {PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");out.println("  <queue name=\"queueA\">");out.println("<maxRunningApps>1</maxRunningApps>");out.println("  </queue>");out.println("  <queue name=\"queueB\" />");out.println("  <queuePlacementPolicy>");out.println("<rule name='default' />");out.println("  </queuePlacementPolicy>");out.println("</allocations>");out.close();MockClock clock = new MockClock();Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, ALLOC_FILE);AllocationFileLoaderService allocLoader = new AllocationFileLoaderService(clock);allocLoader.reloadIntervalMs = 5;allocLoader.init(conf);ReloadListener confHolder = new ReloadListener();allocLoader.setReloadListener(confHolder);allocLoader.reloadAllocations();AllocationConfiguration allocConf = confHolder.allocConf;// Verify confQueuePlacementPolicy policy = allocConf.getPlacementPolicy();List<QueuePlacementRule> rules = policy.getRules();assertEquals(1, rules.size());assertEquals(QueuePlacementRule.Default.class, rules.get(0).getClass());assertEquals(1, allocConf.getQueueMaxApps("root.queueA"));assertEquals(2, allocConf.getConfiguredQueues().get(FSQueueType.LEAF).size());assertTrue(allocConf.getConfiguredQueues().get(FSQueueType.LEAF).contains("root.queueA"));assertTrue(allocConf.getConfiguredQueues().get(FSQueueType.LEAF).contains("root.queueB"));confHolder.allocConf = null;// Modify file and advance the clockout = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");out.println("  <queue name=\"queueB\">");out.println("<maxRunningApps>3</maxRunningApps>");out.println("  </queue>");out.println("  <queuePlacementPolicy>");out.println("<rule name='specified' />");out.println("<rule name='nestedUserQueue' >");out.println(" <rule name='primaryGroup' />");out.println("</rule>");out.println("<rule name='default' />");out.println("  </queuePlacementPolicy>");out.println("</allocations>");out.close();clock.tick(System.currentTimeMillis() + AllocationFileLoaderService.ALLOC_RELOAD_WAIT_MS + 10000);allocLoader.start();while (confHolder.allocConf == null) {Thread.sleep(20);}// Verify confallocConf = confHolder.allocConf;policy = allocConf.getPlacementPolicy();rules = policy.getRules();assertEquals(3, rules.size());assertEquals(QueuePlacementRule.Specified.class, rules.get(0).getClass());assertEquals(QueuePlacementRule.NestedUserQueue.class, rules.get(1).getClass());assertEquals(QueuePlacementRule.PrimaryGroup.class, ((NestedUserQueue) (rules.get(1))).nestedRule.getClass());assertEquals(QueuePlacementRule.Default.class, rules.get(2).getClass());assertEquals(3, allocConf.getQueueMaxApps("root.queueB"));assertEquals(1, allocConf.getConfiguredQueues().get(FSQueueType.LEAF).size());assertTrue(allocConf.getConfiguredQueues().get(FSQueueType.LEAF).contains("root.queueB"));}
public void testAllocationFileParsing() {Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, ALLOC_FILE);AllocationFileLoaderService allocLoader = new AllocationFileLoaderService();PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");// Give queue A a minimum of 1024 Mout.println("<queue name=\"queueA\">");out.println("<minResources>1024mb,0vcores</minResources>");out.println("</queue>");// Give queue B a minimum of 2048 Mout.println("<queue name=\"queueB\">");out.println("<minResources>2048mb,0vcores</minResources>");out.println("<aclAdministerApps>alice,bob admins</aclAdministerApps>");out.println("<schedulingPolicy>fair</schedulingPolicy>");out.println("</queue>");// Give queue C no minimumout.println("<queue name=\"queueC\">");out.println("<aclSubmitApps>alice,bob admins</aclSubmitApps>");out.println("</queue>");// Give queue D a limit of 3 running apps and 0.4f maxAMShareout.println("<queue name=\"queueD\">");out.println("<maxRunningApps>3</maxRunningApps>");out.println("<maxAMShare>0.4</maxAMShare>");out.println("</queue>");// Give queue E a preemption timeout of one minuteout.println("<queue name=\"queueE\">");out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");out.println("</queue>");//Make queue F a parent queue without configured leaf queues using the 'type' attributeout.println("<queue name=\"queueF\" type=\"parent\" >");out.println("</queue>");//Create hierarchical queues G,Hout.println("<queue name=\"queueG\">");out.println("   <queue name=\"queueH\">");out.println("   </queue>");out.println("</queue>");// Set default limit of apps per queue to 15out.println("<queueMaxAppsDefault>15</queueMaxAppsDefault>");// Set default limit of apps per user to 5out.println("<userMaxAppsDefault>5</userMaxAppsDefault>");// Set default limit of AMResourceShare to 0.5fout.println("<queueMaxAMShareDefault>0.5f</queueMaxAMShareDefault>");// Give user1 a limit of 10 jobsout.println("<user name=\"user1\">");out.println("<maxRunningApps>10</maxRunningApps>");out.println("</user>");// Set default min share preemption timeout to 2 minutesout.println("<defaultMinSharePreemptionTimeout>120" + "</defaultMinSharePreemptionTimeout>");// Set fair share preemption timeout to 5 minutesout.println("<fairSharePreemptionTimeout>300</fairSharePreemptionTimeout>");// Set default scheduling policy to DRFout.println("<defaultQueueSchedulingPolicy>drf</defaultQueueSchedulingPolicy>");out.println("</allocations>");out.close();allocLoader.init(conf);ReloadListener confHolder = new ReloadListener();allocLoader.setReloadListener(confHolder);allocLoader.reloadAllocations();AllocationConfiguration queueConf = confHolder.allocConf;assertEquals(6, queueConf.getConfiguredQueues().get(FSQueueType.LEAF).size());assertEquals(Resources.createResource(0), queueConf.getMinResources("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(Resources.createResource(0), queueConf.getMinResources("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(Resources.createResource(1024, 0), queueConf.getMinResources("root.queueA"));assertEquals(Resources.createResource(2048, 0), queueConf.getMinResources("root.queueB"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueC"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueD"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueE"));assertEquals(15, queueConf.getQueueMaxApps("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(15, queueConf.getQueueMaxApps("root.queueA"));assertEquals(15, queueConf.getQueueMaxApps("root.queueB"));assertEquals(15, queueConf.getQueueMaxApps("root.queueC"));assertEquals(3, queueConf.getQueueMaxApps("root.queueD"));assertEquals(15, queueConf.getQueueMaxApps("root.queueE"));assertEquals(10, queueConf.getUserMaxApps("user1"));assertEquals(5, queueConf.getUserMaxApps("user2"));assertEquals(.5f, queueConf.getQueueMaxAMShare("root." + YarnConfiguration.DEFAULT_QUEUE_NAME), 0.01);assertEquals(.5f, queueConf.getQueueMaxAMShare("root.queueA"), 0.01);assertEquals(.5f, queueConf.getQueueMaxAMShare("root.queueB"), 0.01);assertEquals(.5f, queueConf.getQueueMaxAMShare("root.queueC"), 0.01);assertEquals(.4f, queueConf.getQueueMaxAMShare("root.queueD"), 0.01);assertEquals(.5f, queueConf.getQueueMaxAMShare("root.queueE"), 0.01);// Root should get * ACLassertEquals("*", queueConf.getQueueAcl("root", QueueACL.ADMINISTER_QUEUE).getAclString());assertEquals("*", queueConf.getQueueAcl("root", QueueACL.SUBMIT_APPLICATIONS).getAclString());// Unspecified queues should get default ACLassertEquals(" ", queueConf.getQueueAcl("root.queueA", QueueACL.ADMINISTER_QUEUE).getAclString());assertEquals(" ", queueConf.getQueueAcl("root.queueA", QueueACL.SUBMIT_APPLICATIONS).getAclString());// Queue B ACLassertEquals("alice,bob admins", queueConf.getQueueAcl("root.queueB", QueueACL.ADMINISTER_QUEUE).getAclString());// Queue C ACLassertEquals("alice,bob admins", queueConf.getQueueAcl("root.queueC", QueueACL.SUBMIT_APPLICATIONS).getAclString());assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueA"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueB"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueC"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueD"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueA"));assertEquals(60000, queueConf.getMinSharePreemptionTimeout("root.queueE"));assertEquals(300000, queueConf.getFairSharePreemptionTimeout());assertTrue(queueConf.getConfiguredQueues().get(FSQueueType.PARENT).contains("root.queueF"));assertTrue(queueConf.getConfiguredQueues().get(FSQueueType.PARENT).contains("root.queueG"));assertTrue(queueConf.getConfiguredQueues().get(FSQueueType.LEAF).contains("root.queueG.queueH"));// Verify existing queues have default scheduling policyassertEquals(DominantResourceFairnessPolicy.NAME, queueConf.getSchedulingPolicy("root").getName());assertEquals(DominantResourceFairnessPolicy.NAME, queueConf.getSchedulingPolicy("root.queueA").getName());// Verify default is overriden if specified explicitlyassertEquals(FairSharePolicy.NAME, queueConf.getSchedulingPolicy("root.queueB").getName());// Verify new queue gets default scheduling policyassertEquals(DominantResourceFairnessPolicy.NAME, queueConf.getSchedulingPolicy("root.newqueue").getName());}
public void testBackwardsCompatibleAllocationFileParsing() {Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, ALLOC_FILE);AllocationFileLoaderService allocLoader = new AllocationFileLoaderService();PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");// Give queue A a minimum of 1024 Mout.println("<pool name=\"queueA\">");out.println("<minResources>1024mb,0vcores</minResources>");out.println("</pool>");// Give queue B a minimum of 2048 Mout.println("<pool name=\"queueB\">");out.println("<minResources>2048mb,0vcores</minResources>");out.println("<aclAdministerApps>alice,bob admins</aclAdministerApps>");out.println("</pool>");// Give queue C no minimumout.println("<pool name=\"queueC\">");out.println("<aclSubmitApps>alice,bob admins</aclSubmitApps>");out.println("</pool>");// Give queue D a limit of 3 running appsout.println("<pool name=\"queueD\">");out.println("<maxRunningApps>3</maxRunningApps>");out.println("</pool>");// Give queue E a preemption timeout of one minuteout.println("<pool name=\"queueE\">");out.println("<minSharePreemptionTimeout>60</minSharePreemptionTimeout>");out.println("</pool>");// Set default limit of apps per queue to 15out.println("<queueMaxAppsDefault>15</queueMaxAppsDefault>");// Set default limit of apps per user to 5out.println("<userMaxAppsDefault>5</userMaxAppsDefault>");// Give user1 a limit of 10 jobsout.println("<user name=\"user1\">");out.println("<maxRunningApps>10</maxRunningApps>");out.println("</user>");// Set default min share preemption timeout to 2 minutesout.println("<defaultMinSharePreemptionTimeout>120" + "</defaultMinSharePreemptionTimeout>");// Set fair share preemption timeout to 5 minutesout.println("<fairSharePreemptionTimeout>300</fairSharePreemptionTimeout>");out.println("</allocations>");out.close();allocLoader.init(conf);ReloadListener confHolder = new ReloadListener();allocLoader.setReloadListener(confHolder);allocLoader.reloadAllocations();AllocationConfiguration queueConf = confHolder.allocConf;assertEquals(5, queueConf.getConfiguredQueues().get(FSQueueType.LEAF).size());assertEquals(Resources.createResource(0), queueConf.getMinResources("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(Resources.createResource(0), queueConf.getMinResources("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(Resources.createResource(1024, 0), queueConf.getMinResources("root.queueA"));assertEquals(Resources.createResource(2048, 0), queueConf.getMinResources("root.queueB"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueC"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueD"));assertEquals(Resources.createResource(0), queueConf.getMinResources("root.queueE"));assertEquals(15, queueConf.getQueueMaxApps("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(15, queueConf.getQueueMaxApps("root.queueA"));assertEquals(15, queueConf.getQueueMaxApps("root.queueB"));assertEquals(15, queueConf.getQueueMaxApps("root.queueC"));assertEquals(3, queueConf.getQueueMaxApps("root.queueD"));assertEquals(15, queueConf.getQueueMaxApps("root.queueE"));assertEquals(10, queueConf.getUserMaxApps("user1"));assertEquals(5, queueConf.getUserMaxApps("user2"));// Unspecified queues should get default ACLassertEquals(" ", queueConf.getQueueAcl("root.queueA", QueueACL.ADMINISTER_QUEUE).getAclString());assertEquals(" ", queueConf.getQueueAcl("root.queueA", QueueACL.SUBMIT_APPLICATIONS).getAclString());// Queue B ACLassertEquals("alice,bob admins", queueConf.getQueueAcl("root.queueB", QueueACL.ADMINISTER_QUEUE).getAclString());// Queue C ACLassertEquals("alice,bob admins", queueConf.getQueueAcl("root.queueC", QueueACL.SUBMIT_APPLICATIONS).getAclString());assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root." + YarnConfiguration.DEFAULT_QUEUE_NAME));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueA"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueB"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueC"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueD"));assertEquals(120000, queueConf.getMinSharePreemptionTimeout("root.queueA"));assertEquals(60000, queueConf.getMinSharePreemptionTimeout("root.queueE"));assertEquals(300000, queueConf.getFairSharePreemptionTimeout());}
public void testSimplePlacementPolicyFromConf() {Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, ALLOC_FILE);conf.setBoolean(FairSchedulerConfiguration.ALLOW_UNDECLARED_POOLS, false);conf.setBoolean(FairSchedulerConfiguration.USER_AS_DEFAULT_QUEUE, false);PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");out.println("</allocations>");out.close();AllocationFileLoaderService allocLoader = new AllocationFileLoaderService();allocLoader.init(conf);ReloadListener confHolder = new ReloadListener();allocLoader.setReloadListener(confHolder);allocLoader.reloadAllocations();AllocationConfiguration allocConf = confHolder.allocConf;QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();List<QueuePlacementRule> rules = placementPolicy.getRules();assertEquals(2, rules.size());assertEquals(QueuePlacementRule.Specified.class, rules.get(0).getClass());assertEquals(false, rules.get(0).create);assertEquals(QueuePlacementRule.Default.class, rules.get(1).getClass());}
public void testQueueAlongsideRoot() {Configuration conf = new Configuration();conf.set(FairSchedulerConfiguration.ALLOCATION_FILE, ALLOC_FILE);PrintWriter out = new PrintWriter(new FileWriter(ALLOC_FILE));out.println("<?xml version=\"1.0\"?>");out.println("<allocations>");out.println("<queue name=\"root\">");out.println("</queue>");out.println("<queue name=\"other\">");out.println("</queue>");out.println("</allocations>");out.close();AllocationFileLoaderService allocLoader = new AllocationFileLoaderService();allocLoader.init(conf);ReloadListener confHolder = new ReloadListener();allocLoader.setReloadListener(confHolder);allocLoader.reloadAllocations();}
public void onReload(AllocationConfiguration info) {allocConf = info;}